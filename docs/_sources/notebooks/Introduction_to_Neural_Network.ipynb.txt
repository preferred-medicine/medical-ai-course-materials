{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Da9LzFk5G38W"
   },
   "source": [
    "[![colab-logo](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/preferred-medicine/medical-ai-course-materials/blob/master/notebooks/Introduction_to_Neural_Network.ipynb)\n",
    "\n",
    "#  Basics of neural network \n",
    "\n",
    "Here, we will introduce the outline of the neural network (Neural Network). Methods such as Convolutional Neural Network (CNN), which is used for image recognition, and Recurrent Neural Network (RNN), which is used for natural language processing, are types of neural networks.\n",
    "\n",
    "Here, first of all, after explaining the structure of the neural network called the simplest all-connection type, when preparing a training data set consisting of a combination of a plurality of input data and a desired output, how to train the neural network We will explain about what to do (a supervised learning system).\n",
    "\n",
    "We also introduce an algorithm called error backpropagation to learn in real time the complex functions represented by neural networks.\n",
    "\n",
    "First, instead of treating the neural network as a black box, we carefully examine the calculations performed internally. And we will understand that the linear transformation represented by the function characterized by parameters and the subsequent nonlinear transformation are combined to represent one function that can be differentiated as a whole.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LICMyO4rKUX9"
   },
   "source": [
    "## Structure of neural network\n",
    "\n",
    "First, let's look at the structure of the neural network graphically. The input variables are 4 variables of {years, alcohol content, color, smell}, and the output variables are 2 variables of {white wine, red wine}.\n",
    "\n",
    "![„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅÆÂü∫Êú¨ÊßãÈÄ†](https://raw.githubusercontent.com/preferred-medicine/medical-ai-course-materials/master/notebooks/images/3/01.png)\n",
    "\n",
    "Each round part of this figure is called a **node** or **unit**, and its vertical collection is called a **layer** . The first layer is called the **input layer** , the last layer is the **output layer** , and the **middle layer** is called the **intermediate layer** or **hidden layer** . This model has a three-layer structure with an input layer, an intermediate layer, and an output layer, but it is possible to define a multi-layered neural network by increasing the number of intermediate layers. In this example, all nodes between each layer are connected to each other, so it is also called a **fully connected** neural network, which is the most basic structure of neural networks.\n",
    "\n",
    "Input variables are the same as in the previous chapters, but the handling of output variables is different. For example, in the figure above, each node in the output layer corresponds to white wine and red wine, and there are as many output variables as there are categories. Why is this structure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "meyqKCr-NUDs"
   },
   "source": [
    "![„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅÆÂá∫ÂäõÂÄ§](https://raw.githubusercontent.com/preferred-medicine/medical-ai-course-materials/master/notebooks/images/3/02.png)\n",
    "\n",
    "First, let's look at a concrete example of what kind of value is in the final layer. For example, let's say you have a wine whose age is three years and whose alcohol content is 14 degrees, color is 0.2, and smell is 0.8. The internal calculation will be described later, so let's focus on the values obtained when such data are given to the neural network. Above, white wine $y_{1} = 0.15$,, Red wine $y_{2}= 0.85$ It is At this time, the class corresponding to the variable with the largest value among the output values, that is, \"red wine\" in this example can be used as the **prediction result** of this neural network in this classification problem .\n",
    "\n",
    "Here, when we add up all the values of the output layer, we notice that it is 1. This is not a coincidence, because it calculates the value of the output layer to be so\\*. In other words, the numerical values possessed by each node of the output layer represent the probability that the input belongs to each class. Therefore, the output layer needs nodes as many as the number of categories.\n",
    "\n",
    "Now let's take a closer look at the calculations performed inside the neural network. Each layer of the neural network is calculated by sequentially applying linear and nonlinear transformations to the values of the previous layer. First, let's look at what the linear transformation here means.\n",
    "\n",
    "\\* Specifically, apply the softmax activation function (also described later) to the output vector of the neural network so that the sum of all node values in the output layer is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X9-RYTlXNzNQ"
   },
   "source": [
    "### Linear transformation \n",
    "\n",
    "Here we describe the linear transformations that occur in each layer of the neural network.\n",
    "\n",
    "![„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅÆÁ∑öÂΩ¢Â§âÊèõ](https://raw.githubusercontent.com/preferred-medicine/medical-ai-course-materials/master/notebooks/images/3/linear_transformation.png)\n",
    "\n",
    "Linear transformation * said here is weight matrix ($w$) $\\times$ Input vector ($h$) $+$  Bias vector ($b$) It refers to the calculation like). At this time, the input of this conversion is $h$, Parameter is $w$ When $b$. Multiplication here ($\\times$ Note that) is matrix multiplication. Also from now on, $h$ Often appear as letters, but this is the first letter of the hidden layer$h$ I'm from. However, in order to simplify the notation, in the following, the input layerùë• $x_1, x_2, x_3, x_4$ Think of ) as the 0 hidden layer , $h_{01}, h_{02}, h_{03}, h_{04}$ It is written as Now let's describe the calculation shown in the above figure with a formula.\n",
    "\n",
    "(* In linear mathematics, in linear mathematics ${\\bf w} \\times {\\bf h}$ This transformation is strictly called \"affine transformation (or affine transformation)\". However, in the context of deep learning, this transformation is also often referred to as a linear transformation.)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "u_{11}&=w_{11}h_{01}+w_{12}h_{02}+w_{13}h_{03}+w_{14}h_{04}+b_{1} \\\\\n",
    "u_{12}&=w_{21}h_{01}+w_{22}h_{02}+w_{23}h_{03}+w_{24}h_{04}+b_{2} \\\\\n",
    "u_{13}&=w_{31}h_{01}+w_{32}h_{02}+w_{33}h_{03}+w_{34}h_{04}+b_{3}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "bias ($b_1, b_2, b_3$) Note that is omitted in the above figure. Now, the above four expressions can be rewritten as vector and matrix calculations as follows.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\begin{bmatrix}\n",
    "u_{11} \\\\\n",
    "u_{12} \\\\\n",
    "u_{13}\n",
    "\\end{bmatrix}&=\\begin{bmatrix}\n",
    "w_{11} & w_{12} & w_{13} & w_{14} \\\\\n",
    "w_{21} & w_{22} & w_{23} & w_{24} \\\\\n",
    "w_{31} & w_{32} & w_{33} & w_{34}\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "h_{01} \\\\\n",
    "h_{02} \\\\\n",
    "h_{03} \\\\\n",
    "h_{04}\n",
    "\\end{bmatrix}+\\begin{bmatrix}\n",
    "b_{1} \\\\\n",
    "b_{2} \\\\\n",
    "b_{3}\n",
    "\\end{bmatrix}\\\\\n",
    "{\\bf u}_{1}&={\\bf W}{\\bf h}_{0}+{\\bf b}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Initially ${\\bf W}$ and ${\\bf b}$ You should add a suffix to indicate which layer and which layer to use for the calculation, but this is omitted here for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JmlTx1YF4xMD"
   },
   "source": [
    "### Nonlinear transformation/Conversion\n",
    "\n",
    "Next, I will explain nonlinear conversion. Linear transformations alone can not properly represent the relationship between inputs and outputs if they are nonlinear as shown in the right of the figure.\n",
    "![ÂÖ•Âäõ„Å®Âá∫Âäõ„ÅÆÈñ¢‰øÇ](https://raw.githubusercontent.com/preferred-medicine/medical-ai-course-materials/master/notebooks/images/3/04.png)\n",
    "\n",
    "Therefore, in the neural network, the linear transformation is followed by the nonlinear transformation in each layer so that the whole function has nonlinearity. The function that performs this non-linear transformation is called an activation function in the context of neural networks .\n",
    "\n",
    "The result of the linear transformation above $u_{11}, u_{12}, u_{13}$  Result of nonlinear conversion using the activation function $h_{11}, h_{12}, h_{13}$ These are called activation values (see the figure below). This is the input to the next layer.\n",
    "\n",
    "\n",
    "![Ê¥ªÊÄßÂÄ§](https://raw.githubusercontent.com/preferred-medicine/medical-ai-course-materials/master/notebooks/images/3/activation.png)\n",
    "\n",
    "As a specific example of the activation function, the logistic sigmoid function (hereinafter referred to as sigmoid function) shown in the figure below\n",
    "\n",
    "\n",
    "![„Ç∑„Ç∞„É¢„Ç§„ÉâÈñ¢Êï∞](https://raw.githubusercontent.com/preferred-medicine/medical-ai-course-materials/master/notebooks/images/3/05.png)\n",
    "\n",
    "Has been used in the past. However, in recent years, sigmoid functions are hardly used as activation functions in neural networks with many layers. One of the reasons is that adopting a sigmoid function as an activation function makes the phenomenon of gradient disappearance more likely to occur, which may cause a problem that learning does not progress. This will be detailed later. To avoid this, a function called ‚Äú Rectified Linear Unit (ReLU) ‚Äù is often used. This is a function of the form\n",
    "\n",
    "![ReLUÈñ¢Êï∞](https://raw.githubusercontent.com/preferred-medicine/medical-ai-course-materials/master/notebooks/images/3/06.png)\n",
    "\n",
    "here, ${\\rm max}(0, u)$ ,$0$ When $u$ It is a function that compares the and returns the larger one. That is, ReLU is a function that the output is constant at 0 when the input is a negative value, and outputs the input as it is when it is a positive value. In the sigmoid function, it can also be seen from the plot that the slope will become smaller and smaller when the input takes small or large values. On the other hand, ReLU function generates a constant gradient no matter how large the input value. This works well for the problem of gradient loss that we will introduce later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s-eFDAuJ-Cse"
   },
   "source": [
    "###  Check the flow of calculation while watching the numerical \n",
    "\n",
    "Here, input using the specific numbers written in the figure below $x_1, x_2, x_3$ Output from $y$ Let's check the process of calculating. Now bias to simplify the calculations ${\\bf b}$ The calculation of is omitted (assuming the bias is all zeros). As a numerical example, ${\\bf x} = \\begin{bmatrix} 2 & 3 & 1 \\end{bmatrix}^T$ Output when given $y$  Let's follow the calculation procedure of\n",
    "\n",
    "\n",
    "![Âá∫Âäõ„Åæ„Åß„ÅÆË®àÁÆó‰æã](https://raw.githubusercontent.com/preferred-medicine/medical-ai-course-materials/master/notebooks/images/3/output.png)\n",
    "\n",
    "In the multiple regression analysis described in the previous chapter, although it was possible to calculate analytically optimal parameters by setting the derivative of the objective function parameters to 0, it is generally impossible to solve analytically parameters in neural networks . Instead, we will optimize the parameters sequentially in another way using the value of this derivative (slope).\n",
    "\n",
    "For this reason, in the case of a neural network, **parameters are first initialized with random numbers, and data are first input to calculate the value of the objective function**. Next, calculate the gradient of the function, use it to update the parameter, process the input data again using the updated new parameter, calculate the value of the objective function, and so on repeatedly. It will be taken care of.\n",
    "\n",
    "Now, let's consider the case where the linear transformation is applied to the values ‚Äã‚Äãof the input layer with the numerical values ‚Äã‚Äãgiven to the branches of the graph in the above figure as a result of initializing the parameters. This calculation is as follows.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "u_{11}&=3\\times 2+1\\times 3+2\\times 1=11\\\\\n",
    "u_{12}&=-2\\times 2-3\\times 3-1\\times 1=-14\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Next, let's adopt the ReLU function as the activation function that performs non-linear transformation, and calculate the value of the middle layer as follows.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_{11} &= \\max(0, 11) = 11 \\\\\n",
    "h_{12} &= \\max(0, -14)  = 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Similarly, in the output layer ùë¶ If you calculate up to the value of,\n",
    "\n",
    "$$\n",
    "y = 3 \\times 11 + 2 \\times 0 = 33\n",
    "$$\n",
    "\n",
    "Now, from the next section, let's see how to update parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ivVppUOibSHy"
   },
   "source": [
    "## The objective function \n",
    "\n",
    "Even in neural networks, as long as they are differentiable, you can use various objective functions according to the task you want to solve.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BCGZbTKcbUpe"
   },
   "source": [
    "### Mean squared \n",
    "For example, in the output layer $N$ Consider the case of solving a regression problem with a neural network with discrete values.$N$ Each of the $y_n (n=1, 2, \\dots, N)$ÔºâDesired output $t_n (n=1, 2, \\dots, N)$ÔºâWhen the objective function is given,Ôºà$y_n$ÔºâAnd the corresponding correct answer $t_n$ÔºâThe regression problem can be solved by using the **mean squared error** between) .\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\dfrac{1}{N} \\sum_{n=1}^{N}(t_{n} - y_{n})^{2}\n",
    "$$\n",
    "\n",
    "The parameters in the neural network are determined to minimize this. For example, as the correct answer in the example aboveùë°=20 The value of the objective function when given is\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\dfrac{1}{1} (20 - 33)^2 = 169\n",
    "$$\n",
    "\n",
    "is. You just have to look for values in the weight matrix that make this smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WqFc0jYJFLYz"
   },
   "source": [
    "### Cross entropy \n",
    "In the case of classification problems, on the other hand, **cross entropy** is often used as an objective function.\n",
    "\n",
    "\n",
    "As an example,$N$ Consider class classification problems. Certain inputùë• Is given to the output layer of the neural network $N$ Nodes, each of which has $n$ Probability of belonging to the second class $y_n = p(y=n|x)$ Let's say that This is an input $x$ Means a prediction class, given the condition that is given $y$ But $n$ It is the probability that it is.\n",
    "\n",
    "here, $x$ The correct answer for the class to which ${\\bf t} = \\begin{bmatrix} t_1 & t_2 & \\dots & t_N \\end{bmatrix}^T$ It is assumed that it is given by the vector However, this vector is $t_n (n=1, 2, \\dots, N)$ Suppose that the vector is such that only one of is 1 and the other is 0. This is called a **1-hot vector** . And an element with a value of 1 means that the class corresponding to the index of the element is the correct one. For example, $t_3 = 1$ If it is, it means that the class corresponding to the index 3 is correct.\n",
    "\n",
    "Well, with this kind of preparation, we can describe the intersection entropy as something that can be calculated as follows.\n",
    "$$\n",
    "\\mathcal{L} = - \\frac{1}{N} \\sum_{n=1}^{N}t_{n}\\log y_{n}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kxqfo4Yrekaw"
   },
   "source": [
    "#### Note: for cross-entropy \n",
    "\n",
    "The following is for reference only if you want to know about the definition of cross entropy. If you know the definition of cross entropy in information theory etc., it may appear that what is expressed by the above equation is different from cross entropy. But this can be explained as follows. now, $q(y|x)$ Let be the conditional probability defined by the model of the neural network, $p(y|x)$ Let be the conditional probability of real data. here, $p(y|x)$ Instead the empirical distribution of learning data, since\n",
    "\n",
    "$$\n",
    "\\hat{p}(y|x) = \\frac{1}{N} \\sum_{n=1}^N I(x =x_n, y=y_n)\n",
    "$$\n",
    "\n",
    "We will use. However $I$ Is called the Dirac function, and when its equal sign holds, the value is $\\infty$, Otherwise $0$ The function is such that the integral over its domain is 1. At this time, probability distribution $\\hat{p}(y|x)$ When $q(y|x)$ Divergence (measure the distance between probability distributions, and only if and when the probability distributions match $0$ And otherwise take positive values)\n",
    "\n",
    "$$\n",
    "KL(p||q) = \\int_{x, y} \\hat{p}(y|x) \\log \\frac{\\hat{p}(y|x)}{q(y|x)} dx dy\n",
    "$$\n",
    "\n",
    "It is defined as Here we use the Dirac delta function definition and $q$ If you extract only the terms that depend on, you get the objective function of the previous cross entropy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Drh35j_lCYiy"
   },
   "source": [
    "## Optimization of neural \n",
    "\n",
    "Determining the value of the parameter that minimizes the value of the objective function proved to be the purpose of learning of the neural network. So how do you find that parameter? Determining the parameters of a neural network so that the objective function takes a desired value, given an objective function, is called neural network optimization.\n",
    "\n",
    "Before considering the optimization method, let's check again what was the object of optimization. \"Optimizing a neural network\" means \"determining properly the values of all parameters used internally by a neural network\". So what were the parameters in neural networks? In the case of the simple fully coupled neural network introduced so far, it was used for linear transformation of each layer ${\\bf W}$ When ${\\bf b}$ \n",
    "\n",
    "It is generally difficult to analytically solve each parameter of a neural network with a gradient to the objective function as 0. However, if actual data is input to a neural network, it is possible to numerically obtain the gradient of the objective function at the value of the input. Knowing this value, we can know how to change the parameter to reduce the value of the objective function. Therefore, it is possible to optimize the neural network by updating the parameters repeatedly and gradually using this gradient. Let's think about this method in order.\n",
    "\n",
    "First, please look at the following figure. The dotted line in the figure is the parameter $w$ Objective function when changing  $\\mathcal{L}$ Represents the value of. In this example, although it is in the form of a quadratic function for simplicity, the objective function of the neural network is in fact almost multidimensional and more complicated. However, let's imagine a simple form like this for explanation here. Well, this objective function gives the minimum value $w$ How can you discover\n",
    "\n",
    "![„Éë„É©„É°„Éº„Çø„Å®ÁõÆÁöÑÈñ¢Êï∞„ÅÆÈñ¢‰øÇÔºà„Ç§„É°„Éº„Ç∏Ôºâ](https://raw.githubusercontent.com/preferred-medicine/medical-ai-course-materials/master/notebooks/images/3/13.png)\n",
    "\n",
    "As described in the previous section, the neural network parameters are first initialized with random numbers. Here, as an example $w=3$ Let's think that initialization has been done. Then, $w=3$ In $\\mathcal{L}$ Slope of $\\frac{\\partial \\mathcal{L}}{\\partial w}$ Can be obtained. The objective function of neural networks is \\* differentiable \\* for all parameters. Well, let's say here temporarily $w=3$ In $\\frac{\\partial \\mathcal{L}}{\\partial w}$ But 3 Let's say that $\\frac{\\partial \\mathcal{L}}{\\partial w} |_{w=3} = 3$)). Then, as shown in the following figure,$3$ The value is $w=3$ In $\\mathcal{L}(w)$ Represents the slope of the tangent of the function (gradient).\n",
    "\n",
    "(\\* Strictly speaking, loss functions may have indifferentiable points. For example, ReLU $x=0$ There is a non-differentiable point in the neural network including ReLU because it is non-differentiable in terms of. However, in the case of commonly used neural networks, there are only a few such non-differentiable points, so they can be ignored in the optimization method described below.)\n",
    "\n",
    "![ÁõÆÁöÑÈñ¢Êï∞„ÅÆÊé•Á∑ö„ÅÆÂÇæ„Åç](https://raw.githubusercontent.com/preferred-medicine/medical-ai-course-materials/master/notebooks/images/3/11.png)\n",
    "\n",
    "With inclination, $w$ When you increase $\\mathcal{L}$ Now means that the direction of $\\mathcal{L}$ Because we want to decrease the value of, $w$  Change the $w$  **From $\\partial \\mathcal{L} / \\partial w$ It is good to pull the** .\n",
    "\n",
    "This is the basic idea when updating the parameters of the neural network using the gradient of the objective function. At this time $w$ It is common to multiply the gradient by a value called the **learning rate** to adjust the scale of the step size (the amount of updates) of .\n",
    "\n",
    "For example, now learning rate $0.5$ Let's set it to. Then, $w$ Update rate is the **learning rate** $\\times$ Because it depends on the **gradient** , $0.5 \\times 3 = 1.5$. Current $w=3$ So, **subtract this value** $w \\leftarrow w - 1.5$ And after updating $w=1.5$. The figure above shows the state after this one-time update.\n",
    "\n",
    "Perform the first update,$w$ But $w = 1.5$ It moved to the position of. So, I will try to find the slope again at this point. Next time $-1$ Let's say it was Then the **learning rate $\\times$ The slope** is $0.5 \\times -1 = -0.5$. Using this again, $w \\leftarrow w - (-0.5)$ With the second update, this time $w = 2$ It will come to the position of After updating twice in this way, it looks like the following figure.\n",
    "\n",
    "![„Éë„É©„É°„Éº„Çø„ÅÆÊõ¥Êñ∞](https://raw.githubusercontent.com/preferred-medicine/medical-ai-course-materials/master/notebooks/images/3/12.png)\n",
    "\n",
    "gradually $\\mathcal{L}$ Is the minimum valueùë§You can see that it is approaching the value of.\n",
    "\n",
    "Thus, the learning rate $\\times$ When changing the parameter with the gradient as the update amount, the parameter $w$ I want to ask for $\\mathcal{L}$ Give the minimum value of ùë§You can get closer to The objective function minimization method using such a gradient is called gradient descent method . Since neural networks are basically designed using only differentiable functions as functions between layers, all appearing functions are differentiable, and parameters are optimized by gradient descent using a training data set The method is applicable.\n",
    "\n",
    "However, usually, when optimizing a neural network by gradient descent method, it is not necessary to update parameters using data one by one, but input some data together and calculate each gradient, The method of updating parameters using the average value of the gradient is often used. This is called mini-batch learning . This is uniformly random from the training data set $k (>0)$ Data of the $k$ Update the parameters to reduce the average value of the objective function for $k$ It is a method to repeat for the combination of the data As a result, all data included in the data set will be used, but the data used for one update is $k$ It becomes one by one. In the actual implementation, the indices of the samples in the data set are randomly shuffled first to form an array, and $k$ Fetch the index one by one and construct a mini-batch using the corresponding data. Thus, using up all the indexes, that is, finishing using the data in the data set one time each, for updating parameters is called 1 epoch learning . And this $k$ We refer to as batch size or mini-batch size, and the name Stochastic Gradient Descent (SGD) is used to indicate such a learning method . Currently, almost all neural network optimization methods are based on this SGD. With SGD, not only can the overall computation time be dramatically reduced, but as in the figure below, even if the objective function is not a convex function, it converges to the ‚Äúalmost certain‚Äù local optimal solution under appropriate conditions It is known to do.\n",
    "\n",
    "![Â±ÄÊâÄÊúÄÈÅ©Ëß£„Å®Â§ßÂüüÊúÄÈÅ©Ëß£](https://raw.githubusercontent.com/preferred-medicine/medical-ai-course-materials/master/notebooks/images/3/14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gtJOsYWkKPDj"
   },
   "source": [
    "### Calculation of parameter update \n",
    "\n",
    "Now, consider a three-layer fully connected neural network as shown in the figure below, and a linear transformation between the first and second layers. ${\\bf w}_1, {\\bf b}_1$ The linear transformation between the second and third layers is ${\\bf w}_2, {\\bf b}_2$  Is represented by the parameter ${\\bf b}_1, {\\bf b}_2$ Is omitted). Also, put these together $\\boldsymbol{\\Theta}$  I will represent it as.\n",
    "\n",
    "\n",
    "![„Éë„É©„É°„Éº„ÇøÊõ¥Êñ∞„ÅÆ‰æã](https://raw.githubusercontent.com/preferred-medicine/medical-ai-course-materials/master/notebooks/images/3/08.png)\n",
    "\n",
    "The input vector is ${\\bf x}$, The output of the neural network is ${\\bf y} \\in \\mathbb{R}^N$ ($N$ (Meaning real dimensional vector) and input ${\\bf x}$ Is the ‚Äúdesired output‚Äù corresponding to ${\\bf t}$ will do. Let's use the mean squared error function described above as the objective function.\n",
    "\n",
    "Now, after initializing the parameters with appropriate random numbers, input ùê±${\\bf x}$ Let's calculate the gradient for each parameter of the objective function given, and calculate the update amount for each parameter.\n",
    "\n",
    "First, the objective function is rewritten using vector notation as follows.\n",
    "Ôºé\n",
    "\n",
    "$$\n",
    "\\mathcal{L}({\\bf y}, {\\bf t}) = \\frac{1}{N} || {\\bf t} - {\\bf y} ||_2^2\n",
    "$$\n",
    "\n",
    "$|| {\\bf t} - {\\bf y} ||_2^2$„ÅØ„Åì„Åì„Åß„ÅØ$({\\bf t} - {\\bf y})^T({\\bf t} - {\\bf y})$„Å®ÂêåÁ≠â„ÅÆÊÑèÂë≥„Å®„Å™„Çä„Åæ„ÅôÔºé„Åï„Çâ„Å´Ôºå„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÂÖ®‰Ωì„Çí $f$ „Å®Êõ∏„Åè„Åì„Å®„Å´„Åô„Çã„Å®ÔºåÂá∫Âäõ ${\\bf y}$ „ÅØ\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "{\\bf y} &= f({\\bf x}; \\boldsymbol{\\Theta}) \\\\\n",
    "&= a_2 ( {\\bf w}_2 a_1({\\bf w}_1 {\\bf x} + {\\bf b}_1) + {\\bf b}_2 )\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "You can write here, $a_1, a_2$ Denotes the non-linear transformation (activation function) applied after the linear transformation between the first and second layers and between the second and third layers, respectively. Below, for the sake of simplicity, the result of the linear transformation performed between each layer is ${\\bf u}_1, {\\bf u}_2$ And the value of the middle layer, ie ${\\bf u}_1$ Apply the activation function to ${\\bf h}_1$ Write. However, ${\\bf u}_2$ The result of applying the activation function to ${\\bf y}$ It is written as Then, these relations can be organized as follows.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "{\\bf y} &= a_2({\\bf u}_2) \\\\\n",
    "{\\bf u}_2 &= {\\bf w}_2 {\\bf h}_1 + {\\bf b}_2 \\\\\n",
    "{\\bf h}_1 &= a_1({\\bf u}_1) \\\\\n",
    "{\\bf u}_1 &= {\\bf w}_1 {\\bf x} + {\\bf b}_1\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k1L6xptUs3l4"
   },
   "source": [
    "#### Parameters ùê∞2 Of the update amount \n",
    "So first, the parameters closer to the output layer,${\\bf w}_2$ about $\\mathcal{L}$ Find the slope of. Since this is a partial derivative of the composite function, it can be expanded as follows using the chain rule.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial {\\bf w}_2}\n",
    "&= \\frac{\\partial \\mathcal{L}}{\\partial {\\bf y}} \\frac{\\partial {\\bf y}}{\\partial {\\bf w}_2} \\\\\n",
    "&= \\frac{\\partial \\mathcal{L}}{\\partial {\\bf y}} \\frac{\\partial {\\bf y}}{\\partial {\\bf u}_2} \\frac{\\partial {\\bf u}_2}{\\partial {\\bf w}_2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The three partial derivatives are\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial {\\bf y}}\n",
    "&= -\\frac{2}{N} ({\\bf t} - {\\bf y}) \\\\\n",
    "\\frac{\\partial {\\bf y}}{\\partial {\\bf u}_2}\n",
    "&= \\frac{\\partial a_2}{\\partial {\\bf u}_2} \\\\\n",
    "\\frac{\\partial {\\bf u}_2}{\\partial {\\bf w}_2} \n",
    "&= {\\bf h}_1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "I will ask. Where the slope of the output with respect to the input of the activation function\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a_2}{\\partial {\\bf u}_2}\n",
    "$$\n",
    "\n",
    "Has appeared. This is, for example, when using a sigmoid function as the activation function,\n",
    "\n",
    "\n",
    "$$\n",
    "a_2({\\bf u}_2) = \\frac{1}{1 + \\exp(-{\\bf u}_2)}\n",
    "$$\n",
    "\n",
    "Because it is a derivative of\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial a_2({\\bf u}_2)}{\\partial {\\bf u}_2}\n",
    "&= -\\frac{-(\\exp(-{\\bf u}_2))}{(1 + \\exp(-{\\bf u}_2))^2} \\\\\n",
    "&= \\frac{1}{1 + \\exp(-{\\bf u}_2)} \\cdot \\frac{\\exp(-{\\bf u}_2)}{1 + \\exp(-{\\bf u}_2)} \\\\\n",
    "&= \\frac{1}{1 + \\exp(-{\\bf u}_2)} \\cdot \\frac{1 + \\exp(-{\\bf u}_2) - 1}{1 + \\exp(-{\\bf u}_2)} \\\\\n",
    "&= \\frac{1}{1 + \\exp(-{\\bf u}_2)} (1 - \\frac{1}{1 + \\exp(-{\\bf u}_2)}) \\\\\n",
    "&= a_2({\\bf u}_2)(1 - a_2({\\bf u}_2))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The slope of the sigmoid function can thus be easily calculated using the output value of the sigmoid function.\n",
    "\n",
    "with this ${\\bf w}_2$ All the values needed to calculate the slope of were available. Let's actually calculate these using NumPy. Here, for the sake of simplicity, all bias vectors are assumed to be initialized to zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fD0MA510V1f-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ÂÖ•Âäõ\n",
    "x = np.array([2, 3, 1])\n",
    "\n",
    "# Ê≠£Ëß£\n",
    "t = np.array([20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VOznWs5PV3pQ"
   },
   "source": [
    "First, load the NumPy module and define an array of inputs. Here, a three-dimensional vector with three values is defined to be the same as the above figure . In addition, I decided to give tentatively as the correct answer . Next, define the parameters. `2, 3, 1` `20`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cBuj19ptXu1f"
   },
   "outputs": [],
   "source": [
    "# 1-2Â±§Èñì„ÅÆ„Éë„É©„É°„Éº„Çø\n",
    "w1 = np.array([[3, 1, 2], [-2, -3, -1]])\n",
    "b1 = np.array([0, 0])\n",
    "\n",
    "# 2-3Â±§Èñì„ÅÆ„Éë„É©„É°„Éº„Çø\n",
    "w2 = np.array([[3, 2]])\n",
    "b2 = np.array([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2xT4AnmBXwIb"
   },
   "source": [
    "Here, the following four parameters are defined.\n",
    "\n",
    "**Parameters of linear transformation between layer 1 and layer 2**\n",
    "\n",
    "${\\bf w}_1 \\in \\mathbb{R}^{2 \\times 3}$ : A matrix that converts a 3D vector to a 2D vector\n",
    "\n",
    "${\\bf b}_1 \\in \\mathbb{R}^2$ : Two-dimensional bias vector\n",
    "\n",
    "**Parameters of linear transformation between layer 2 and layer 3**\n",
    "\n",
    "${\\bf w}_2 \\in \\mathbb{R}^{1 \\times 2}$ :A matrix that converts a two-dimensional vector to a one-dimensional vector\n",
    "\n",
    "${\\bf b}_2 \\in \\mathbb{R}^1$ : One-dimensional bias vector\n",
    "\n",
    "Let's actually carry out the calculation of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "d-ELmfkiKQfd",
    "outputId": "ef26b9e1-ecac-4e5c-f69b-d7443a2338ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95257194]\n"
     ]
    }
   ],
   "source": [
    "# ‰∏≠ÈñìÂ±§„ÅÆË®àÁÆó\n",
    "u1 = w1.dot(x) + b1\n",
    "h1 = 1. / (1 + np.exp(-u1))\n",
    "\n",
    "# Âá∫Âäõ„ÅÆË®àÁÆó\n",
    "u2 = w2.dot(h1) + b2\n",
    "y = 1. / (1 + np.exp(-u2))\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8t-YjO8BZ6kU"
   },
   "source": [
    "The output is 0.95257194I asked for it. In other words,$f([2, 3, 1]^T) = 0.95257194$ will be said. Next, I asked for it\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial {\\bf w}_2}\n",
    "= \\frac{\\partial \\mathcal{L}}{\\partial {\\bf y}} \\frac{\\partial {\\bf y}}{\\partial {\\bf u}_2} \\frac{\\partial {\\bf u}_2}{\\partial {\\bf w}_2}\n",
    "$$\n",
    "\n",
    "Let's calculate each of the three partial derivatives on the right side of.Ôºé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8RkBjaaaZ7U8"
   },
   "outputs": [],
   "source": [
    "# dL / dy\n",
    "g_Ly = -2 / 1 * (t - y)\n",
    "\n",
    "# dy / du_2\n",
    "g_yu2 = y * (1 - y)\n",
    "\n",
    "# du_2 / dw_2\n",
    "g_u2w2 = h1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uPmRfu3_aXJG"
   },
   "source": [
    "If you multiply these, you will find the parameter you wanted ${\\bf w}_2$ You can get the gradient about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZrKfXtzfaWk4",
    "outputId": "cc2060bc-b039-465c-b156-cb4b45264c5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.72104507e+00 -1.43112111e-06]\n"
     ]
    }
   ],
   "source": [
    "# dL / dw_2: Ê±Ç„ÇÅ„Åü„ÅÑÂãæÈÖç\n",
    "g_Lw2 = g_Ly * g_yu2 * g_u2w2\n",
    "\n",
    "print(g_Lw2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XN293R5NU1zP"
   },
   "source": [
    "I got the slope. This is $\\partial \\mathcal{L} / \\partial {\\bf w}_2$ Is the value of If this is scaled by the learning rate, the parametersùê∞2Can be updated. Specifically, the update expression is as follows.\n",
    "\n",
    "\n",
    "$$\n",
    "{\\bf w}_2 \\leftarrow {\\bf w}_2 - \\eta \\frac{\\partial \\mathcal{L}}{\\partial {\\bf w}_2}\n",
    "$$\n",
    "\n",
    "Here is the learning rate $\\eta$ Indicated in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HLAwYypPshj8"
   },
   "source": [
    "####  Learning rate for the (Learning Rate) \n",
    "If the learning rate is too high, the value of the objective function may oscillate or diverge while updating parameters repeatedly. If it is too small, convergence will take time. Therefore, it is very important in neural network learning to determine this learning rate properly. In many cases, empirically searching for the largest value that learning proceeds properly is done. For simple image recognition tasks etc., usually 0.1 to 0.01 etc. are relatively often seen first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SC0RoRpucZh9"
   },
   "source": [
    "####  Parameters ${\\bf w}_1$ Of the update amount \n",
    "\n",
    "next, ${\\bf w}_1$ Let's also calculate the update amount of. for that purpose, ${\\bf w}_1$ Objective function at $\\mathcal{L}$ You need a value that is a partial derivative of. This can be calculated as follows.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial {\\bf w}_1}\n",
    "&= \\frac{\\partial \\mathcal{L}}{\\partial {\\bf y}} \\frac{\\partial {\\bf y}}{\\partial {\\bf w}_1} \\\\\n",
    "&=\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial {\\bf y}}\n",
    "\\frac{\\partial {\\bf y}}{\\partial {\\bf u}_2}\n",
    "\\frac{\\partial {\\bf u}_2}{\\partial {\\bf w}_1} \\\\\n",
    "&=\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial {\\bf y}}\n",
    "\\frac{\\partial {\\bf y}}{\\partial {\\bf u}_2}\n",
    "\\frac{\\partial {\\bf u}_2}{\\partial {\\bf h}_1}\n",
    "\\frac{\\partial {\\bf h}_1}{\\partial {\\bf w}_1} \\\\\n",
    "&=\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial {\\bf y}}\n",
    "\\frac{\\partial {\\bf y}}{\\partial {\\bf u}_2}\n",
    "\\frac{\\partial {\\bf u}_2}{\\partial {\\bf h}_1}\n",
    "\\frac{\\partial {\\bf h}_1}{\\partial {\\bf u}_1}\n",
    "\\frac{\\partial {\\bf u}_1}{\\partial {\\bf w}_1}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The first of these five partial derivatives has already been found. The remaining three are\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial {\\bf y}}{\\partial {\\bf u}_2}\n",
    "&= {\\bf y}(1 - {\\bf y}) \\\\\n",
    "\\frac{\\partial {\\bf u}_2}{\\partial {\\bf h}_1}\n",
    "&= {\\bf w}_2 \\\\\n",
    "\\frac{\\partial {\\bf h}_1}{\\partial {\\bf u}_1}\n",
    "&= {\\bf h}_1(1 - {\\bf h}_1) \\\\\n",
    "\\frac{\\partial {\\bf u}_1}{\\partial {\\bf w}_1}\n",
    "&= {\\bf x}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "You can calculate Now, let's actually execute the calculation using NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "v41tSnDkcbPk",
    "outputId": "d6075fb2-5da9-4981-f1b7-d65eaf219fbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.72463398e-04 -2.58695098e-04 -8.62316992e-05]\n",
      " [-5.72447970e-06 -8.58671954e-06 -2.86223985e-06]]\n"
     ]
    }
   ],
   "source": [
    "g_yu2 = y * (1 - y)\n",
    "g_u2h1 = w2\n",
    "g_h1u1 = h1 * (1 - h1)\n",
    "g_u1w1 = x\n",
    "\n",
    "# ‰∏ä„Åã„Çâ du1 / dw1 „ÅÆÁõ¥Ââç„Åæ„Åß„Çí‰∏ÄÊó¶Ë®àÁÆó\n",
    "g_Lu1 = g_Ly * g_yu2 * g_u2h1 * g_h1u1\n",
    "\n",
    "# g_u1w1„ÅØ (3,) „Å®„ÅÑ„ÅÜshape„Å™„ÅÆ„ÅßÔºåg_u1w1[None]„Å®„Åó„Å¶(1, 3)„Å´Â§âÂΩ¢\n",
    "g_u1w1 = g_u1w1[None]\n",
    "\n",
    "# dL / dw_1: Ê±Ç„ÇÅ„Åü„ÅÑÂãæÈÖç\n",
    "g_Lw1 = g_Lu1.T.dot(g_u1w1)\n",
    "\n",
    "print(g_Lw1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Al0CmLzTtyvR"
   },
   "source": [
    "This is $\\partial \\mathcal{L} / \\partial {\\bf w}_1$ Is the value of Using this, ${\\bf w}_2$ Update parameters like the following ${\\bf w}_1$  You can update the.\n",
    "\n",
    "$$\n",
    "{\\bf w}_1 \\leftarrow {\\bf w}_1 - \\eta \\frac{\\partial \\mathcal{L}}{\\partial {\\bf w}_1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zIv_rP32uH1y"
   },
   "source": [
    "## Back propagation method (back propagation) \n",
    "Up to this point, I have experienced that the derivative of the objective function for each parameter is derived by hand calculation and the numerical calculation of the gradient is actually performed. So what happens if the neural network has more layers? Similarly, it is of course possible to obtain the derivative by hand calculation, but it is possible to derive a function which gives a gradient automatically by a computer by using the property that a neural network applies a differentiable function repeatedly. is. Recall that partial derivatives of composite functions can be transformed into the product of multiple partial derivatives by the chain law.\n",
    "\n",
    "The figure below shows the calculation for obtaining the output of the 3-layer fully connected neural network used in the explanation so far, and the process of calculating the value of the objective function using that value with blue arrows, and the hand in the previous section. It is an animation that expresses the process of calculating the partial derivatives of the objective function by each parameter by the calculation with red arrows.\n",
    "\n",
    "![Ë™§Â∑ÆÈÄÜ‰ºùÊí≠Ê≥ï(Backpropagation)„ÅÆË®àÁÆóÈÅéÁ®ã](https://raw.githubusercontent.com/preferred-medicine/medical-ai-course-materials/master/notebooks/images/3/backpropagation.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R8YaTki1Dhhx"
   },
   "source": [
    "First, the output of the objective function $l = \\mathcal{L}({\\bf y}, {\\bf t})$  will do. The round nodes in this figure represent variables, and the square nodes represent functions. Now, the whole neural network that can be seen as one huge synthetic function $f$ In which the function used for the linear transformation between each layer is $f_1$, $f_2$, Non-linear transformation ùëé1, ùëé2It is expressed as. In this case, how can the calculation of the update amount performed in the previous section be understood?\n",
    "\n",
    "Now, as represented by the blue arrow above, a new input  ${\\bf x}$ Is given to the neural network and transmitted to the output side in order, and finally the value of the objective function $l$ Suppose that the calculation is finished. This is called **forward propagation**.\n",
    "\n",
    "Then, next, we would like to find the update amount of each parameter that makes the value of the output of the objective function smaller, but the gradient of the objective function necessary for this is the part beyond the round node of each parameter You can see that it can be calculated only by the gradient of the function at (output side). Specifically, they are all multiplied. That is, as shown by the red arrows in the above figure, if the gradient for the input in each function is determined and multiplied in the **opposite direction** to the **forward propagation** from the output side to the input side, The gradient of the objective function can be calculated.\n",
    "\n",
    "Thus, by using the mechanism of chain rule of differentiation, the purpose of the gradient of a function, for parameters of the function to configure the neural network **so as to follow a path through in forward propagation in a reverse direction** gradient of the course of function The algorithm obtained by multiplication of is called error backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uHGp6qIBcKOI"
   },
   "source": [
    "## Slope \n",
    "\n",
    "When I first mentioned the activation function, the sigmoid function has a problem that the phenomenon of gradient disappearance is likely to occur, and explained that it is not used so much at present. Let's look more closely at the reasons.\n",
    "\n",
    "Recall the derivative of the sigmoid function that you have already calculated above.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f\\left( u\\right) &=\\dfrac {1}{1+e^{-u}} \\\\\n",
    "f'\\left( u\\right) &= f\\left( u\\right) \\left( 1-f\\left( u\\right) \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now, when plotting the value of this derivative with respect to the input variable, it becomes as follows.\n",
    "\n",
    "![„Ç∑„Ç∞„É¢„Ç§„ÉâÈñ¢Êï∞„ÅÆÂ∞éÈñ¢Êï∞](https://raw.githubusercontent.com/preferred-medicine/medical-ai-course-materials/master/notebooks/images/3/09.png)\n",
    "\n",
    "\n",
    "The top two in this figure are the two parts that make up the derivative $f(u)$ When $1 - f(u)$ Plotted separately for the values of, the lower middle figure is the actual derivative value. If you look at the shape of the derivative in the lower center of the upper figure, you can see that the value of the gradient decreases gradually as the input goes away from the origin and asymptotically approaches 0.\n",
    "\n",
    "In order to obtain the update amount of each parameter, it was necessary **to multiply the gradients of all functions ahead of that parameter** , as described in the previous section . At this time, if a sigmoid function is used for the activation function, the gradient must have a value of **at most 0.25** at most. Then, each time a linear transformation appears in the computation graph, the slope of the objective function is multiplied by at most 0.25. This is because as the number of layers increases, the maximum value of 0.25 is repeatedly multiplied, so the gradient flowing to the layer closer to the input approaches 0 more and more.\n",
    "\n",
    "Let's look at a concrete example. This time, the explanation was made using a three-layer neural network, but let's consider the case of four layers. Then, the gradient of the parameter of the linear transformation closest to the input is at most the gradient of the objective function. $0.25 \\times 0.25 = 0.0625$ It will be doubled. It is clear that the slope decreases exponentially as the number of layers increases.\n",
    "\n",
    "In deep learning, a neural network is used in which more than four layers are stacked. Then, when the sigmoid function is used as the **activation function, the gradient of the objective function is almost completely not transmitted to the parameters of the function close to the input, and when** only a very small gradient is transmitted, the parameter update amount is almost Because it is 0, the parameters of functions close to the input layer will not change no matter how large the objective function is. In other words, the value hardly changes from the time of initialization, which means that learning is not being performed. This **gradient loss (vanishing gradient** is referred to as, was a long deep (more than a dozen layers) one of the factors learning is difficult in the neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IRvZlChZ9Qna"
   },
   "source": [
    "## What \"layer\" points \n",
    "In the explanation up to this point, the number of round nodes (values such as intermediate output) in the graph as shown in the explanation of the error back propagation method of the last two clauses indicates the neural network of ‚óã layer (‚óã -layer), It has been said that. However, in some cases, the square nodes (functions) in this graph are referred to as layers. And in frameworks used to implement neural networks, various functions are often organized as layer types. So, be careful to use the words \"layer\" or \"layer\" for this function below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7dD2mgCOTvdW"
   },
   "source": [
    "## Various layer \n",
    "\n",
    "Up to this point, it is explained that the neural network includes two kinds of functions, roughly divided into a function that performs linear transformation and a function that performs nonlinear transformation, and as a linear transformation example, only **fully-connected layer** I used it.\n",
    "\n",
    "However, layers that can be used as components of neural networks are not limited to all connection layers and activation functions. There are various tasks such as image recognition, image generation, image conversion, super-resolution (a technique for creating an image with high resolution from low-resolution images), and so on. In the task, the convolution layer, which is often a transformation that is compatible with the characteristics of the data format of the image, carries a linear transformation in the neural network instead of the full connection layer.\n",
    "\n",
    "Also, layers that are components of neural networks can be used with any differentiable function. Therefore, in addition to this convolutional layer, a layer called a pooling layer has often been used in a type of network architecture called so-called **convolutional neural networks (CNN)** .\n",
    "\n",
    "This section outlines the calculations for the convolution and pooling layers often used in this CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w-3rsIzXpCBo"
   },
   "source": [
    "### 3.7.1. Convolution layer \n",
    "\n",
    "In the total connection layer, the parameter matrix for calculating the output ${\\bf W}$ Every single value in the input data was multiplied by all elements of the input data (if it is a vector, it is the value of all dimensions). In other words, if \"join\" refers to a relation in which some operation is performed, the parameter matrix ${\\bf W}$ of $i, j$ Is an element $W_{ij}$  And the input data ${\\bf x}$ It means that all elements of were completely connected (fully connected).\n",
    "\n",
    "On the other hand, in the convolutional layer, unlike in the all coupling layer described above, the coupling between parameters and input data is local, and each parameter is coupled to all elements of the input data. There is no limit. Specifically, in the case of a two-dimensional convolutional layer, the parameters are prepared as a set of small image patches or the like called kernels (or filters), and for each patch, an operation between part of the input data and It is in the form of being done.\n",
    "\n",
    "Using the figure below, $3 \\times 3$ Let's examine the actual calculation process of a convolution layer with two convolution kernels such as small image patches of size.\n",
    "\n",
    "![Áï≥„ÅøËæº„ÅøÂ±§„ÅÆË®àÁÆóÈÅéÁ®ã](https://raw.githubusercontent.com/preferred-medicine/medical-ai-course-materials/master/notebooks/images/3/convolution.gif)\n",
    "\n",
    "ÔºàFigure is taken from [CS231n Convolutional Neural Networks for Visual Recognition\n",
    "](http://cs231n.github.io/convolutional-networks/)Ôºâ\n",
    "\n",
    "\n",
    "The leftmost row of blue blocks represents the input image. Input width and height $5 \\times 5$ It is a 3-channel image of the size of. In the figure above, first, processing called padding is performed to add an area with a value of 0 around the input image. This time we pad the area of width 1.\n",
    "\n",
    "The middle two columns each represent one convolution kernel per column. One convolution kernel has as many channels as the input image (or the output of the previous layer called feature map) has. Since the input image is 3 channels here, each kernel has a value for 3 channels. Three reds, which are arranged vertically $3 \\times 3$It is represented by a block of size. Each channel in each kernel performs calculations in a manner that slides itself to the corresponding input channel. The slide width at this time is called stride. In the figure, the input image is $3 \\times 3$ You can see how the frame of has moved. That is, the stride is set to 2. The calculations performed here are overlapping when the kernel is superimposed at a certain position of the input $3 \\times 3 = 9$ It is a calculation of multiplying the elements by each other and adding all the results. In the figure above, for example, the first channel of the first kernel (W0) is\n",
    "\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "1 & 1 & -1 \\\\\n",
    "0 & -1 & 0 \\\\\n",
    "1 & -1 & -1\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "From the top left of the first channel of the corresponding input image $3 \\times 3$ In the small area of\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "0 & 2 & 0\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "The values are listed. Therefore, when the overlapping values are multiplied,Ôºå\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "0 & -2 & 0\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "It turns out that it becomes. The sum of all nine values is the first calculation result of the first channel of this kernel. Calculated, the result is -2. If this is done for the second channel and the third channel in the same way, the second channel will be calculated as 0, and the third channel will be 1. And finally add the results of all the channels. The result is -1. If you add the value of bias prepared for each kernel (the bias of the first kernel is 1 in the above figure), the output result for the upper left corner of the input of this kernel is obtained. The result is 0. The green blocks in the rightmost column represent the output of this convolutional layer, and looking at the top left box of the first block you can see that it is indeed zero.\n",
    "\n",
    "Similarly, try calculating some of the other mass values yourself.\n",
    "\n",
    "It seems at first glance complicated to think of the derivative of this layer, but since the operations performed using individual parameter values are only weighted and added bias , in principle it is only a simple linear transformation From the construction it can be seen that it is possible to differentiate this transformation with respect to parameters and inputs. In fact, the kernel size is $1 \\times 1$ A convolution operation using a kernel of is equivalent to performing a linear transformation using the same weight matrix for each input dimension. For example, the width and height of the input $W \\times H$ Kernel size for 3 channels $1 \\times 1$ The kernel of the convolutional layer of $1 \\times 1 \\times 3$ Becomes a one-dimensional scalar, $W \\times H$ Apply this tensor and scalar linear transformation to all input dimensions.\n",
    "\n",
    "Thus, the convolutional layer can handle various data formats by defining kernels in various ways. A one-dimensional kernel can be applied to one-dimensional series data, etc. A three-dimensional kernel can be applied to data such as video and voxels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SUEmd9pOKe2D"
   },
   "source": [
    "### Pooling layer \n",
    "\n",
    "Pooling is an operation mainly performed on the feature map to reduce the size of the spatial dimension of the feature map (spatial dimension, the dimension corresponding to the width and height in the input image), and reduce the amount of calculation, It is used to improve robustness in image recognition tasks etc. by making the output invariant for minute translation.\n",
    "\n",
    "The pooling calculation is similar to convolution. However, there is no parameter, and the part of the calculation performed using the convolution kernel has been replaced with the calculation of the average or the maximum value for the corresponding input subregion. The pooling that calculates the average value for each partial area is called average pooling (average pooling), and the pooling that calculates the maximum value is called maximum pooling (max pooling).\n",
    "\n",
    "In the left part of the figure below, downsampling (reducing the resolution) using the pooling layer $224 \\times 224$  Of size,$64$ Apply to the channel's feature map, and $112 \\times 112$ Size map). At this time, the number of channels (also called depth) is maintained.\n",
    "\n",
    "The right part of this figure is one of the 64 channels on the left side of the rectangular solid, the dark $4 \\times 4$ In the partial representation of the part of $2 \\times 2$ The figure shows the result when the maximum value is calculated for each area and maximum value pooling, which is the representative value of the area, is performed while shifting the area by two (with stride 2).\n",
    "\n",
    "![„Éó„Éº„É™„É≥„Ç∞Â±§„ÅÆË®àÁÆóÈÅéÁ®ã](https://raw.githubusercontent.com/preferred-medicine/medical-ai-course-materials/master/notebooks/images/3/pooling.png)\n",
    "\n",
    "ÔºàFigure is taken from [CS231n Convolutional Neural Networks for Visual Recognition\n",
    "](http://cs231n.github.io/convolutional-networks/)Ôºâ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Introduction to Neural Network",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
