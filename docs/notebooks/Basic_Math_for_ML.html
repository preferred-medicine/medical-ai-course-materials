

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>1. Basis of the mathematics required to machine learning &mdash; „É°„Éá„Ç£„Ç´„É´AIÂ∞ÇÈñÄ„Ç≥„Éº„Çπ „Ç™„É≥„É©„Ç§„É≥Ë¨õÁæ©Ë≥áÊñô  documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="2. Basics of machine learning" href="Introduction_to_ML_libs.html" />
    <link rel="prev" title="„É°„Éá„Ç£„Ç´„É´AIÂ∞ÇÈñÄ„Ç≥„Éº„Çπ „Ç™„É≥„É©„Ç§„É≥Ë¨õÁæ©Ë≥áÊñô" href="../index.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-797798-11"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'UA-797798-11');
  </script>

  <meta name="description" content="„É°„Éá„Ç£„Ç´„É´AIÂ≠¶‰ºöÂÖ¨Ë™çË≥áÊ†ºÂêë„Åë„Ç™„É≥„É©„Ç§„É≥Ë¨õÁæ©Ë≥áÊñô„ÄÇÊ©üÊ¢∞Â≠¶Áøí„Å´ÂøÖË¶Å„Å™Êï∞Â≠¶„ÅÆÂü∫Á§é„ÅÆËß£Ë™¨„Åã„ÇâÊ∑±Â±§Â≠¶ÁøíÔºà„Éá„Ç£„Éº„Éó„É©„Éº„Éã„É≥„Ç∞Ôºâ„ÇíÁî®„ÅÑ„ÅüÂÆüË∑µÁöÑ„Å™ÂÜÖÂÆπ„Åæ„ÅßGoogle Colaboratory‰∏ä„ÅßGPU„ÇíÁî®„ÅÑ„Å¶ÂÆüÈöõ„Å´„Ç≥„Éº„Éâ„ÇíÂÆüË°åÂèØËÉΩ„Å™ÂΩ¢Âºè„Å´„Åó„Ç™„É≥„É©„Ç§„É≥Ë≥áÊñô„Å®„Åó„Å¶ÁÑ°ÊñôÂÖ¨Èñã„ÄÇ">
  <meta property="og:title" content="„É°„Éá„Ç£„Ç´„É´AIÂ∞ÇÈñÄ„Ç≥„Éº„Çπ „Ç™„É≥„É©„Ç§„É≥Ë¨õÁæ©Ë≥áÊñô">
  <meta property="og:description" content="„É°„Éá„Ç£„Ç´„É´AIÂ≠¶‰ºöÂÖ¨Ë™çË≥áÊ†ºÂêë„Åë„Ç™„É≥„É©„Ç§„É≥Ë¨õÁæ©Ë≥áÊñô„ÄÇÊ©üÊ¢∞Â≠¶Áøí„Å´ÂøÖË¶Å„Å™Êï∞Â≠¶„ÅÆÂü∫Á§é„ÅÆËß£Ë™¨„Åã„ÇâÊ∑±Â±§Â≠¶ÁøíÔºà„Éá„Ç£„Éº„Éó„É©„Éº„Éã„É≥„Ç∞Ôºâ„ÇíÁî®„ÅÑ„ÅüÂÆüË∑µÁöÑ„Å™ÂÜÖÂÆπ„Åæ„ÅßGoogle Colaboratory‰∏ä„ÅßGPU„ÇíÁî®„ÅÑ„Å¶ÂÆüÈöõ„Å´„Ç≥„Éº„Éâ„ÇíÂÆüË°åÂèØËÉΩ„Å™ÂΩ¢Âºè„Å´„Åó„Ç™„É≥„É©„Ç§„É≥Ë≥áÊñô„Å®„Åó„Å¶ÁÑ°ÊñôÂÖ¨Èñã„ÄÇ">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://japan-medical-ai.github.io/medical-ai-course-materials/">
  <meta property="og:image" content="https://raw.githubusercontent.com/japan-medical-ai/medical-ai-course-materials/master/notebooks/images/medical_ai.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@PreferredNetJP">
  <meta name="twitter:creator" content="@PreferredNetJP">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> „É°„Éá„Ç£„Ç´„É´AIÂ∞ÇÈñÄ„Ç≥„Éº„Çπ „Ç™„É≥„É©„Ç§„É≥Ë¨õÁæ©Ë≥áÊñô
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">1. Basis of the mathematics required to machine learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#What-is-machine-learning?">1.1. What is machine learning?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Derivative">1.2. Derivative</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Slope-of-a-straight-line-passing-through-two">1.2.1. Slope of a straight line passing through two</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Slope-of-tangent-at-one-point">1.2.2. Slope of tangent at one point</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Differentiation">1.2.3. Differentiation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Derivative-of-the-synthetic-function">1.2.4. Derivative of the synthetic function</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Partial-derivative">1.2.5. Partial derivative</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Linear">1.3. Linear</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#What-is-linear-algebra?">1.3.1. What is linear algebra?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Scalar,-vector,-matrix,">1.3.2. Scalar, vector, matrix,</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Addition,-subtraction">1.3.3. Addition, subtraction</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Dot-inner">1.3.4. Dot inner</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Multiplication-(matrix-multiplication)">1.3.5. Multiplication (matrix multiplication)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Transpose">1.3.6. Transpose</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Vector,-size-of-matrix">1.3.7. Vector, size of matrix</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Unit-matrix">1.3.8. Unit matrix</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Inverse-matrix">1.3.9. Inverse matrix</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Linear-joins-and-quadratic">1.3.10. Linear joins and quadratic</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Differential-by-the-vector-and-gradient">1.3.11. Differential by the vector and gradient</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Probability-and-Statistics">1.4. Probability and Statistics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#What-are-probability-and-statistics-available-for?">1.4.1. What are probability and statistics available for?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Probability">1.4.2. Probability</a></li>
<li class="toctree-l3"><a class="reference internal" href="#The-likelihood-and-maximum-likelihood-estimation">1.4.3. The likelihood and maximum likelihood estimation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Posterior-probability-maximization-estimation-(MAP-estimation)">1.4.4. Posterior probability maximization estimation (MAP estimation)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Statistics">1.4.5. Statistics</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Normal-distribution-and-the-normalized">1.4.6. Normal distribution and the normalized</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Scaling-using-the-standard-deviation">1.4.7. Scaling using the standard deviation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Outlier-removal">1.4.8. Outlier removal</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Introduction_to_ML_libs.html">2. Basics of machine learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Introduction_to_Neural_Network.html">3. Basics of neural network</a></li>
<li class="toctree-l1"><a class="reference internal" href="Introduction_to_Chainer.html">4. Introduction to Deep Learning Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="Image_Segmentation.html">5. Practice: Segmentation of MRI</a></li>
<li class="toctree-l1"><a class="reference internal" href="Blood_Cell_Detection.html">6. Practice section: Detection of cells from microscope images of blood</a></li>
<li class="toctree-l1"><a class="reference internal" href="DNA_Sequence_Data_Analysis.html">7. Practical part: sequence analysis using deep learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Sequential_Data_Analysis_with_Deep_Learning.html">8. Practical part: Time series analysis of monitoring data using the deep learning</a></li>
</ul>

            
          
          <div style="padding-right:20px; bottom:10px;">
            <a href="https://short-term.kikagaku.co.jp/dnn-seminar/">
              <img src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/img_handson.png" />
              <p style="padding:5px; font-size:small; line-height: 150%">„Éá„Ç£„Éº„Éó„É©„Éº„Éã„É≥„Ç∞„ÅÆË©≥„Åó„ÅÑËß£Ë™¨„ÇÑÁîªÂÉè„ÉªËá™ÁÑ∂Ë®ÄË™û„ÅÆÂèñ„ÇäÊâ±„ÅÑ„ÄÅ„ÇØ„É©„Ç¶„Éâ‰∏ä„ÅÆGPU„Çí‰Ωø„Å£„ÅüÂÆüË∑µÁöÑ„Å™ÊºîÁøí„Çí„ÅîÂ∏åÊúõ„ÅÆÊñπ„ÅØ„Åì„Å°„Çâ„Åå„Åä„Åô„Åô„ÇÅ„Åß„Åô</p>
            </a>
          </div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">„É°„Éá„Ç£„Ç´„É´AIÂ∞ÇÈñÄ„Ç≥„Éº„Çπ „Ç™„É≥„É©„Ç§„É≥Ë¨õÁæ©Ë≥áÊñô</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>1. Basis of the mathematics required to machine learning</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/notebooks/Basic_Math_for_ML.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Basis-of-the-mathematics-required-to-machine-learning">
<h1>1. Basis of the mathematics required to machine learning<a class="headerlink" href="#Basis-of-the-mathematics-required-to-machine-learning" title="Permalink to this headline">¬∂</a></h1>
<p>In this chapter, we will briefly introduce three of the fundamentals of mathematics necessary for machine learning including deep learning, ‚Äúdifferential‚Äù ‚Äúlinear algebra‚Äù ‚Äúprobability / statistics‚Äù.</p>
<div class="section" id="What-is-machine-learning?">
<h2>1.1. What is machine learning?<a class="headerlink" href="#What-is-machine-learning?" title="Permalink to this headline">¬∂</a></h2>
<p>Machine learning is a method of acquiring a function that extracts patterns such as rules and judgment criteria contained in the data by computer learning from data, and predicting new data using that function. Machine learning technology is now applied to a wide range of fields such as image recognition, speech recognition, document classification, medical diagnosis, spam mail detection, product recommendation.</p>
<p>Here, (a function that is acquired by the learning <strong>model</strong> will be called) is often <strong>parameters</strong> have been characterized by a determines the behavior of the function if determined parameters. Considering the function of a straight line as the simplest example, this is a gradient <span class="math notranslate nohighlight">\(a\)</span> and a cut point <span class="math notranslate nohighlight">\(b\)</span>It is characterized by two parameters,<span class="math notranslate nohighlight">\(f(x; a, b) = ax + b\)</span> I will write like this. (here <span class="math notranslate nohighlight">\(x\)</span> is called the input variable of the function . Also <span class="math notranslate nohighlight">\(Ôºõ\)</span>
Parameters are written after the). The goal of machine learning is to determine these parameters using learning data.</p>
<p>Machine learning determines parameters that will learn, that is, make the desired behavior by minimizing (or maximizing) the <strong>objective function</strong> . Therefore, the objective function is designed to take a small (or large) value if the output value of the model is desired, and a large (or small) value if not.</p>
<p>For example, as a learning data, a data set composed of input and output pairs <span class="math notranslate nohighlight">\(D=\left((x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n) \right)\)</span> You are given. Here,<span class="math notranslate nohighlight">\(x_{i}\)</span> The input of the second sample,<span class="math notranslate nohighlight">\(y_{i}\)</span> Represents the output of the second sample. A straight line that passes as close as possible to these points <span class="math notranslate nohighlight">\(f(x; a, b) = ax + b\)</span> You want to learn. If the output is a real value, the parameter <span class="math notranslate nohighlight">\(\theta = (a, b)\)</span> Then, the following objective function is
often used.</p>
<p><span class="math notranslate nohighlight">\(L( \theta) = \sum_{i=1}^n (y_i - f(x_i; \theta))^2\)</span></p>
<p>Consider minimizing this function. In the above equation, the predicted value of the model <span class="math notranslate nohighlight">\(f(x_i; \theta)\)</span> And the correct answer <span class="math notranslate nohighlight">\(y_i\)</span> We calculate the total value by finding the square error between and. Only when prediction and correct answer are consistent with all data <span class="math notranslate nohighlight">\(0\)</span> Otherwise, it takes a large positive value as it makes a big mistake. Functions that measure the degree of mistake are sometimes called <strong>loss functions</strong> in particular . Further, purposes such as
obtaining the total value of the penalty for the entire data given set function <strong>cost function</strong> sometimes also called. The argument of the objective function is <span class="math notranslate nohighlight">\(\theta\)</span> It is optimal to minimize the objective function <span class="math notranslate nohighlight">\(\theta\)</span>, The data set <span class="math notranslate nohighlight">\(D\)</span> Function to predict accurately<span class="math notranslate nohighlight">\(f(x; \theta)\)</span>Will be obtained</p>
<p>In order to solve the problem of minimizing the objective function, knowledge of differentiation and linear algebra is required. However, knowledge of all differentiation and linear algebra is not necessary. After that, I will explain the minimum knowledge necessary for understanding machine learning.</p>
</div>
<div class="section" id="Derivative">
<h2>1.2. Derivative<a class="headerlink" href="#Derivative" title="Permalink to this headline">¬∂</a></h2>
<p>Differentiation in the input value of the function corresponds to <strong>the slope</strong> of the <strong>tangent</strong> of the graph at that point and can be expressed as a straight line that contacts the function as shown below.</p>
<p><img alt="image0" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/01.png" /></p>
<p>For example, in the function shown above,<span class="math notranslate nohighlight">\(a\)</span> The tangent line at the point of <span class="math notranslate nohighlight">\(+3\)</span> It has become. The inclination of the straight line rising to the right is a positive value.</p>
<p><img alt="image1" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/02.png" /></p>
<p>On the other hand, in the figure below <span class="math notranslate nohighlight">\(b\)</span> , The slope is -1, and the tangent line is a straight downward sloping straight line.</p>
<p>If the value of the objective function can be calculated for all parameters, it is possible to select the minimum value of the objective function from among them, but such is usually impossible. However, if you can calculate the derivative with respect to parameters at a certain point, you can determine the slope of the tangent and you can see how the objective function changes when changing the parameters, even if you do not know the graph shape across the parameter. Based on this information,
you can update the parameters to reduce the objective function.</p>
<p>Returning to the explanation of differentiation again, I will look in detail about its definition, multivariable input, and multivariable output.</p>
<div class="section" id="Slope-of-a-straight-line-passing-through-two">
<h3>1.2.1. Slope of a straight line passing through two<a class="headerlink" href="#Slope-of-a-straight-line-passing-through-two" title="Permalink to this headline">¬∂</a></h3>
<p>First, in order to understand the principle of differentiation, the slope of a straight line passing through two points shown in the figure below <span class="math notranslate nohighlight">\(a\)</span> Let‚Äôs ask.</p>
<p><img alt="image2" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/04.png" /></p>
<p>At this time, the inclination <span class="math notranslate nohighlight">\(a\)</span> ,</p>
<div class="math notranslate nohighlight">
\[a = \dfrac{f(x_{2}) - f(x_{1})}{x_{2}-x_{1}}\]</div>
<p>You can get.</p>
</div>
<div class="section" id="Slope-of-tangent-at-one-point">
<h3>1.2.2. Slope of tangent at one point<a class="headerlink" href="#Slope-of-tangent-at-one-point" title="Permalink to this headline">¬∂</a></h3>
<p>Next, we will find the slope of the tangent of the given function. To do that, the <strong>limit</strong> idea is necessary. In the limit, when the variable approaches a certain value as close as possible, we will consider how the function described by that variable behaves. In order to express the limit,<span class="math notranslate nohighlight">\(\lim\)</span> The symbol ‚Äòcommon‚Äô is used. For example,</p>
<div class="math notranslate nohighlight">
\[\displaystyle \lim _{x\rightarrow 0}3x=3\times 0=0\]</div>
<p><span class="math notranslate nohighlight">\(x\)</span> The variable <span class="math notranslate nohighlight">\(0\)</span> The value of the expression will be given when approaching it.</p>
<p>Then, some point in the figure below <span class="math notranslate nohighlight">\(x\)</span> Slope of tangent at <span class="math notranslate nohighlight">\(a\)</span> Let‚Äôs seek for.</p>
<p><img alt="image3" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/05.png" /></p>
<p>Tangent can be obtained by combining the straight line passing through the two points I thought earlier and the limit.</p>
<p><img alt="image4" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/06.png" /></p>
<p>First, <span class="math notranslate nohighlight">\(x\)</span> From <span class="math notranslate nohighlight">\(h\)</span> Only a point away <span class="math notranslate nohighlight">\(x+h\)</span> Consider the slope of the straight line passing through the two points. next <span class="math notranslate nohighlight">\(h\)</span> to <span class="math notranslate nohighlight">\(h \rightarrow 0\)</span> If you make it as small as you can, the starting point and ending point of the line converge to 1 point and you can think of it as a tangent at 1 point. Looking at this expression</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
a &amp;=\lim _{h\rightarrow 0}\dfrac {f\left( x+h\right) -f\left( x\right) }{\left( x+h\right) -x}\\
&amp;=\lim _{h\rightarrow 0}\dfrac {f\left( x+h\right) -f\left( x\right) }{h}\\
\end{aligned}\end{split}\]</div>
<p>The above equation is called a <strong>derivative</strong> ,ùëì‚Ä≤(ùë•)It is represented by. It is said to <strong>differentiate</strong> finding <strong>derivatives</strong>. Also, as a way of using the symbols,</p>
<div class="math notranslate nohighlight">
\[(\cdot)' = \dfrac{d}{dx}(\cdot)\]</div>
<p>It can be expressed as follows. this <span class="math notranslate nohighlight">\(d\)</span> The symbol ‚Äúdifferentiation‚Äù represents,<span class="math notranslate nohighlight">\(d(\cdot)\)</span>Is the change in the value of the object, <span class="math notranslate nohighlight">\(dx\)</span> But <span class="math notranslate nohighlight">\(x\)</span> It shows the amount of change and expresses the limit when decreasing them. This notation is cumbersome, but the variable <span class="math notranslate nohighlight">\(x\)</span>Ôºå<span class="math notranslate nohighlight">\(y\)</span> If there are multiple such as,<span class="math notranslate nohighlight">\(x\)</span> Whether differentiating,<span class="math notranslate nohighlight">\(y\)</span> It becomes clear whether to differentiate, so you can express accurately.</p>
</div>
<div class="section" id="Differentiation">
<h3>1.2.3. Differentiation<a class="headerlink" href="#Differentiation" title="Permalink to this headline">¬∂</a></h3>
<p>There are some useful formulas for differentiation to keep in mind, so I will introduce some of them below (<span class="math notranslate nohighlight">\(c\)</span>Is a constant,<span class="math notranslate nohighlight">\(x\)</span> Represents a variable).</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\left( c\right) ^{'}&amp;=0 \\
\left( x\right)^{'}&amp;=1\\
\left( cf(x) \right)^{'} &amp;= c f'(x) \\
\left( x^{n} \right)^{'} &amp;=nx^{n-1} \\
\left( f(x) + g(x) \right) ^{'} &amp;=f^{'}(x)+g^{'}(x) \\
\left( f(x) g(x) \right) ^{'} &amp;= f^{'}(x)g(x) + f(x)g^{'}(x) \\
\left( f(g(x)) \right) ^{'} &amp;= \frac{df(u)}{du}\frac{du}{dx}, u = g(x)
\end{align}\end{split}\]</div>
<p>For example, consider the following differentiation.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\left( 3x^{2} + 4x + 5 \right)' &amp;= \left( 3x^{2} \right)' + \left( 4x \right)' + \left( 5 \right)' \\
&amp;= 3 \times \left( x^{2} \right)' + 4 \times \left( x \right)' + 5 \times \left( 1 \right)' \\
&amp;= 3 \times 2x + 4 \times 1 + 5 \times 0  \\
&amp;= 6x + 4
\end{aligned}\end{split}\]</div>
<p>n this way, the derivation of each term with respect to the sum is differentiated with respect to each term and even if it is summed, the relationship of equality is established. Also, when differentiating each term, the coefficients of the constant (the number of variables) can be placed outside the differential operation. These are the properties called <strong>linearity</strong> of differential , and by using this property, we can easily calculate the differentiation.</p>
</div>
<div class="section" id="Derivative-of-the-synthetic-function">
<h3>1.2.4. Derivative of the synthetic function<a class="headerlink" href="#Derivative-of-the-synthetic-function" title="Permalink to this headline">¬∂</a></h3>
<p>In detail later in the chapter, in general machine learning it is necessary to consider the <strong>differentiation</strong> of complicated <strong>synthetic functions</strong> . As a simple example,</p>
<div class="math notranslate nohighlight">
\[\left\{ (3x + 4)^{2} \right\}'\]</div>
<p>Consider to calculate. In this equation,<span class="math notranslate nohighlight">\(3x+4\)</span> And the inner part <span class="math notranslate nohighlight">\((\cdot)^{2}\)</span> It is composed of the outside part. This equation <span class="math notranslate nohighlight">\((9x^2 + 24x + 16)'\)</span>‚Ä≤It is OK to calculate the derivative after it has been expanded, but it will be hard to develop it as it becomes the third power or the fourth power. The useful idea here is the differentiation of the composite function. It is an expression that appeared at the end of the differentiation formula introduced earlier.
Differentiation of the composite function can be obtained by doing inner differentiation and outer differentiation and multiplying the result with each other. When external differentiation, argument of function is regarded as input and differentiation is taken on its input.</p>
<p>Well then,<span class="math notranslate nohighlight">\((3x+4)^2\)</span>) Consider the differentiation of the function.</p>
<p>First, inside functions ùë¢=(3ùë•+4) And, First, inside functions <span class="math notranslate nohighlight">\(u = (3x+4)\)</span> And,</p>
<div class="math notranslate nohighlight">
\[\left\{ (3x + 4)^{2} \right\}' = (u^{2})'\]</div>
<p>Will do. here,<span class="math notranslate nohighlight">\((\cdot)'\)</span> We need to think more strictly. now,<span class="math notranslate nohighlight">\(x\)</span> When <span class="math notranslate nohighlight">\(u\)</span> Two variables have appeared,<span class="math notranslate nohighlight">\((\cdot)'\)</span> Then,<span class="math notranslate nohighlight">\(x\)</span> Differentiating with <span class="math notranslate nohighlight">\(u\)</span> You can not distinguish whether you are differentiating with. So, it looks somewhat complicated, but as I mentioned earlier <span class="math notranslate nohighlight">\(d\)</span> If you strictly describe the variable to be differentiated by the notation using</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\left\{ (3x + 4)^{2} \right\}' &amp;= \dfrac{d}{dx} \left\{ (3x + 4)^{2} \right\} \\
&amp;= \dfrac{du}{dx} \dfrac{d}{du} (u^2) \\
&amp;= \dfrac{d}{du} (u^{2}) \cdot \dfrac{d}{dx} (3x + 4) \\
&amp;= 2u \cdot 3 \\
&amp;= 6u = 6(3x + 4) = 18x + 24 \\
\end{aligned}\end{split}\]</div>
<p>Learn this calculation method firmly as scenes using derivative of composite function appear many times in learning of neural network.</p>
</div>
<div class="section" id="Partial-derivative">
<h3>1.2.5. Partial derivative<a class="headerlink" href="#Partial-derivative" title="Permalink to this headline">¬∂</a></h3>
<p>In machine learning, one input variable <span class="math notranslate nohighlight">\(x\)</span> Output variable from <span class="math notranslate nohighlight">\(y\)</span> The case of predicting is rare, basically it is possible to use multiple input variables <span class="math notranslate nohighlight">\(x_{1}\)</span>, <span class="math notranslate nohighlight">\(x_{2}\)</span>, <span class="math notranslate nohighlight">\(\ldots\)</span>, <span class="math notranslate nohighlight">\(x_{M}\)</span> Output variable using <span class="math notranslate nohighlight">\(y\)</span> It handles multivariable functions that predicts. For example, when predicting rent, it is expected that it is more accurate to predict not only the size of the room, but also the distance from the station and the rate of crime
occurrence. Multiple inputs <span class="math notranslate nohighlight">\(x_1, x_2, \ldots, x_M\)</span> A function that takes into consideration <span class="math notranslate nohighlight">\(f(x_1, x_2, \ldots, x_M)\)</span> Is called a multivariable function. In this multivariable function,<span class="math notranslate nohighlight">\(x_m\)</span> Differentiating by focusing attention only on <strong>partial derivatives</strong> is called <strong>partial differentiation</strong> ,</p>
<div class="math notranslate nohighlight">
\[\dfrac{\partial}{\partial x_{m}} f(x_{1}, x_{2}, \ldots, x_{M})\]</div>
<p>It is expressed as follows. The symbol representing the differentiation, <span class="math notranslate nohighlight">\(d\)</span> From <span class="math notranslate nohighlight">\(\partial\)</span> As a calculation, <span class="math notranslate nohighlight">\(\dfrac{\partial}{\partial x_{m}}\)</span> In the case of <span class="math notranslate nohighlight">\(x_{m}\)</span> Think of it as a constant except for <span class="math notranslate nohighlight">\(x_{m}\)</span> We focus only on the differentiation. (However, if the input variable is not independent of other input variables, it can not be thought of as a constant.There is no such case in this lecture.)</p>
<p>Let‚Äôs check the flow of concrete calculation by example.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\dfrac {\partial }{\partial x_{1}}\left( 3x_{1}+4x_{2}\right) &amp;=\dfrac {\partial }{\partial x_{1}}\left( 3x_{1}\right) +\dfrac {\partial }{\partial x_{1}}\left( 4x_{2}\right) \\
&amp;=3\times \dfrac {\partial }{\partial x_{1}}\left( x_{1}\right) +4x_{2}\times \dfrac {\partial }{\partial x_{1}}\left( 1\right) \\
&amp;=3\times 1+4x_{2}\times 0\\
&amp;= 3
\end{aligned}\end{split}\]</div>
<p>Even partial derivatives can apply the same formulas as differentiation. In this case,<span class="math notranslate nohighlight">\(x_{1}\)</span> To focus only on,<span class="math notranslate nohighlight">\(x_{2}\)</span> If you grasp that it treats it as a constant, you should be able to understand the flow of the above calculation.</p>
</div>
</div>
<div class="section" id="Linear">
<h2>1.3. Linear<a class="headerlink" href="#Linear" title="Permalink to this headline">¬∂</a></h2>
<div class="section" id="What-is-linear-algebra?">
<h3>1.3.1. What is linear algebra?<a class="headerlink" href="#What-is-linear-algebra?" title="Permalink to this headline">¬∂</a></h3>
<p>Next, I will explain about <strong>linear algebra</strong> . Vectors, matrices, inverse matrices etc. will appear.</p>
<p>Introducing linear algebra makes it possible to simply describe the relationship between multiple variables, so it will appear frequently in machine learning. It is a very important concept, so let‚Äôs definitely wear it.</p>
</div>
<div class="section" id="Scalar,-vector,-matrix,">
<h3>1.3.2. Scalar, vector, matrix,<a class="headerlink" href="#Scalar,-vector,-matrix," title="Permalink to this headline">¬∂</a></h3>
<p>We first explain four scalars, vectors, matrices, and tensors used in linear algebra.</p>
<p><strong>A scalar</strong> is a value or variable. For example,</p>
<div class="math notranslate nohighlight">
\[x, \ y,\  M,\  N\]</div>
<p>It is expressed as follows. A scalar is used to represent a single quantity such as temperature or height.</p>
<p><strong>A vector</strong> is a collection of a plurality of scalars arranged in a vertical direction (or a horizontal direction)</p>
<div class="math notranslate nohighlight">
\[\begin{split}\boldsymbol{x}=\begin{bmatrix}
x_{1} \\
x_{2} \\
x_{3}
\end{bmatrix}, \
\boldsymbol{y}=\begin{bmatrix}
y_{1} \\
y_{2} \\
\vdots \\
y_{N}
\end{bmatrix}\end{split}\]</div>
<p>It is expressed as follows. The notation of a vector is often expressed in boldface, so it can distinguish between scalar and vector. When expressing a vector, columns aligned in the vertical direction are called column vectors, and those arranged in the horizontal direction are called row vectors. Since there are many papers and reference books that use column vectors in mathematics and machine learning, unless otherwise specified, we will refer to column vectors when expressed simply as
vectors.</p>
<p>A matrix is obtained by arranging a plurality of vectors of the same size,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\boldsymbol{X}=\begin{bmatrix}
x_{11} &amp; x_{12} \\
x_{21} &amp; x_{22} \\
x_{31} &amp; x_{32}
\end{bmatrix}\end{split}\]</div>
<p>It is expressed as follows. The size of the matrix is represented by rows and columns. For example,<span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> Is 3 rows by 2 columns and is called a matrix of size (3, 2). In many cases, matrices are written in uppercase letters or upper case bold letters.</p>
<p><strong>Tensors</strong> are generalized concepts of vectors and matrices, vectors can be expressed as tensors on the first floor and matrices as tensors on the second floor. Also, as shown in the figure, the further arranged in the depth direction is the tensor on the third floor. For example, in the case of digital representation of a color image, it is common to use a color space such as RGB (Red Green Blue) for each pixel constituting an image, and by using three axes (row number, column number, color)
In order to specify one value, it is represented by the tensor on the third floor. When including this course, if it is expressed simply as a tensor, it often points to the tensor of the third floor or more, so please be careful.</p>
<p><img alt="image5" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/07.png" /></p>
<p>In linear algebra <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> Ya <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> It is easy to understand what type of numerical value is handled because it deforms expressions with only letters like this, but be careful not to lose sight of its shape by always keeping aware of vectors etc Let‚Äôs see.</p>
<table class="docutils align-center">
<colgroup>
<col style="width: 18%" />
<col style="width: 41%" />
<col style="width: 41%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Â∞èÊñáÂ≠ó</p></th>
<th class="head"><p>Â§ßÊñáÂ≠ó</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Á¥∞ÊñáÂ≠ó</p></td>
<td><p>„Çπ„Ç´„É©„Éº„ÅÆÂ§âÊï∞</p></td>
<td><p>„Çπ„Ç´„É©„Éº„ÅÆÂÆöÊï∞</p></td>
</tr>
<tr class="row-odd"><td><p>Â§™ÊñáÂ≠ó</p></td>
<td><p>„Éô„ÇØ„Éà„É´</p></td>
<td><p>Ë°åÂàóÔºå„ÉÜ„É≥„ÇΩ„É´</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="Addition,-subtraction">
<h3>1.3.3. Addition, subtraction<a class="headerlink" href="#Addition,-subtraction" title="Permalink to this headline">¬∂</a></h3>
<p>Let‚Äôs memorize matrices and vector operations. The addition is established only between matrices of the same size and vectors, and is defined as follows.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}&amp;\begin{bmatrix}
1 \\
2 \\
3
\end{bmatrix}+\begin{bmatrix}
4 \\
5 \\
6
\end{bmatrix}=\begin{bmatrix}
1+4 \\
2+5 \\
3+6
\end{bmatrix}=\begin{bmatrix}
5 \\
7 \\
9
\end{bmatrix}\\
&amp;\begin{bmatrix}
1 &amp; 2 &amp; 3 \\
4 &amp; 5 &amp; 6
\end{bmatrix}+\begin{bmatrix}
7 &amp; 8 &amp; 9 \\
10 &amp; 11 &amp; 12
\end{bmatrix}=\begin{bmatrix}
1+7 &amp; 2+8 &amp; 3+9 \\
4+10 &amp; 5+11 &amp; 6+12
\end{bmatrix}=\begin{bmatrix}
8 &amp; 10 &amp; 12 \\
14 &amp; 16 &amp; 18
\end{bmatrix}\end{aligned}\end{split}\]</div>
<p>In this way add the corresponding places with the <strong>elements</strong> in the matrix or vector . Subtraction is similar. Let‚Äôs remember that <strong>calculation is not established unless it is the same size</strong>.</p>
</div>
<div class="section" id="Dot-inner">
<h3>1.3.4. Dot inner<a class="headerlink" href="#Dot-inner" title="Permalink to this headline">¬∂</a></h3>
<p>Dot product can be defined between vectors of the same size. The inner product is the sum of the corresponding values of the same position and adding them together.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}&amp; \begin{bmatrix}
1 &amp; 2 &amp; 3
\end{bmatrix} \begin{bmatrix}
4 \\
5 \\
6
 \end{bmatrix} = 1 \times 4 + 2 \times 5  + 3 \times 6 = 32 \end{aligned}\end{split}\]</div>
</div>
<div class="section" id="Multiplication-(matrix-multiplication)">
<h3>1.3.5. Multiplication (matrix multiplication)<a class="headerlink" href="#Multiplication-(matrix-multiplication)" title="Permalink to this headline">¬∂</a></h3>
<p>There are multiple kinds of matrix multiplication, such as matrix multiplication, cross product, element product (Hadamard product). Here we explain the most commonly used <strong>matrix product</strong> . Unless explicitly stated below, matrix multiplication refers to matrix multiplication.</p>
<p>matrix <span class="math notranslate nohighlight">\(A\)</span> And matrix <span class="math notranslate nohighlight">\(B\)</span> The matrix product of <span class="math notranslate nohighlight">\(A\)</span>With each row of <span class="math notranslate nohighlight">\(B\)</span> It is defined as an array of dot products of each column of. For example, the inner product of the row vector of the second row of matrix A and the column vector of the third column of matrix B corresponds to the second row and the third column of matrix C of the result.</p>
<p><img alt="image6" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/08.png" /></p>
<p>And the condition that the inner product is defined is that the size of the vector is equal, but also here it holds that the size of row A (= the number of columns of A) and the size of the column of B (= B The number of lines) must match. The result is a matrix consisting of the number of rows of A and the number of columns of B.</p>
<p><img alt="image7" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/09.png" /></p>
<p>Also, as one of the properties where matrix multiplication is significantly different from scalar product,<span class="math notranslate nohighlight">\(AB\)</span> When <span class="math notranslate nohighlight">\(BA\)</span> It is not necessarily the same thing.</p>
<p>Matrix products are used for many problems of linear algebra and machine learning. Although there is no operation equivalent to division in the matrix, using the inverse matrix described later <span class="math notranslate nohighlight">\(4 / 2 = 4 \times \dfrac{1}{2}\)</span> Describe the division as multiplication of reciprocal (inverse matrix) as shown in.</p>
<p>Then, based on the confirmation of the calculation condition, please solve the following three as exercises.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
&amp;\left( 1\right)
\begin{bmatrix}
1 &amp; 2
\end{bmatrix}
\begin{bmatrix}
3 \\
4
\end{bmatrix}\\
&amp;\left( 2\right)
\begin{bmatrix}
1 &amp; 2 \\
3 &amp; 4
\end{bmatrix}\begin{bmatrix}
5 \\
6
\end{bmatrix}\\
&amp;\left( 3\right)
\begin{bmatrix}
1 &amp; 2
\end{bmatrix}\begin{bmatrix}
3 &amp; 4 \\
5 &amp; 6
\end{bmatrix}\begin{bmatrix}
3 \\
1
\end{bmatrix}
\end{aligned}\end{split}\]</div>
<p>Here is the answer.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
&amp;\left( 1\right)
\begin{bmatrix}
1 &amp; 2
\end{bmatrix}\begin{bmatrix}
3 \\
4
\end{bmatrix} = 1\times 3 + 2 \times 4 = 11\\
&amp;\left( 2\right)
\begin{bmatrix}
1 &amp; 2 \\
3 &amp; 4
\end{bmatrix}\begin{bmatrix}
5 \\
6
\end{bmatrix} = \begin{bmatrix}
1 \times 5 + 2 \times 6 \\
3 \times 5 + 4 \times 6
\end{bmatrix} = \begin{bmatrix}
17 \\
39
\end{bmatrix}\\
&amp;\left( 3\right)
\begin{bmatrix}
1 &amp; 2
\end{bmatrix}\begin{bmatrix}
3 &amp; 4 \\
5 &amp; 6
\end{bmatrix}\begin{bmatrix}
3 \\
1
\end{bmatrix}
=\begin{bmatrix}
1 &amp; 2
\end{bmatrix}\begin{bmatrix}
3 \times 3 + 4 \times 1 \\
5 \times 3 + 6 \times 1
\end{bmatrix} = \begin{bmatrix}
1 &amp; 2
\end{bmatrix}\begin{bmatrix}
13 \\
21
\end{bmatrix}
= 1 \times 13 + 2 \times 21
=55
\end{aligned}\end{split}\]</div>
<p>Calculations of this form often appear in machine learning. Let‚Äôs keep in mind that the matrix product changes shape after operation.</p>
</div>
<div class="section" id="Transpose">
<h3>1.3.6. Transpose<a class="headerlink" href="#Transpose" title="Permalink to this headline">¬∂</a></h3>
<p>Vectors are based on portrait oriented column vectors, but sometimes you may want to use a vector of landscape orientation. Therefore, the operation of swapping vertically oriented vectors to transverse vectors and horizontally oriented vectors into vertically oriented vectors is called <strong>Transpose)ùëá</strong> I will write it. For example,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}\boldsymbol{x}&amp;=\begin{bmatrix}
1 \\
2 \\
3
\end{bmatrix}, \
\boldsymbol{x}^{T}=\begin{bmatrix} 1 &amp; 2 &amp; 3 \end{bmatrix} \\
\boldsymbol{X}&amp;=\begin{bmatrix}
1 &amp; 4 \\
2 &amp; 5 \\
3 &amp; 6
\end{bmatrix}, \
\boldsymbol{X}^{T}=\begin{bmatrix}
1 &amp; 2 &amp; 3 \\
4 &amp; 5 &amp; 6
\end{bmatrix}\end{aligned}\end{split}\]</div>
<p>It looks like. On transposition to a matrix, the size is<span class="math notranslate nohighlight">\((N, M)\)</span> From <span class="math notranslate nohighlight">\((M, N)\)</span> become,<span class="math notranslate nohighlight">\(i\)</span> line <span class="math notranslate nohighlight">\(j\)</span> After transposing the value of the column <span class="math notranslate nohighlight">\(j\)</span> line <span class="math notranslate nohighlight">\(i\)</span> It becomes the value of the column column. It is a good idea to remember the following as the transpose formula.</p>
<p>„ÅÆ„Çà„ÅÜ„Å´„Å™„Çä„Åæ„ÅôÔºéË°åÂàó„Å´ÂØæ„Åô„ÇãËª¢ÁΩÆ„Åß„ÅØÔºå„Çµ„Ç§„Ç∫„Åå<span class="math notranslate nohighlight">\((N, M)\)</span>„Åã„Çâ<span class="math notranslate nohighlight">\((M, N)\)</span>„Å´„Å™„ÇäÔºå<span class="math notranslate nohighlight">\(i\)</span>Ë°å<span class="math notranslate nohighlight">\(j\)</span>ÂàóÁõÆ„ÅÆÂÄ§„ÅåËª¢ÁΩÆÂæå„Å´„ÅØ<span class="math notranslate nohighlight">\(j\)</span>Ë°å<span class="math notranslate nohighlight">\(i\)</span>ÂàóÁõÆ„ÅÆÂÄ§„Å´„Å™„Çä„Åæ„ÅôÔºéËª¢ÁΩÆ„ÅÆÂÖ¨Âºè„Å®„Åó„Å¶Ê¨°„ÇíË¶ö„Åà„Å¶„Åä„Åè„Å®„Çà„ÅÑ„Åß„Åó„Çá„ÅÜÔºé</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}&amp;\left( 1\right) \ \left( \boldsymbol{A}^{T}\right)^{T}=\boldsymbol{A}\\
&amp;\left( 2\right) \ \left( \boldsymbol{A}\boldsymbol{B}\right) ^{T}=\boldsymbol{B}^{T}\boldsymbol{A}^{T}\\
&amp;\left( 3\right) \ \left( \boldsymbol{A}\boldsymbol{B}\boldsymbol{C}\right) ^{T}=\boldsymbol{C}^{T}\boldsymbol{B}^{T}\boldsymbol{A}^{T}\end{aligned}\end{split}\]</div>
</div>
<div class="section" id="Vector,-size-of-matrix">
<h3>1.3.7. Vector, size of matrix<a class="headerlink" href="#Vector,-size-of-matrix" title="Permalink to this headline">¬∂</a></h3>
<p>After matrix multiplication, matrix size will change. size is <span class="math notranslate nohighlight">\((L, M)\)</span> With a matrix of <span class="math notranslate nohighlight">\((M ,N)\)</span> The matrix multiplication result of <span class="math notranslate nohighlight">\((L, N)\)</span>. For example, when summarizing how the sizes of the three exercise problems changed,</p>
<p><img alt="image8" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/10.png" /></p>
<ol class="arabic simple" start="3">
<li><p>Note that the result of the first vector and matrix are horizontal vectors and result in (1). Also, if the size of a dimension becomes 1, the dimension may be deleted, the vector may be a scalar, and the matrix may be a vector.</p></li>
</ol>
</div>
<div class="section" id="Unit-matrix">
<h3>1.3.8. Unit matrix<a class="headerlink" href="#Unit-matrix" title="Permalink to this headline">¬∂</a></h3>
<p>Scalar value1,<span class="math notranslate nohighlight">\(10 \times 1 = 10\)</span> It has the property that it does not change even if that number is multiplied by an arbitrary number. In matrices, matrices that work like this are <strong>unit matrices</strong>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\boldsymbol{I}=\begin{bmatrix}
1 &amp; 0 &amp; \ldots  &amp; 0 \\
0 &amp; 1 &amp; \ldots  &amp; 0 \\
\vdots &amp; \vdots  &amp; \ddots  &amp; \vdots  \\
0 &amp; 0 &amp; \ldots  &amp; 1
\end{bmatrix}\end{split}\]</div>
<p>It has the shape as above and the symbol <span class="math notranslate nohighlight">\(\boldsymbol{I}\)</span> It is common to be represented by. The diagonal elements of a matrix are called <strong>diagonal elements</strong> , and the other elements are called offdiagonal elements. An identity matrix is a <strong>square matrix</strong> ( <strong>a matrix</strong> whose number of row elements matches the number of column elements) such that the diagonal elements are 1 and the off-diagonal elements are 0 . For example, in the case of a 2 √ó 2 matrix,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\boldsymbol{I} =\begin{bmatrix}
1 &amp; 0 \\
0 &amp; 1
\end{bmatrix}\end{split}\]</div>
<p>In the case of a 3 √ó 3 matrix,Ôºå</p>
<div class="math notranslate nohighlight">
\[\begin{split}\boldsymbol{I}=\begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix}\end{split}\]</div>
<p>If you want to specify the size of the matrix,<span class="math notranslate nohighlight">\(I_{n}\)</span> (<span class="math notranslate nohighlight">\(n√ón\)</span> and subscripts to distinguish them.</p>
<p>An identity matrix is an arbitrary matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> The following holds.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\boldsymbol{A}\boldsymbol{I}&amp;=\boldsymbol{A}\\
\boldsymbol{I}\boldsymbol{A}&amp;=\boldsymbol{A}
\end{aligned}\end{split}\]</div>
<p>As explained earlier, in order for multiplication of a matrix to be established,<span class="math notranslate nohighlight">\(I\)</span> The size of <span class="math notranslate nohighlight">\(A\)</span> It must be the same as.</p>
<p>By actually calculating and checking whether the value does not change from the original matrix,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}\begin{bmatrix}
1 &amp; 2 \\
3 &amp; 4
\end{bmatrix}\begin{bmatrix}
1 &amp; 0 \\
0 &amp; 1
\end{bmatrix}
&amp;=\begin{bmatrix}
1\times 1+2\times 0 &amp; 1\times 0+2\times 1 \\
3\times 1+4\times 0 &amp; 3\times 0+4\times 1
\end{bmatrix}\\
&amp;=
\begin{bmatrix}
1 &amp; 2 \\
3 &amp; 4
\end{bmatrix}
\end{aligned}\end{split}\]</div>
<p>As you can see, it matches the original value.</p>
</div>
<div class="section" id="Inverse-matrix">
<h3>1.3.9. Inverse matrix<a class="headerlink" href="#Inverse-matrix" title="Permalink to this headline">¬∂</a></h3>
<p><strong>The inverse matrix</strong> is a matrix that becomes an identity matrix when multiplied by the original matrix, and the reciprocal number in the scalar (<span class="math notranslate nohighlight">\(2 \times 2^{-1} = 1\)</span>) It is a matrix corresponding to. matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span>, Its inverse matrix is <span class="math notranslate nohighlight">\(\boldsymbol{A}^{-1}\)</span> I will write like this.</p>
<p>When the definition of the inverse matrix is represented by a mathematical expression,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\boldsymbol{A}\boldsymbol{A}^{-1}=\boldsymbol{I}\\
\boldsymbol{A}^{-1}\boldsymbol{A}=\boldsymbol{I}
\end{aligned}\end{split}\]</div>
<p>here,<span class="math notranslate nohighlight">\(\boldsymbol{I}\)</span> Is the unit matrix of the previous time. size is<span class="math notranslate nohighlight">\(2 \times 2\)</span> Ya <span class="math notranslate nohighlight">\(3 \times 3\)</span> In the case of a small matrix such as small matrix, there are formulas for inverse matrix calculation, but in machine learning a matrix of larger size ( <span class="math notranslate nohighlight">\(1000 \times 1000\)</span> Etc.), it is necessary to deal with calculation methods for efficient inversion of inverse matrices.</p>
<p>Inverse matrices do not always exist. A matrix in which an inverse matrix exists is called a <strong>regular matrix</strong> (conditions for the matrix being regular are not explained this time).</p>
</div>
<div class="section" id="Linear-joins-and-quadratic">
<h3>1.3.10. Linear joins and quadratic<a class="headerlink" href="#Linear-joins-and-quadratic" title="Permalink to this headline">¬∂</a></h3>
<p>As a form often appearing in machine learning equations, <span class="math notranslate nohighlight">\(\boldsymbol{b}^{T}\boldsymbol{x}\)</span> When <span class="math notranslate nohighlight">\(\boldsymbol{x}^{T}\boldsymbol{A}\boldsymbol{x}\)</span> There are two forms. The former linear combination or linear combination , the latter quadratic form has been called. For scalars, linear expressions <span class="math notranslate nohighlight">\(ax+b\)</span>Ôºâ And quadratic equation Ôºà<span class="math notranslate nohighlight">\(ax^2+bx+c\)</span>Ôºâ, But it is an extension of it to a vector.</p>
<p>Looking at the contents of the linear combination calculation,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\boldsymbol{b}&amp;=\begin{bmatrix}
1 \\
2
\end{bmatrix},\
\boldsymbol{x}=\begin{bmatrix}
x_{1} \\
x_{2}
\end{bmatrix}\\
\boldsymbol{b}^{T}\boldsymbol{x}&amp;=\begin{bmatrix}
1 &amp; 2
\end{bmatrix}\begin{bmatrix}
x_{1} \\
x_{2}
\end{bmatrix}=x_{1}+2x_{2}\end{aligned}\end{split}\]</div>
<p>like <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> It is an element of <span class="math notranslate nohighlight">\(x_{1}\)</span> Or <span class="math notranslate nohighlight">\(x_{2}\)</span> As you can see, it is a primary equation.</p>
<p>Also, if you check the contents of the calculation for the quadratic form as well,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\boldsymbol{A}&amp;=\begin{bmatrix}
1 &amp; 2 \\
3 &amp; 4
\end{bmatrix},\
\boldsymbol{x}=\begin{bmatrix}
x_{1} \\
x_{2}
\end{bmatrix}\\
\boldsymbol{x}^{T}\boldsymbol{A}\boldsymbol{x}
&amp;=\begin{bmatrix} x_{1} &amp; x_{2}\end{bmatrix}
\begin{bmatrix}
1 &amp; 2 \\
3 &amp; 4
\end{bmatrix}\begin{bmatrix}
x_{1} \\
x_{2}
\end{bmatrix}\\
&amp;=\begin{bmatrix}x_{1} &amp; x_{2}\end{bmatrix} \begin{bmatrix}
x_{1}+2x_{2} \\
3x_{1}+4x_{2}
\end{bmatrix}\\
&amp;=x_{1}\left( x_{1}+2x_{2}\right) +x_{2}\left( 3x_{1}+4x_{2}\right) \\
&amp;=x^{2}_{1}+5x_{1}x_{2}+4x_{2}^{2}\end{aligned}\end{split}\]</div>
<p>It turns out that each element has a quadratic expression.</p>
<p>Therefore, an arbitrary quadratic function</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{x}^{T}\boldsymbol{A}\boldsymbol{x} + \boldsymbol{b}^{T}\boldsymbol{x} + c\]</div>
<p>It can be expressed in the form of. here, <span class="math notranslate nohighlight">\(c\)</span> Is a constant term of a scalar.</p>
</div>
<div class="section" id="Differential-by-the-vector-and-gradient">
<h3>1.3.11. Differential by the vector and gradient<a class="headerlink" href="#Differential-by-the-vector-and-gradient" title="Permalink to this headline">¬∂</a></h3>
<p>Differential is described as the amount of change in function value when input is changed. Likewise, if the input of the function is a vector, we can think of a differentiation with a vector. Calculate partial derivatives for each vector component of the function, and arrange them into vectors called <strong>gradients</strong>.</p>
<p>Before introducing the gradient calculation, let‚Äôs calculate the following example.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\boldsymbol{b}&amp;=\begin{bmatrix}
3 \\
4
\end{bmatrix}, \
\boldsymbol{x}=\begin{bmatrix}
x_{1} \\
x_{2}
\end{bmatrix}\\
\boldsymbol{b}^{T}\boldsymbol{x}&amp;=\begin{bmatrix}
3 &amp; 4
\end{bmatrix}\begin{bmatrix}
x_{1} \\
x_{2}
\end{bmatrix}
=3x_{1}+4x_{2}\end{aligned}\end{split}\]</div>
<p>this <span class="math notranslate nohighlight">\(\boldsymbol{b}^{T}\boldsymbol{x}\)</span> With vector <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> ,</p>
<div class="math notranslate nohighlight">
\[\dfrac {\partial }{\partial \boldsymbol{x}}\left( \boldsymbol{b}^{T}\boldsymbol{x}\right)\]</div>
<p>„Å®It is expressed. It is said <strong>to differentiate</strong> this with a <strong>vector</strong> . In this example,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\dfrac {\partial }{\partial \boldsymbol{x}}\left( \boldsymbol{b}^{T}\boldsymbol{x}\right) &amp;=\dfrac {\partial }{\partial \boldsymbol{x}}\left( 3x_{1}+4x_{2}\right) \\
&amp;=\begin{bmatrix}
\dfrac {\partial }{\partial x_{1}} \left( 3x_{1}+4x_{2}\right)  \\
\dfrac {\partial }{\partial x_{2}} \left( 3x_{1}+4x_{2}\right)
\end{bmatrix}
\end{aligned}\end{split}\]</div>
<p>As we go forward with the calculation</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}\dfrac {\partial }{\partial x_{1}}\left( 3x_{1}+4x_{2}\right) &amp;=\dfrac {\partial }{\partial x_{1}}\left( 3x_{1}\right) +\dfrac {\partial }{\partial x_{1}}\left( 4x_{2}\right) \\
&amp;=3\times \dfrac {\partial }{\partial x_{1}}\left( x_{1}\right) +4x_{2}\times \dfrac {\partial }{\partial x_{1}}\left( 1\right) \\
&amp;=3\times 1+4x_{2}\times 0\\
&amp;=3\end{aligned}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}\dfrac {\partial }{\partial x_{2}}\left( 3x_{1}+4x_{2}\right)&amp;=\dfrac {\partial }{\partial x_{2}}\left( 3x_{1}\right) +\dfrac {\partial }{\partial x_{2}}\left( 4x_{2}\right) \\
&amp;=3x_{1}\times \dfrac {\partial }{\partial x_{2}}\left( 1\right) +4\times \dfrac {\partial }{ax_{2}}\left( x_{2}\right) \\
&amp;=3x_{1} \times 0 + 4 \times 1 \\
&amp;= 4
\end{aligned}\end{split}\]</div>
<p>The following calculation results are obtained.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\dfrac {\partial }{\partial \boldsymbol{x}}\left( \boldsymbol{b}^{T}\boldsymbol{x}\right)
&amp;=\begin{bmatrix}
\dfrac {\partial }{\partial x_{1}} \left( 3x_{1}+4x_{2}\right)  \\
\dfrac {\partial }{\partial x_{2}} \left( 3x_{1}+4x_{2}\right)
\end{bmatrix} =\begin{bmatrix}
3  \\
4
\end{bmatrix} = \boldsymbol{b}
\end{aligned}\end{split}\]</div>
<p>Another question, let‚Äôs consider the following example.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\boldsymbol{b}&amp;=\begin{bmatrix}
3 \\
4
\end{bmatrix}, \
\boldsymbol{x}=\begin{bmatrix}
x_{1} \\
x_{2}
\end{bmatrix}\\
\dfrac {\partial }{\partial \boldsymbol{x}}\left( \boldsymbol{b}\right) &amp;=\begin{bmatrix}
\dfrac {\partial }{\partial x_{1}}\left( 3 \right)  \\
\dfrac {\partial }{\partial x_{2}}\left( 4 \right)
\end{bmatrix}
=\begin{bmatrix}
0 \\
0
\end{bmatrix}=\boldsymbol{0}\end{aligned}\end{split}\]</div>
<p>If the variable to be subjected to partial differentiation is not included, its partial differential is 0. Elements0A vector consisting solely of it is called a <strong>zero (zero) vector</strong>.</p>
<p>Based on these, let‚Äôs summarize as official.</p>
<p>Ôºé</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
&amp;\left( 1\right) \ \dfrac {\partial}{\partial \boldsymbol{x}}\left( \boldsymbol{c} \right) = \boldsymbol{0}\\
&amp;\left( 2\right) \ \dfrac {\partial }{\partial \boldsymbol{x}}\left( \boldsymbol{b}^{T}\boldsymbol{x}\right) = \boldsymbol{b}\\
&amp;\left( 3\right) \ \dfrac {\partial }{\partial \boldsymbol{x}}\left( \boldsymbol{x}^{T}\boldsymbol{A}\boldsymbol{x}\right) =\left( \boldsymbol{A}+\boldsymbol{A}^{T}\right) \boldsymbol{x}\end{aligned}\end{split}\]</div>
<p>(1)„Å®(2) „ÅØ„Åô„Åß„Å´Â∞éÂá∫Ê∏à„Åø„Åß„ÅôÔºé(3) „ÅØÂ∞éÂá∫„ÅåÂ∞ë„ÅóË§áÈõë„Å™„ÅÆ„ÅßÁúÅÁï•„Åó„Åæ„Åô„ÅåÔºåÊï∞ÂÄ§„Çí‰ª£ÂÖ•„Åó„Å¶Á¢∫Ë™ç„Åó„Å¶„Åø„Å¶„Åè„Å†„Åï„ÅÑÔºé„Åì„ÅÆ3„Å§„ÅÆÂÖ¨Âºè„ÅØÊ©üÊ¢∞Â≠¶Áøí„ÇíÂ≠¶„Çì„Åß„ÅÑ„Åè‰∏ä„ÅßÈùûÂ∏∏„Å´ÈáçË¶Å„Å™ÂÖ¨Âºè„Å®„Å™„Çä„Åæ„Åô„ÅÆ„ÅßÔºåÂøÖ„ÅöË¶ö„Åà„Å¶„Åä„Åç„Åæ„Åó„Çá„ÅÜÔºé</p>
<p>There are many other formulas in such matrices etc. It is also important to know what formulas are available when reading papers and the like. For example, please refer to <a class="reference external" href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">The Matrix Cookbook</a>.</p>
<p>In addition, this time we introduced gradients as derivatives of the multiple input single output function, but the Jacobian matrix (Jacobian), which is a derivative of the multiple input multiple output function, is also necessary to understand the error back propagation method of the neural network However, in most cases it is sufficient to memorize that the Jacobian matrix multiplied by a matrix is its transposed matrix.) For more information, please refer to, for example, The <a class="reference external" href="https://arxiv.org/abs/1802.01528">The Matrix
Calculus You Need For Deep Learning</a>.</p>
</div>
</div>
<div class="section" id="Probability-and-Statistics">
<h2>1.4. Probability and Statistics<a class="headerlink" href="#Probability-and-Statistics" title="Permalink to this headline">¬∂</a></h2>
<div class="section" id="What-are-probability-and-statistics-available-for?">
<h3>1.4.1. What are probability and statistics available for?<a class="headerlink" href="#What-are-probability-and-statistics-available-for?" title="Permalink to this headline">¬∂</a></h3>
<p>Speaking of machine learning, I think that there are many studying in the image such as probability and statistics, but if it is a simple algorithm, it can explain by just understanding differential and linear algebra, probability and statistics come out There is nothing to come. So, why are probability and statistics necessary?</p>
<p>Machine learning is a method of handling data. Data is a collection of individual events, but the purpose of learning is to capture the universality and the law behind that data. Probability can formulate the concept of data distribution and uncertainty. Also, by statistics, various statistics can be obtained for a certain group, and normalization is made so that data can be easily learned using them, and it is judged whether each data is valid or outliers can do.</p>
</div>
<div class="section" id="Probability">
<h3>1.4.2. Probability<a class="headerlink" href="#Probability" title="Permalink to this headline">¬∂</a></h3>
<p>The probability represents the degree to which the event is expected to occur for various possible events. In the context of parameter estimation, there may be cases in which the probability represents a belief that it is likely to occur. The probability is <span class="math notranslate nohighlight">\(p(x)\)</span> In the form of a function like <span class="math notranslate nohighlight">\(x\)</span> The random variable is called. A random variable is a variable that takes one of the possible events. further <span class="math notranslate nohighlight">\(p(x=u)\)</span> As a random variable <span class="math notranslate nohighlight">\(x\)</span> The value of the <span class="math notranslate nohighlight">\(u\)</span> It is
assumed to be the probability when it was. I omit this <span class="math notranslate nohighlight">\(p(x=u)\)</span> In the form of. The probability is ‚Äúthe sum of the probabilities of all events is 1‚Äù The probability of all events is0 It meets the two constraints of ‚ÄúIt is over‚Äù. When writing this with an expression,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\sum_{x } p(x) &amp;= 1 \\
p(x) &amp; \geq  0
\end{align}\end{split}\]</div>
<p>The probability that two events occur <strong>simultaneously</strong> is called <strong>simultaneous probability</strong> <span class="math notranslate nohighlight">\(p(x, y)\)</span> It is expressed as follows. For example, swipe the dice twice, the first eye <span class="math notranslate nohighlight">\(0\)</span>, The second eye <span class="math notranslate nohighlight">\(5\)</span> Probability to be represented by joint probability.</p>
<p>Among the joint probability, paying attention only to a particular random variable, the operation to be erased by taking the sum for the other random variable <strong>marginalization</strong> is called (the word marginalization is, first of random variables a row, column Is associated with the second random variable and the joint probability is written in a table, it is said that it is because the sum of the rows and the sum of the columns are written around the table). The result of marginalization matches
the probability of random variables of interest.</p>
<div class="math notranslate nohighlight">
\[\begin{split}p(x) = \sum_y p(x, y) \\
p(y) = \sum_x p(x, y)\end{split}\]</div>
<p>The conditional probability is called the probability distribution that shows the probability of the other under the condition that one random variable is fixed with a certain value ,<span class="math notranslate nohighlight">\(p(y|x)\)</span>It is expressed as follows. For exampleùë¶A random variable representing whether it is raining outside,ùë•As a random variable representing whether the person who entered the room had an umbrella (<span class="math notranslate nohighlight">\(y=1\)</span> Rain is shaking,<span class="math notranslate nohighlight">\(y=0\)</span> Assign it as if the rain did not shake). At this time
<span class="math notranslate nohighlight">\(p(y|x)\)</span> Represents the conditional probability of rain outside when the person who entered the room had an umbrella.</p>
<p>The conditional probability is equal to the value obtained by dividing the joint probability by the probability of the condition.</p>
<div class="math notranslate nohighlight">
\[p(y|x) = \frac{p(x, y)}{p(x)}\]</div>
<p>Here, we modified the expression of the conditional probability <span class="math notranslate nohighlight">\(p(y|x)p(x) = p(x, y)\)</span> Note that,</p>
<div class="math notranslate nohighlight">
\[p(x|y) = \frac{p(x, y)}{p(y)} = \frac{p(y | x)p(x)}{p(y)}\]</div>
<p>can be obtained. This is called <strong>Bayes‚Äô theorem</strong> . It is an important theorem, so let‚Äôs remember.</p>
<p>For example, as an application example of Bayes‚Äô theorem, there is a spam (unsolicited) mail filter. A word in e-mailùëñA probability variable representing whether or not <span class="math notranslate nohighlight">\(i\)</span> , A random variable representing whether or not the mail is spam <span class="math notranslate nohighlight">\(y\)</span> Then, <span class="math notranslate nohighlight">\(p(x_{i}=1)\)</span> ‚ÄúMail is a wordùëñProbability including‚Äù <span class="math notranslate nohighlight">\(p(y=1)\)</span> ‚ÄúProbability that mail is spam‚Äù,<span class="math notranslate nohighlight">\(p(x_{i}=1|y=1)\)</span> ‚ÄúWhen the mail was spam, a wordùëñProbability of being included‚Äù. By aggregating each proportion from the
received large number of e-mails and applying Bayes‚Äô theorem, <span class="math notranslate nohighlight">\(p(y=1|x_{i}=1)\)</span> As a word ‚Äú<span class="math notranslate nohighlight">\(i\)</span> The probability that the mail is spam‚Äùcan be obtained.</p>
</div>
<div class="section" id="The-likelihood-and-maximum-likelihood-estimation">
<h3>1.4.3. The likelihood and maximum likelihood estimation<a class="headerlink" href="#The-likelihood-and-maximum-likelihood-estimation" title="Permalink to this headline">¬∂</a></h3>
<p>(Parametric) stochastic model <span class="math notranslate nohighlight">\(p(x; \theta)\)</span> Is the parameter <span class="math notranslate nohighlight">\(\theta\)</span> It is a function as characterized by. Probabilistic model <span class="math notranslate nohighlight">\(p(x)\)</span> Event onùë¢Probability of observation <span class="math notranslate nohighlight">\(p(x=u; \theta)\)</span> Event <span class="math notranslate nohighlight">\(u\)</span> It is called the likelihood of . The likelihood likelihood means ‚Äúlikelihood (more) likely‚Äù and it indicates the likelihood of the event.</p>
<p>here,<span class="math notranslate nohighlight">\(N\)</span> Pieces of data <span class="math notranslate nohighlight">\(X = \left( x^{(1)}, x^{(2)}, \ldots, x^{(N)} \right)\)</span> And the data <span class="math notranslate nohighlight">\(X\)</span> We will consider the problem of estimating the probability distribution that generates. In this case, a method called maximum likelihood estimation is often used. <strong>Maximum likelihood estimation</strong> is based on observed data <span class="math notranslate nohighlight">\(X\)</span> Parameters most likely to be generated <span class="math notranslate nohighlight">\(\theta\)</span> It is a method to estimate. When the data to be observed are generated independently, the
likelihood is</p>
<div class="math notranslate nohighlight">
\[L(\theta) = p(X; \theta) = \prod_{i=1}^N p(x^{(i)}; \theta)\]</div>
<p>It is represented as follows. this <span class="math notranslate nohighlight">\(\prod\)</span> The sign <span class="math notranslate nohighlight">\(\sum\)</span> It means to multiply all the values with the multiplication version of. As for the likelihood for multiple data,1. Since it is a product of smaller values, it becomes a very small number and becomes difficult to handle on a computer. When maximizing the likelihood, it is known that maximization of product form is difficult. Therefore, instead of likelihood, we consider the log likelihood taking the logarithm.</p>
<div class="math notranslate nohighlight">
\[\log L(\theta) = \log p(X; \theta) = \sum_{i=1}^N \log p(x^{(i)}; \theta)\]</div>
<p>Parameters that maximize this log likelihood <span class="math notranslate nohighlight">\(\theta\)</span> If it can be determined that the value is data <span class="math notranslate nohighlight">\(X\)</span> It is a parameter of the probability model that is most likely to generate.</p>
<p>Here, as a concrete example easy to understand, let us consider the problem of estimating the probability that the front and back of a coin will appear. A random variable representing the front and back of the coinùë•Aside,<span class="math notranslate nohighlight">\(x\)</span> = 1 If it is a table,<span class="math notranslate nohighlight">\(x\)</span> =0 If it is, it will be behind. Also, in the table (<span class="math notranslate nohighlight">\(X\)</span>=1) As a parameter representing the probability of <span class="math notranslate nohighlight">\(\theta\)</span> I will leave. Coin <span class="math notranslate nohighlight">\(10\)</span> As a result of throwing, the following observation result <span class="math notranslate nohighlight">\(X\)</span> Is
obtained.</p>
<div class="math notranslate nohighlight">
\[X = \left(1, 0, 1, 1, 1, 0, 0, 1, 0, 0 \right)\]</div>
<p>Then,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
L(\theta) &amp;= \theta \cdot (1 - \theta) \cdot  \ldots  \cdot (1 - \theta) \cdot (1 - \theta) \\
&amp;= \theta^{5} \cdot (1 - \theta)^{5}
\end{aligned}\end{split}\]</div>
<p>The log likelihood is calculated as follows:</p>
<div class="math notranslate nohighlight">
\[\log L(\theta) = 5 \log \theta + 5 \log \left( 1 - \theta \right)\]</div>
<p>this <span class="math notranslate nohighlight">\(\theta\)</span> Differentiate with <span class="math notranslate nohighlight">\(0\)</span> Is obtained,</p>
<div class="math notranslate nohighlight">
\[\frac{5}{\theta} - \frac{5}{\left( 1 - \theta \right)} = 0\]</div>
<p>ThanÔºå<span class="math notranslate nohighlight">\(\theta = 0.5\)</span> is obtained by maximum likelihood estimation.Ôºé</p>
<p>In the case of using the sum of the square error of the true value and the predicted value as the objective function of the regression model ( called the <strong>least squares method</strong> ), the maximum likelihood estimation is performed assuming the error of the normal distribution (described later) to the output value of the model It is known to be equivalent to being.</p>
</div>
<div class="section" id="Posterior-probability-maximization-estimation-(MAP-estimation)">
<h3>1.4.4. Posterior probability maximization estimation (MAP estimation)<a class="headerlink" href="#Posterior-probability-maximization-estimation-(MAP-estimation)" title="Permalink to this headline">¬∂</a></h3>
<p>Although maximum likelihood estimation is effective in many cases, if there is some prior information in the parameter to be obtained, maximum likelihood estimation can not handle that a priori information. Therefore, trying to estimate the parameters while the number of trials is small may not work well with maximum likelihood estimation.</p>
<p>As in the previous example, let‚Äôs consider an example of estimating the probability that the front and back of a coin will appear. Coin5When throwing it, it happens <span class="math notranslate nohighlight">\(5\)</span> Times and tables (<span class="math notranslate nohighlight">\(x = 1\)</span>) Has come out. In this case, in the maximum likelihood estimation, the probability that the table appears is <span class="math notranslate nohighlight">\(100\)</span>% (Probability of backing out <span class="math notranslate nohighlight">\(0\)</span>%) In the future. However, the probability of obvious backing out is0If there is prior information that it should be larger than
that, it is likely to make a better estimate.</p>
<p>In such a case, the method of estimating the parameters based on observation data while also considering prior information is estimation of a posteriori probability <strong>(Maximum A Posteriori, MAP)</strong> . In MAP estimation, parameters <span class="math notranslate nohighlight">\(\theta\)</span> Is also a random variable, its distribution ( also called <strong>prior probability</strong>)<span class="math notranslate nohighlight">\(p\left( \theta \right)\)</span> I think that it exists. Then, observation data <span class="math notranslate nohighlight">\(X\)</span> In the given condition, the parameters <span class="math notranslate nohighlight">\(\theta\)</span> The conditional probability (
also called <strong>posteriori probability</strong>) <span class="math notranslate nohighlight">\(p\left( \theta|X\right)\)</span> To maximize <span class="math notranslate nohighlight">\(\theta\)</span> You will be asked.</p>
<p>Let‚Äôs recall Bayes‚Äô theorem here. When using Bayes‚Äô theorem, the posterior probability is</p>
<div class="math notranslate nohighlight">
\[p(\theta|X) = \frac{p(X|\theta)p(\theta)}{p(X)}\]</div>
<p>Considering maximizing this for parameters,<span class="math notranslate nohighlight">\(P(X)\)</span> Can not be ignored since it has no relation with the parameter,</p>
<div class="math notranslate nohighlight">
\[p(X|\theta) p(\theta)\]</div>
<p>You will be asked to maximize parameters.<span class="math notranslate nohighlight">\(p(X|\theta)\)</span> is the same as the maximum likelihood estimation, but in the MAP estimation, the prior probability of the parameter <span class="math notranslate nohighlight">\(p(\theta)\)</span> It will maximize the probability of multiplication. (I will not explain the process of obtaining the solution of MAP estimation here.)</p>
<p>When optimizing parameters in machine learning, we set a penalty term for the large value of the parameter called regularization, but this can be regarded as the prior probability (the log of) of the parameters, It can be interpreted as MAP estimation.</p>
</div>
<div class="section" id="Statistics">
<h3>1.4.5. Statistics<a class="headerlink" href="#Statistics" title="Permalink to this headline">¬∂</a></h3>
<p>Here, I will introduce the representative statistics, mean, variance, standard deviation.</p>
<p>First, I will introduce the average . For example, the average of 300 yen, 400 yen, 500 yen,</p>
<div class="math notranslate nohighlight">
\[\dfrac{300 + 400 + 500}{3} = 400\]</div>
<p>Then, add up all and divide by the number of targets. When this is formulated,</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}\overline {x}=\dfrac {x_{1}+x_{2}+\ldots +x_{N}}{N}
=\dfrac {1}{N}\sum ^{N}_{n=1}x_{n}\end{aligned}\]</div>
<p>It looks like. <span class="math notranslate nohighlight">\(N\)</span> Represents the number of samples . The average, <span class="math notranslate nohighlight">\(\bar{x}\)</span> Ya <span class="math notranslate nohighlight">\(\mu\)</span> It is common to be represented by a symbol such as. In the data distribution, the average is the value corresponding to its center of gravity.</p>
<p>Next, I will introduce distribution . The definition of dispersion</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}\sigma ^{2}=\dfrac {1}{N}\sum ^{N}_{n=1}\left( x_{n}-\overline {x}\right) ^{2}\end{aligned}\]</div>
<p>Average of each sample <span class="math notranslate nohighlight">\(\bar{x}\)</span> Difference from <span class="math notranslate nohighlight">\(x- \bar{x}\)</span> And calculate the average value of their squared errors. There is another definition in dispersion,</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
\sigma ^{2}=\dfrac {1}{N-1}\sum ^{N}_{n=1}\left( x_{n}-\overline {x}\right) ^{2}
\end{aligned}\]</div>
<p>t may be expressed as. The former is called specimen variance and the latter is called unbiased variance . The derivation of these expressions will be handed over to other books, and here we will explain their use properly. <img alt="image9" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/12.png" /></p>
<p>When analyzing data, it is important to be conscious of analysis on <strong>population</strong> or analysis on <strong>specimen group</strong> . The population is a case where all the data is available for the range of assumptions to be analyzed, and the sample group is for extracting a part of them. For example, when aggregating height and weight of primary school students nationwide, if you can collect elementary school students nationwide without leakage of one person, it is a population, but if you collect 100 people
in each prefecture and compile it, it becomes a specimen group. It is practically difficult to gather population data and it is common to estimate population distribution from sample group data. In that case, you will basically use the unbiased variance, which is for sample groups. The number of samples <span class="math notranslate nohighlight">\(N\)</span> If there are many, there is almost no difference between population variance and unbiased variance, but be careful as the number of samples is small when it is small.</p>
<p>By using variance, it becomes possible to quantitatively evaluate the variation of data. For example, if there are many variations in the results of experiments, it is possible that reproducibility could not be ensured in each experiment. In this way, it is important to quantify and evaluate the degree of variation in situations where it is desirable that the results of many trials gather at certain values. Besides, depending on the variation of data, you can also evaluate the difference in
scale by using variance.</p>
<p>Finally, I will introduce the standard deviation . In the variance, the unit is the square of the original unit because of the sum of the square of the difference from the average of each sample. For example, if the original unit is kg, the variance is in units of square of kg. So I took the square root of the variance <span class="math notranslate nohighlight">\(\sigma\)</span> By using, it becomes equal to the original unit, making interpretation easier. This is called standard deviation.</p>
<p>Let‚Äôs check the concrete calculation procedure with exercises. Please calculate average, variance, standard deviation for the following data in ‚ë† and ‚ë°. However, we will use the mother variance this time.</p>
<p><img alt="image10" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/13.png" /></p>
<p>The answers to ‚ë† are as follows.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\bar{x}&amp;=\dfrac {1}{5}\left( -2-1+0+1+2\right) =0\\
\sigma ^{2}&amp;=\dfrac {1}{5}\left\{ \left( -2-0\right) ^{2}+\left( -1-0\right) ^{2}+(0-0)^{2}+(1-0)^{2}+(2-0)^{2}\right\} \\
&amp;=\dfrac {1}{5}\times 10=2\\
\sigma &amp;=\sqrt {2}
\end{aligned}\end{split}\]</div>
<p>The answer to ‚ë° is as follows.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\overline {x}&amp;=\dfrac {1}{5}\left( -4-2+0+2+4\right) =0\\
\sigma ^{2}&amp;=\dfrac {1}{5}\left\{ \left( -4-0\right) ^{2}+\left( -2-0\right) ^{2}+\left( 0-0\right) ^{2}+\left( 2-0\right) ^{2}+\left( 4-0\right) ^{2}\right\} \\
&amp;=\dfrac {1}{5}\times 40=8\\
\sigma &amp;=\sqrt {8}=2\sqrt {2}
\end{aligned}\end{split}\]</div>
<p>From this, it can be seen that the case of ‚ë° has larger variance and large data variation.</p>
</div>
<div class="section" id="Normal-distribution-and-the-normalized">
<h3>1.4.6. Normal distribution and the normalized<a class="headerlink" href="#Normal-distribution-and-the-normalized" title="Permalink to this headline">¬∂</a></h3>
<p>Here, we will introduce the normal distribution that appears frequently with probability . It is also called <strong>Gaussian distribution</strong> . average<span class="math notranslate nohighlight">\(\mu\)</span>,standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span> The normal distribution with the shape has the following form.</p>
<p><img alt="image11" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/14.png" /></p>
<p>Why does this normal distribution appear frequently? There are the following physical and mathematical background as the reason.</p>
<ul class="simple">
<li><p>A random variable expressed as a sum of independent and numerous factors approximates a normal distribution</p></li>
<li><p>Easy to handle formulas</p></li>
</ul>
<p>Many data seen in the world are known to follow the normal distribution (eg height according to gender age, points of examination, measurement error of physical experiments, etc.). On the other hand, the data does not always obey the normal distribution. In many cases there are many cases where wrong conclusions are derived by applying it to a normal distribution for distribution that is not a normal distribution. Always think whether distributing the data can be handled as a normal distribution
by making it diagrammatic.</p>
<p>For normal distribution average <span class="math notranslate nohighlight">\(\mu\)</span> And standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span> We will do a lot of discussions on what percentage is in that distribution. For example,<span class="math notranslate nohighlight">\(\mu \pm 3\sigma\)</span> Since 99.7% of the total of the data falls within the range of <span class="math notranslate nohighlight">\(\mu \pm 3 \sigma\)</span> It can be used to define an area that does not fall into an outlier value (a value greatly deviating from other values).</p>
</div>
<div class="section" id="Scaling-using-the-standard-deviation">
<h3>1.4.7. Scaling using the standard deviation<a class="headerlink" href="#Scaling-using-the-standard-deviation" title="Permalink to this headline">¬∂</a></h3>
<p>Scaling is important as preprocessing in most machine learning algorithms,</p>
<p>To illustrate why scaling is important, I will take an example of calculating the distance between two points. Variables with different scales <span class="math notranslate nohighlight">\(x1\)</span> When <span class="math notranslate nohighlight">\(x2\)</span> If there is, the situation will be as shown in the figure below. Please note that the scales on the vertical axis and the horizontal axis are greatly different here.</p>
<p><img alt="image12" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/15.png" /></p>
<p>Distance between these two points <span class="math notranslate nohighlight">\(d\)</span> Is obtained,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
d&amp;=\sqrt {\left( 100-1000\right) ^{2}+\left( 0.1-1\right) ^{2}}\\
&amp;= \sqrt {900^{2}+0.9^{2}}\\
&amp;= \sqrt {810000+0.81} \\
&amp;= \sqrt {810000.81}
\end{aligned}\end{split}\]</div>
<p>It looks like. distance <span class="math notranslate nohighlight">\(d\)</span> among <span class="math notranslate nohighlight">\(x_{1}\)</span> Influence amount is large <span class="math notranslate nohighlight">\(x_{2}\)</span> As for the scale, it has little influence because it is small. With this <span class="math notranslate nohighlight">\(x_{2}\)</span> Can not be taken into account even when the meaning of data is important. One method of solving these problems is the <strong>scaling described</strong> here. There are two typical scaling methods.</p>
<p>The first method scales the sample set to a <strong>minimum value of 0</strong> and a <strong>maximum value of 1</strong> . This is called <strong>Min-Max scaling</strong> . In this method, the minimum value <span class="math notranslate nohighlight">\(x_{\min}\)</span> And the maximum value <span class="math notranslate nohighlight">\(x_{\max}\)</span> For all the data,</p>
<div class="math notranslate nohighlight">
\[\widetilde{x} = \dfrac{x - x_{\min}}{x_{\max} - x_{\min}}\]</div>
<p>Calculation is performed. Although Min-Max scaling has the merit of simple calculation, on the other hand, as shown in the figure below <span class="math notranslate nohighlight">\(x_1\)</span> If data points with outliers are present, <span class="math notranslate nohighlight">\(x_{\max}\)</span> There is a weak point that it is greatly pulled by outliers.</p>
<p><img alt="image13" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/16.png" /></p>
<p>Another way of scaling is to scale to an <strong>average of 0</strong> and a <strong>standard deviation of 1</strong> . This is commonly referred to as <strong>normalization</strong> . When you subtract the average from all data, the average <span class="math notranslate nohighlight">\(0\)</span> When dividing by the standard deviation, the standard deviation becomes 1 It becomes.</p>
<div class="math notranslate nohighlight">
\[\widetilde{x}  = \dfrac{x - \bar{x}}{\sigma}\]</div>
<p>Applying this scaling to the example ‚ë† in which the variance was calculated,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
x_{1}&amp;=\dfrac {-2-0}{\sqrt {2}}=-\dfrac {2}{\sqrt {2}}\\
x_{2}&amp;=\dfrac {-1-0}{\sqrt {2}}=-\dfrac {1}{\sqrt {2}}\\
x_{3}&amp;=\dfrac {0-0}{\sqrt {2}}=0\\
x_{4}&amp;=\dfrac {1-0}{\sqrt {2}}=\dfrac {1}{\sqrt {2}}\\
x_{5}&amp;=\dfrac {2-0}{\sqrt {2}}=\dfrac {2}{\sqrt {2}}
\end{aligned}\end{split}\]</div>
<p>The data is converted like this. When we look at the mean and standard deviation at this time,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\overline {x}&amp;=\dfrac {1}{5}\left( -\dfrac {2}{\sqrt {2}}-\dfrac {1}{\sqrt {2}}+0+\dfrac {1}{\sqrt {2}}+\dfrac {2}{\sqrt {2}}\right) =0\\
\sigma ^{2}&amp;=\dfrac {1}{5}\left\{ \left( -\dfrac {2}{\sqrt {2}}-0\right) ^{2}+\left( -\dfrac {1}{\sqrt {2}}-0\right) ^{2}+\left( 0-0\right) ^{2}
 +\left( \dfrac {1}{\sqrt {2}}-0\right) ^{2}+\left( \dfrac {2}{\sqrt {2}}-0\right) ^{2}\right\} =1\\
\sigma &amp;=\sqrt {\sigma ^{2}}=1
\end{aligned}\end{split}\]</div>
<p>As you can see, you can scaled to 0 average and 1 standard deviation. With this method, strong scaling can be achieved for a small number of outliers compared with Min-Max scaling.</p>
</div>
<div class="section" id="Outlier-removal">
<h3>1.4.8. Outlier removal<a class="headerlink" href="#Outlier-removal" title="Permalink to this headline">¬∂</a></h3>
<p>Let‚Äôs handle data that varies with time as follows. For example, the horizontal axis is time and the vertical axis is temperature. Also assume that the average temperature is constant and the temperature fluctuates randomly.</p>
<p><img alt="image14" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/17.png" /></p>
<p>If you want to detect abnormality (outlier) of temperature due to abnormality of thermometer or failure of this data, how should we define and detect this outlier? One way is to focus on the <strong>frequency</strong> of values .</p>
<p><img alt="image15" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/18.png" /></p>
<p>As shown above, if you draw a line against the mean, calculate the frequency at each value and draw a histogram, a normal distribution appears. Here we assume that normality can be assumed for the distribution followed by the data (there is a method such as normality test if you want to check statistically whether the data follows the normal distribution), to define outliers, Average of data <span class="math notranslate nohighlight">\(\mu\)</span> And standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span> Is calculated, <span class="math notranslate nohighlight">\(\mu \pm 3\sigma\)</span> You can remove
outliers by subtracting a line to the value of. This is called the 3œÉ method . However, when the number of outliers increases or the outliers have extreme values, the mean and standard deviation are pulled to their outliers, and the 3œÉ method may not be able to cope well.</p>
<p>In that case, you can also arrange the data in descending order and remove the top 5% and bottom 5%.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="Introduction_to_ML_libs.html" class="btn btn-neutral float-right" title="2. Basics of machine learning" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../index.html" class="btn btn-neutral" title="„É°„Éá„Ç£„Ç´„É´AIÂ∞ÇÈñÄ„Ç≥„Éº„Çπ „Ç™„É≥„É©„Ç§„É≥Ë¨õÁæ©Ë≥áÊñô" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Preferred Networks &amp; „Ç≠„Ç´„Ç¨„ÇØ

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>