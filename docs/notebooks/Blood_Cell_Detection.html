

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>6. 6. Practice section: Detection of cells from microscopic images of &mdash; メディカルAI専門コース オンライン講義資料  documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="7. 7. practical part: sequence analysis using deep learning" href="DNA_Sequence_Data_Analysis.html" />
    <link rel="prev" title="5. 5. Practice: Segmentation of MRI" href="Image_Segmentation.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-797798-11"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'UA-797798-11');
  </script>

  <meta name="description" content="メディカルAI学会公認資格向けオンライン講義資料。機械学習に必要な数学の基礎の解説から深層学習（ディープラーニング）を用いた実践的な内容までGoogle Colaboratory上でGPUを用いて実際にコードを実行可能な形式にしオンライン資料として無料公開。">
  <meta property="og:title" content="メディカルAI専門コース オンライン講義資料">
  <meta property="og:description" content="メディカルAI学会公認資格向けオンライン講義資料。機械学習に必要な数学の基礎の解説から深層学習（ディープラーニング）を用いた実践的な内容までGoogle Colaboratory上でGPUを用いて実際にコードを実行可能な形式にしオンライン資料として無料公開。">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://japan-medical-ai.github.io/medical-ai-course-materials/">
  <meta property="og:image" content="https://raw.githubusercontent.com/japan-medical-ai/medical-ai-course-materials/master/notebooks/images/medical_ai.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@PreferredNetJP">
  <meta name="twitter:creator" content="@PreferredNetJP">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> メディカルAI専門コース オンライン講義資料
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Basic_Math_for_ML.html">1. 1. basis of the mathematics required to machine learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Introduction_to_ML_libs.html">2. 2. Basics of machine learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Introduction_to_Neural_Network.html">3. Basics of neural network</a></li>
<li class="toctree-l1"><a class="reference internal" href="Introduction_to_Chainer.html">4. Introduction to Deep Learning Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="Image_Segmentation.html">5. 5. Practice: Segmentation of MRI</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">6. 6. Practice section: Detection of cells from microscopic images of</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#6.1.-Environment">6.1. 6.1. Environment</a></li>
<li class="toctree-l2"><a class="reference internal" href="#6.2.-Object-Detection-(Object-detection)">6.2. 6.2. Object Detection (Object detection)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#6.3.-Preparing-the-data">6.3. 6.3. Preparing the data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#6.3.1.-Data-set-download">6.3.1. 6.3.1. Data set download</a></li>
<li class="toctree-l3"><a class="reference internal" href="#6.3.2.-Creating-Dataset">6.3.2. 6.3.2. Creating Dataset</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#6.4.-Single-Shot-Multibox-Detector-(SSD)">6.4. 6.4. Single Shot Multibox Detector (SSD)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#6.5.-Model-of-definition">6.5. 6.5. Model of definition</a></li>
<li class="toctree-l2"><a class="reference internal" href="#6.6.-Data-augmentation-of-implementation">6.6. 6.6. Data augmentation of implementation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#6.7.-The-start-of-the-learning">6.7. 6.7. The start of the learning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#6.7.1.-Evaluation">6.7.1. 6.7.1. Evaluation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#6.8.-Inference-using-learning">6.8. 6.8. Inference using learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="#6.9.-Evaluating-learned">6.9. 6.9. Evaluating learned</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="DNA_Sequence_Data_Analysis.html">7. 7. practical part: sequence analysis using deep learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Sequential_Data_Analysis_with_Deep_Learning.html">8. Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="Sequential_Data_Analysis_with_Deep_Learning.html#8.2.-Electrocardiogram-(ECG)-and-arrhythmia-diagnosis">9. 8.2. Electrocardiogram (ECG) and arrhythmia diagnosis</a></li>
<li class="toctree-l1"><a class="reference internal" href="Sequential_Data_Analysis_with_Deep_Learning.html#8.3.-Data-sets">10. 8.3. Data sets</a></li>
<li class="toctree-l1"><a class="reference internal" href="Sequential_Data_Analysis_with_Deep_Learning.html#8.4.-Data-pre-processing">11. 8.4. Data pre-processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="Sequential_Data_Analysis_with_Deep_Learning.html#8.5.-Series-data-analysis-when-using-the-deep-learning">12. 8.5. Series data analysis when using the deep learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Sequential_Data_Analysis_with_Deep_Learning.html#8.5.2.-Evaluation">13. 8.5.2. Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="Sequential_Data_Analysis_with_Deep_Learning.html#8.6.-Towards-the-accuracy">14. 8.6. Towards the accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="Sequential_Data_Analysis_with_Deep_Learning.html#8.7.-Conclusion">15. 8.7. Conclusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="Sequential_Data_Analysis_with_Deep_Learning.html#8.8.-References">16. 8.8. References</a></li>
</ul>

            
          
          <div style="padding-right:20px; bottom:10px;">
            <a href="https://short-term.kikagaku.co.jp/dnn-seminar/">
              <img src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/img_handson.png" />
              <p style="padding:5px; font-size:small; line-height: 150%">ディープラーニングの詳しい解説や画像・自然言語の取り扱い、クラウド上のGPUを使った実践的な演習をご希望の方はこちらがおすすめです</p>
            </a>
          </div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">メディカルAI専門コース オンライン講義資料</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>6. 6. Practice section: Detection of cells from microscopic images of</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/notebooks/Blood_Cell_Detection.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 5ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    background: #f5f5f5;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<p><a class="reference external" href="https://colab.research.google.com/github/preferred-medicine/medical-ai-course-materials/blob/master/notebooks/Blood_Cell_Detection.ipynb"><img alt="colab-logo" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<div class="section" id="6.-Practice-section:-Detection-of-cells-from-microscopic-images-of">
<h1>6. 6. Practice section: Detection of cells from microscopic images of<a class="headerlink" href="#6.-Practice-section:-Detection-of-cells-from-microscopic-images-of" title="Permalink to this headline">¶</a></h1>
<p>Here we are working on the task of detecting blood cells. When a microscope image of human blood is given,</p>
<ul class="simple">
<li>Red Blood Cell (RBC)</li>
<li>White Blood Cell (WBC)</li>
<li>Platelet For each of the three types of cells, think about a method to recognize individually what each position is in . When this is possible, you can see how many of those cells are in a given image, and how they are located .</li>
</ul>
<p>Such tasks are commonly referred to as object detection . By inputting the image as an input, for each object of interest (here, for example, the above three kinds of cells), individually</p>
<ol class="arabic simple">
<li>The smallest area rectangle (called Bounding box)</li>
<li>“What is inside object” = class label</li>
</ol>
<p>The purpose is to estimate. However, since it is not known beforehand how many objects are included in the image, it is necessary to be a method capable of outputting a set of predicted values ​​of a bounding box of an arbitrary number (or a sufficient number) of objects and a class label There is</p>
<p>Bounding box (hereinafter bbox) is <code class="docutils literal notranslate"><span class="pre">The</span> <span class="pre">y</span> <span class="pre">coordinate</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">upper</span> <span class="pre">left</span> <span class="pre">corner</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">rectangle</span></code>, <code class="docutils literal notranslate"><span class="pre">The</span> <span class="pre">x</span> <span class="pre">coordinate</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">upper</span> <span class="pre">left</span> <span class="pre">corner</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">rectangle</span></code>, <code class="docutils literal notranslate"><span class="pre">The</span> <span class="pre">y</span> <span class="pre">coordinate</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">lower</span> <span class="pre">right</span> <span class="pre">corner</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">rectangle</span></code>, <code class="docutils literal notranslate"><span class="pre">The</span> <span class="pre">x</span> <span class="pre">coordinate</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">lower</span> <span class="pre">right</span> <span class="pre">corner</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">rectangle</span></code> form is the often defined as the class seems often represented by ID assigned to each type of object (hereinafter class label) . For example, it is common for non-negative integers corresponding to one-to-one
correspondence to be allocated such as 0 for RBC, 1 for WBC and 2 for Platelet.</p>
<p>Below, we show an example from the data set of the cell image used in this article, and show the bbox given as correct answer on the image and the name of the corresponding class visualized.</p>
<p>The red rectangle is what is called bbox. You can see that different rectangles surround each target blood cell one by one. A white label is displayed so as to overlap the upper side of this rectangle. It represents the type (class) of the object inside the rectangle.</p>
<p><img alt="血液の顕微鏡画像からRBC, WBC, Plateletを検出している例" src="https://github.com/mitmul/medical-ai-course-materials/raw/master/notebooks/images/detection_samples.png" /></p>
<div class="section" id="6.1.-Environment">
<h2>6.1. 6.1. Environment<a class="headerlink" href="#6.1.-Environment" title="Permalink to this headline">¶</a></h2>
<p>First of all, let’s finish installing Python package such as Chainer, CuPy, ChainerCV, matplotlib by executing the following cell on Colab for environment building. These steps are the same as before.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="o">!</span>curl https://colab.chainer.org/install <span class="p">|</span> sh -  # ChainerとCuPyのインストール
<span class="o">!</span>pip install chainercv matplotlib               # ChainerCVとmatplotlibのインストール
</pre></div>
</div>
</div>
<p>Let’s confirm by executing the following cells that the setup of the environment succeeded by executing the previous cell.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">chainer</span>

<span class="n">chainer</span><span class="o">.</span><span class="n">print_runtime_info</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Platform: Linux-4.14.65+-x86_64-with-Ubuntu-18.04-bionic
Chainer: 5.0.0
NumPy: 1.14.6
CuPy:
  CuPy Version          : 5.0.0
  CUDA Root             : /usr/local/cuda
  CUDA Build Version    : 9020
  CUDA Driver Version   : 9020
  CUDA Runtime Version  : 9020
  cuDNN Build Version   : 7201
  cuDNN Version         : 7201
  NCCL Build Version    : 2213
iDeep: 2.0.0.post3
</pre></div></div>
</div>
</div>
<div class="section" id="6.2.-Object-Detection-(Object-detection)">
<h2>6.2. 6.2. Object Detection (Object detection)<a class="headerlink" href="#6.2.-Object-Detection-(Object-detection)" title="Permalink to this headline">¶</a></h2>
<p>Object detection is one of the tasks that is still being actively studied in Computer Vision application field, and it plays an important role in a wide range of fields such as automatic driving and robotics. Unlike Semantic Segmentation, we do not recognize the shape (contour) of the object, but we will output the type and position of the object separately for each object .</p>
<p>When we call a “kind of thing” a class, we can call an individual object belonging to that class as an instance. Then, when there are pictures of two dogs, it can be said that there are two instances belonging to the class “dog”. In other words, the previous is task of Semantic Segmentation learned by chapter did not mean area for each instance is output are distinguished on the one hand, the output of the object detection becomes each instance (separate bbox each instance the output It will be
different ) There is a difference. Sometimes we express the form of such output with the word “instance - wise”.</p>
<p>Starting with the method announced in 2014 called <a class="reference external" href="https://arxiv.org/abs/1311.2524">R-CNN</a> , various refinement methods have been proposed for the object detection method using the neural network . First, as one flow, object detection methods such as <a class="reference external" href="https://arxiv.org/abs/1311.2524">R-CNN</a> , <a class="reference external" href="https://arxiv.org/abs/1504.08083">Fast R-CNN</a> , and <a class="reference external" href="https://arxiv.org/abs/1506.01497">Faster R-CNN</a> estimate object candidates first, then estimate object classes and positions in detail for
each candidate I will. This is called a <strong>two stage</strong> type .</p>
<p>On the other hand, although it is based on CNN, there is a method called <strong>single stage</strong> type . <a class="reference external" href="https://arxiv.org/abs/1512.02325">SSD</a> , <a class="reference external" href="https://arxiv.org/abs/1506.02640">YOLO</a> , <a class="reference external" href="https://arxiv.org/abs/1612.08242">YOLOv2</a> , <a class="reference external" href="https://arxiv.org/abs/1804.02767">YOLOv3</a> , etc. are well known as single stage type. They do not generate candidates for objects. Estimate the class and position of each object directly. In general, the single stage type is said to be faster than the two
stage type, while the accuracy is said to be lower. However, recently the boundaries of these methods have become ambiguous, and performance differences are almost gone.</p>
<p>In this case, we will challenge the task of extracting the position and type of three types of cells from cell images using SSD, one of single stage type object detection methods.</p>
</div>
<div class="section" id="6.3.-Preparing-the-data">
<h2>6.3. 6.3. Preparing the data<a class="headerlink" href="#6.3.-Preparing-the-data" title="Permalink to this headline">¶</a></h2>
<div class="section" id="6.3.1.-Data-set-download">
<h3>6.3.1. 6.3.1. Data set download<a class="headerlink" href="#6.3.1.-Data-set-download" title="Permalink to this headline">¶</a></h3>
<p>First , prepare a data set of blood microscopic images called <a class="reference external" href="https://github.com/Shenggan/BCCD_Dataset">BCCD Dataset</a> . This data set contains 364 images and XML files with file names corresponding to each image. In the XML file, the coordinate information of the bounding box surrounding one of three cells, RBC, WBC, Platelet, which appeared in the corresponding image, is stored. Because there are cases where multiple images are contained in one image, the XML file may contain descriptions
about multiple cells.</p>
<p>The BCCD Dataset is very small compared to the benchmark dataset widely used for object detection research, and it is distributed on Github. Let’s download the dataset by executing the following cell.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="o">!</span><span class="k">if</span> <span class="o">[</span> ! -d BCCD_Dataset <span class="o">]</span><span class="p">;</span> <span class="k">then</span> git clone https://github.com/Shenggan/BCCD_Dataset.git<span class="p">;</span> <span class="k">fi</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Cloning into &#39;BCCD_Dataset&#39;...
remote: Enumerating objects: 770, done.
remote: Total 770 (delta 0), reused 0 (delta 0), pack-reused 770
Receiving objects: 100% (770/770), 7.33 MiB | 9.58 MiB/s, done.
Resolving deltas: 100% (367/367), done.
</pre></div></div>
</div>
<p>When the download is complete, <code class="docutils literal notranslate"><span class="pre">BCCD_Datasetlet</span></code>’s look at the file structure under the directory. This data set is distributed with the following file structure.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>BCCD
|-- Annotations
|   |
|   `-- BloodImage_00XYZ.xml (364 items)
|
|-- ImageSets
|   |
|   `-- Main
|       |
|       |-- test.txt
|       |-- train.txt
|       `-- val.txt
|
`-- JPEGImages
  |
   `-- BloodImage_00XYZ.jpg (364 items)
</pre></div>
</div>
<p>This configuration is consistent with the format of the <strong>Pascal VOC dataset</strong> that has been used as a standard benchmark dataset for object detection for many years . Therefore, it is possible to divert classes that make it easy to handle Pascal VOC data set prepared by ChainerCV.</p>
<p>Actually there are other directories, but this time it is only included in the above file tree. I will explain what is included in each directory.</p>
<ul class="simple">
<li><strong>Annotations directory:</strong> In the same format as the Pascal VOC data set, correct answer information on <strong>what position is present</strong> for each cell image is stored. Correct information is stored as an XML file, and it is saved with the same file name except the extension so that correspondence with the image file is easy to understand.</li>
<li><strong>ImageSets directory:</strong> A text file containing a list of images to be used for each of the learning data set (train), the verification data set (val), and the test data set (test). According to these lists, divided into three parts data sets, respectively <code class="docutils literal notranslate"><span class="pre">train.txt</span></code> learning listed image, <code class="docutils literal notranslate"><span class="pre">val.txt</span></code> the validation listed image (data set split used to examine the generalization performance roughly in learning) , For <code class="docutils literal notranslate"><span class="pre">test.txt</span></code> the final performance evaluation after learning is
completed.</li>
<li><strong>JPEGImages directory:</strong> Contains all image data included in this data set.</li>
</ul>
</div>
<div class="section" id="6.3.2.-Creating-Dataset">
<h3>6.3.2. 6.3.2. Creating Dataset<a class="headerlink" href="#6.3.2.-Creating-Dataset" title="Permalink to this headline">¶</a></h3>
<p>ChainerCV has convenient classes for easily reading Pascal VOC dataset. We inherit this and <code class="docutils literal notranslate"><span class="pre">_get_annotations</span></code> override the method so that we can read the dataset we will use this time. Only one line needs to be changed. Let’s copy the corresponding code ( method part) from <a class="reference external" href="https://github.com/chainer/chainercv/blob/v0.10.0/chainercv/datasets/voc/voc_bbox_dataset.py#L90-L115">here</a>,<code class="docutils literal notranslate"><span class="pre">_get_annotations</span></code> make the following changes, and add it as a method method of the
<code class="docutils literal notranslate"><span class="pre">VOCBboxDataset</span></code>inheriting <code class="docutils literal notranslate"><span class="pre">BCCDDataset</span></code>class. (This means that it is called diff format - it deletes the line that begins with - and adds the line that starts with +)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">-</span> <span class="n">label</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">voc_utils</span><span class="o">.</span><span class="n">voc_bbox_label_names</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
<span class="o">+</span> <span class="n">label</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bccd_labels</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
</pre></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">xml.etree.ElementTree</span> <span class="kn">as</span> <span class="nn">ET</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">chainercv.datasets</span> <span class="kn">import</span> <span class="n">VOCBboxDataset</span>


<span class="n">bccd_labels</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;rbc&#39;</span><span class="p">,</span> <span class="s1">&#39;wbc&#39;</span><span class="p">,</span> <span class="s1">&#39;platelets&#39;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">BCCDDataset</span><span class="p">(</span><span class="n">VOCBboxDataset</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">_get_annotations</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
        <span class="n">id_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ids</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

        <span class="c1"># Pascal VOC形式のアノテーションデータは，XML形式で配布されています</span>
        <span class="n">anno</span> <span class="o">=</span> <span class="n">ET</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span>
            <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data_dir</span><span class="p">,</span> <span class="s1">&#39;Annotations&#39;</span><span class="p">,</span> <span class="n">id_</span> <span class="o">+</span> <span class="s1">&#39;.xml&#39;</span><span class="p">))</span>

        <span class="c1"># XMLを読み込んで，bboxの座標・大きさ，bboxごとのクラスラベルなどの</span>
        <span class="c1"># 情報を取り出し，リストに追加していきます</span>
        <span class="n">bbox</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">label</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">difficult</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">obj</span> <span class="ow">in</span> <span class="n">anno</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s1">&#39;object&#39;</span><span class="p">):</span>
            <span class="n">bndbox_anno</span> <span class="o">=</span> <span class="n">obj</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="s1">&#39;bndbox&#39;</span><span class="p">)</span>

            <span class="c1"># bboxの座標値が0-originになるように1を引いています</span>
            <span class="c1"># subtract 1 to make pixel indexes 0-based</span>
            <span class="n">bbox</span><span class="o">.</span><span class="n">append</span><span class="p">([</span>
                <span class="nb">int</span><span class="p">(</span><span class="n">bndbox_anno</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">tag</span><span class="p">)</span><span class="o">.</span><span class="n">text</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
                <span class="k">for</span> <span class="n">tag</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;ymin&#39;</span><span class="p">,</span> <span class="s1">&#39;xmin&#39;</span><span class="p">,</span> <span class="s1">&#39;ymax&#39;</span><span class="p">,</span> <span class="s1">&#39;xmax&#39;</span><span class="p">)])</span>
            <span class="n">name</span> <span class="o">=</span> <span class="n">obj</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
            <span class="n">label</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bccd_labels</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
        <span class="n">bbox</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">bbox</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">label</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">label</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

        <span class="c1"># オリジナルのPascal VOCには，difficultという</span>
        <span class="c1"># 属性が画像ごとに真偽値で与えられていますが，今回は用いません</span>
        <span class="c1"># （今回のデータセットでは全画像がdifficult = 0に設定されているため）</span>
        <span class="c1"># When `use_difficult==False`, all elements in `difficult` are False.</span>
        <span class="n">difficult</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">difficult</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">bbox</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">difficult</span>
</pre></div>
</div>
</div>
<p>By the way, I was able to prepare a class to do data loading etc. to use data set for learning, verification, test etc etc. Let’s immediately create a dataset object for learning, verification and testing using this class.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">BCCDDataset</span><span class="p">(</span><span class="s1">&#39;BCCD_Dataset/BCCD&#39;</span><span class="p">,</span> <span class="s1">&#39;train&#39;</span><span class="p">)</span>
<span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">BCCDDataset</span><span class="p">(</span><span class="s1">&#39;BCCD_Dataset/BCCD&#39;</span><span class="p">,</span> <span class="s1">&#39;val&#39;</span><span class="p">)</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">BCCDDataset</span><span class="p">(</span><span class="s1">&#39;BCCD_Dataset/BCCD&#39;</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/usr/local/lib/python3.6/dist-packages/chainercv/datasets/voc/voc_bbox_dataset.py:63: UserWarning: please pick split from &#39;train&#39;, &#39;trainval&#39;, &#39;val&#39;for 2012 dataset. For 2007 dataset, you can pick &#39;test&#39; in addition to the above mentioned splits.
  &#39;please pick split from \&#39;train\&#39;, \&#39;trainval\&#39;, \&#39;val\&#39;&#39;
</pre></div></div>
</div>
<p>A warning may be displayed here, but you do not have to worry about it. It is because it uses the class which was originally specialized only for Pascal VOC data set for BCCD Dataset.</p>
<p>Well, we were able to create three dataset objects. Let’s check each size (how many data are included).</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="k">print</span><span class="p">(</span><span class="s1">&#39;Number of images in &quot;train&quot; dataset:&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Number of images in &quot;valid&quot; dataset:&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Number of images in &quot;test&quot; dataset:&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Number of images in &#34;train&#34; dataset: 205
Number of images in &#34;valid&#34; dataset: 87
Number of images in &#34;test&#34; dataset: 72
</pre></div></div>
</div>
<p>Now, <code class="docutils literal notranslate"><span class="pre">train_dataset</span></code> let’s access to the first data.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="n">first_datum</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<p>Well, <code class="docutils literal notranslate"><span class="pre">train_dataset</span></code> teeth <code class="docutils literal notranslate"><span class="pre">VOCBboxDataset</span></code> inherited <code class="docutils literal notranslate"><span class="pre">BCCDDataset</span></code> was the object of the class. Therefore, <code class="docutils literal notranslate"><span class="pre">_get_annotations</span></code> except for the methods overridden above, <code class="docutils literal notranslate"><span class="pre">VOCBboxDataset</span></code> you should inherit the functionality provided by the class. <code class="docutils literal notranslate"><span class="pre">VOCBboxDataset</span></code> Let’s see what kind of function is provided , see the class documentation: <a class="reference external" href="https://chainercv.readthedocs.io/en/stable/reference/datasets.html?highlight=VOCBboxDataset#vocbboxdataset">VOCBboxDataset</a></p>
<p>The following table is stated. This dataset looks like a list with the following in each element:</p>
<table border="1" class="docutils">
<colgroup>
<col width="39%" />
<col width="12%" />
<col width="12%" />
<col width="37%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">name</th>
<th class="head">shape</th>
<th class="head">dtype</th>
<th class="head">format</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>img</td>
<td>(3,H,W)</td>
<td>float32</td>
<td>RGB, [0,255]</td>
</tr>
<tr class="row-odd"><td>bbox</td>
<td>(R,4)</td>
<td>float32</td>
<td>(ymin,xmin,ymax,xmax)</td>
</tr>
<tr class="row-even"><td>label</td>
<td>(R,)</td>
<td>int32</td>
<td>[0,#fg_class−1]</td>
</tr>
<tr class="row-odd"><td>difficult (optional)*</td>
<td>(R,)</td>
<td>bool</td>
<td>–</td>
</tr>
</tbody>
</table>
<ul class="simple">
<li>#fg_class is the number of classes in foreground (foreground)</li>
<li>Difficult only when <code class="docutils literal notranslate"><span class="pre">return_difficult</span> <span class="pre">=</span> <span class="pre">True</span></code></li>
</ul>
<p>However, since the <code class="docutils literal notranslate"><span class="pre">return_difficult</span></code> option <code class="docutils literal notranslate"><span class="pre">True</span></code> has not been specified explicitly when creating the dataset object this time , the default value <code class="docutils literal notranslate"><span class="pre">False</span></code>is used. Therefore <code class="docutils literal notranslate"><span class="pre">difficult</span></code> we will not return the element which is in the last row of the above table .</p>
<p>All of the three dataset objects created this time are three arrays of each element <code class="docutils literal notranslate"><span class="pre">Image</span> <span class="pre">data</span></code>,<code class="docutils literal notranslate"><span class="pre">correct</span> <span class="pre">bbox</span> <span class="pre">list</span></code>, <code class="docutils literal notranslate"><span class="pre">class</span> <span class="pre">for</span> <span class="pre">each</span> <span class="pre">bbox</span></code></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="nb">len</span><span class="p">(</span><span class="n">first_datum</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>3
</pre></div>
</div>
</div>
<p>Certainly, the number of elements was three. Let’s take out image data and look at its shape and dtype.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="k">print</span><span class="p">(</span><span class="n">first_datum</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">first_datum</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-1-8bfb1700b6fb&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> </span><span class="ansi-green-fg">print</span><span class="ansi-blue-fg">(</span>first_datum<span class="ansi-blue-fg">[</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">.</span>shape<span class="ansi-blue-fg">,</span> first_datum<span class="ansi-blue-fg">[</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">.</span>dtype<span class="ansi-blue-fg">)</span>

<span class="ansi-red-fg">NameError</span>: name &#39;first_datum&#39; is not defined
</pre></div></div>
</div>
<p>Indeed, it is in the form of, and the data type has become. It was as it was in the table above. So what format is bbox like? I will display contents and show its shape. <code class="docutils literal notranslate"><span class="pre">(3</span> <span class="pre">=</span> <span class="pre">number</span> <span class="pre">of</span> <span class="pre">channels,</span> <span class="pre">H</span> <span class="pre">=</span> <span class="pre">height,</span> <span class="pre">W</span> <span class="pre">=</span> <span class="pre">width)</span></code> <code class="docutils literal notranslate"><span class="pre">float32</span></code></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="k">print</span><span class="p">(</span><span class="n">first_datum</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">first_datum</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[314.  67. 479. 285.]
 [360. 345. 453. 445.]
 [178.  52. 298. 145.]
 [399. 448. 479. 535.]
 [131. 460. 211. 547.]
 [294. 453. 374. 540.]
 [282. 416. 382. 507.]
 [341. 277. 450. 368.]
 [ 61. 544. 158. 635.]
 [ 90. 484. 187. 575.]
 [170. 375. 252. 437.]
 [176. 328. 270. 394.]
 [ 58. 290. 167. 406.]
 [  0. 298.  67. 403.]
 [ 25. 345. 137. 448.]
 [  0. 133.  94. 240.]
 [ 37.   0. 163.  97.]
 [159. 164. 263. 256.]
 [208. 463. 318. 565.]]
(19, 4)
</pre></div></div>
</div>
<p>Information of 19 bboxes is lined up, and each one is represented by four numbers. These four numbers represent the image coordinates (the position on the image plane) of the upper left and lower right of the bbox. <code class="docutils literal notranslate"><span class="pre">(y_min,</span> <span class="pre">x_min,</span> <span class="pre">y_max,</span> <span class="pre">x_max)</span></code></p>
<p>For each object appearing in the image, outputting these four numbers is one object detection object. However, in addition to that, it also needs to output which class each bbox belongs to (the type of object inside that bbox). Correct information about this is contained in the last element. I will show this.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="k">print</span><span class="p">(</span><span class="n">first_datum</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">first_datum</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
(19,)
</pre></div></div>
</div>
<p>There were 19 numbers. <code class="docutils literal notranslate"><span class="pre">first_datum[1]</span></code> Each of them corresponds to bbox ( ) displayed above in order, and it indicates which class each bbox belongs to (0: RBC, 1: WBC, 2: Platelet).</p>
<p>Let’s visualize and check one data in the dataset which is grouped by these three elements at the end of this section. We extract one image extracted from the train data set and its corresponding class label of bbox and their corresponding class labels, display the image using a convenient function for visualization prepared by ChainerCV, and there correspond to the bounding box Let’s display the names of classes to be superimposed.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">from</span> <span class="nn">chainercv.visualizations</span> <span class="kn">import</span> <span class="n">vis_bbox</span>

<span class="n">img</span><span class="p">,</span> <span class="n">bbox</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">vis_bbox</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">bbox</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">label_names</span><span class="o">=</span><span class="n">bccd_labels</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">figure</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Blood_Cell_Detection_29_0.png" src="../_images/notebooks_Blood_Cell_Detection_29_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="6.4.-Single-Shot-Multibox-Detector-(SSD)">
<h2>6.4. 6.4. Single Shot Multibox Detector (SSD)<a class="headerlink" href="#6.4.-Single-Shot-Multibox-Detector-(SSD)" title="Permalink to this headline">¶</a></h2>
<p>Data preparation is completed.</p>
<p>Next, I will briefly explain the model to train this time. For this time we will use the technique called <a class="reference external" href="https://arxiv.org/abs/1512.02325">Single Shot MultiBox Detector (SSD)</a></p>
<p>SSD is a kind of object detection method called single stage type as mentioned above. First, <strong>feature maps</strong> are extracted from images by using a network structure which has achieved great results with image classification like VGG or ResNet . We prepare candidates for each position of feature maps (SSD paper is called default box, but anchor is more commonly used). Each candidate region has a different form (square, portrait, landscape, different sizes, etc.). For example, we prepare 16 x 16
candidates, 16 x 12 candidates, 12 x 16 candidates at the position of (x = 0, y = 0) in the feature map. Then find the candidate that is most <strong>appropriate</strong> for the <strong>correct answer</strong>, calculate <strong>how far candidates are deviated from the correct bounding box</strong>, and learn to minimize this deviation. This and the one that is reflected at the same time in the area inside each <strong>one belongs to which class</strong> is also predicted, make the learning as also to reduce this mistake. Ensure that you can
predict that no candidate that did not match any correct answer was in that position. For details on this process, please refer to the <a class="reference external" href="https://arxiv.org/abs/1512.02325">original paper</a>.</p>
<p>On the other hand, in a two stage type technique, for example Faster R-CNN, another network predicts a candidate region (region proposal) of an object with respect to the extracted feature map, and using the result, features of each candidate region create a vector (is used a calculation called RoI pooling), they <strong>further pass to two different small networks for solving regression problem of finding the correction amount for the position and size of the classification problems and candidate
region</strong>, that Take a structure.</p>
<p>For this reason, it is generally said that a single stage type network is faster. On the other hand, it is said that the accuracy of two stage type one is higher. For such a trade-off , the following figure is often referred to from the paper (<a class="reference external" href="https://arxiv.org/abs/1611.10012">Speed/accuracy trade-offs for modern convolutional object detectors</a>) comparing various object detection methods.</p>
<p><img alt="予測精度と実行速度の関係" src="https://github.com/mitmul/medical-ai-course-materials/raw/master/notebooks/images/speed-accuracy-tradeoffs.png" /></p>
<p>Well, the network architecture of the SSD method used this time has the following form (cited from Fig. 2 of the SSD paper).</p>
<p><img alt="SSDのネットワーク構造" src="https://github.com/mitmul/medical-ai-course-materials/raw/master/notebooks/images/ssd-architecture.png" /></p>
<p>The VGG-16 network that performs feature extraction is constructed by stacking many convolutional layers, and the pooling process is applied to each of a group of several convolutional layers to lower the resolution of the feature map, and the layer It is designed to acquire more abstract expressions as they accumulate. Therefore, by holding the intermediate output at the time when the data passed through each block, by finally utilizing the intermediate output (the feature map of different
size) taken out from a plurality of different depths, It is a feature of SSD that it enables consideration of multiple scales.</p>
</div>
<div class="section" id="6.5.-Model-of-definition">
<h2>6.5. 6.5. Model of definition<a class="headerlink" href="#6.5.-Model-of-definition" title="Permalink to this headline">¶</a></h2>
<p>Implementation of the network part of SSD is provided by ChainerCV. <code class="docutils literal notranslate"><span class="pre">chainercv.links.SSD300</span></code> The class called ChainerCV represents a model of SSD with input of 300 pixels vertically and horizontally, and by default the feature extractor uses 16 layers of network structure called <a class="reference external" href="https://arxiv.org/abs/1409.1556">VGG16</a>.</p>
<p>Let’s prepare a class to calculate the loss function necessary for learning.</p>
<p>Class that defined below, and the object of the first SSD model, is a hyper-parameters for the loss calculation <code class="docutils literal notranslate"><span class="pre">alpha</span></code> and <code class="docutils literal notranslate"><span class="pre">k</span></code> has received a constructor. <code class="docutils literal notranslate"><span class="pre">alpha</span></code> is a coefficient that weights between error for position prediction and error for class prediction, respectively. <code class="docutils literal notranslate"><span class="pre">k</span></code> is a parameter for hard negative mining. At learning, for one correct bounding box, the model outputs at least one close (positive) prediction and many false (negative) predictions. Sort this many wrong
predictions by the confidence score (value expressing how confidently the model is outputting that prediction), then negative samples from positive to negative as 1: k above Select it and use it to calculate the loss. It is <code class="docutils literal notranslate"><span class="pre">k</span></code> a parameter that determines this balance, and in the above paper 𝑘=3 Because it is said, here also uses 3 by default.</p>
<p><code class="docutils literal notranslate"><span class="pre">forward</span></code> In the method, we receive the input image and the list of correct positions and labels, and actually calculate the loss. Object detection solves two problems of object localization (prediction of position) and classification (prediction of type (= class)), SSD calculates localization loss and classification loss separately.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">chainer</span>
<span class="kn">from</span> <span class="nn">chainercv.links</span> <span class="kn">import</span> <span class="n">SSD300</span>
<span class="kn">from</span> <span class="nn">chainercv.links.model.ssd</span> <span class="kn">import</span> <span class="n">multibox_loss</span>


<span class="k">class</span> <span class="nc">MultiboxTrainChain</span><span class="p">(</span><span class="n">chainer</span><span class="o">.</span><span class="n">Chain</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiboxTrainChain</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_scope</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">imgs</span><span class="p">,</span> <span class="n">gt_mb_locs</span><span class="p">,</span> <span class="n">gt_mb_labels</span><span class="p">):</span>
        <span class="n">mb_locs</span><span class="p">,</span> <span class="n">mb_confs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span>
        <span class="n">loc_loss</span><span class="p">,</span> <span class="n">conf_loss</span> <span class="o">=</span> <span class="n">multibox_loss</span><span class="p">(</span>
            <span class="n">mb_locs</span><span class="p">,</span> <span class="n">mb_confs</span><span class="p">,</span> <span class="n">gt_mb_locs</span><span class="p">,</span> <span class="n">gt_mb_labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loc_loss</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">+</span> <span class="n">conf_loss</span>

        <span class="n">chainer</span><span class="o">.</span><span class="n">reporter</span><span class="o">.</span><span class="n">report</span><span class="p">(</span>
            <span class="p">{</span><span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;loss/loc&#39;</span><span class="p">:</span> <span class="n">loc_loss</span><span class="p">,</span> <span class="s1">&#39;loss/conf&#39;</span><span class="p">:</span> <span class="n">conf_loss</span><span class="p">},</span>
            <span class="bp">self</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">SSD300</span><span class="p">(</span><span class="n">n_fg_class</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">bccd_labels</span><span class="p">),</span> <span class="n">pretrained_model</span><span class="o">=</span><span class="s1">&#39;imagenet&#39;</span><span class="p">)</span>
<span class="n">train_chain</span> <span class="o">=</span> <span class="n">MultiboxTrainChain</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Downloading ...
From: https://chainercv-models.preferred.jp/ssd_vgg16_imagenet_converted_2017_06_09.npz
To: /root/.chainer/dataset/_dl_cache/b4130ae0aa259c095b50ff95d81c32ee
  %   Total    Recv       Speed  Time left
100   76MiB   76MiB   3745KiB/s    0:00:00
</pre></div></div>
</div>
<p>Executing the above cell will automatically download the weight (pre-trained model) when training the network called VGG 16 with the ImageNet - 1 K data set (large scale dataset of image classification).</p>
<p>In general, large-scale data sets are required for learning of deep learning models, but in some cases it is practically difficult to gather large amounts of data according to individual tasks. In such a case, a learning method called Fine-tuning, in which a model is pre-trained with a large-scale image classification data set that has been published (Pre-trained model) and this is re-learned with a small data set at hand It is useful. Using a large-scale image classification data set, it is
expected that the pre-trained model already has the ability to extract most of the various image features in the real world, so the same task or data set , There is a possibility of obtaining high accuracy even with less learning.</p>
<p>ChainerCV provides several pre-trained models in such a way that they can start using it very easily. Various pre-trained models are listed here: <a class="reference external" href="https://chainercv.readthedocs.io/en/latest/license.html#pretrained-models">Pretrained Models</a></p>
</div>
<div class="section" id="6.6.-Data-augmentation-of-implementation">
<h2>6.6. 6.6. Data augmentation of implementation<a class="headerlink" href="#6.6.-Data-augmentation-of-implementation" title="Permalink to this headline">¶</a></h2>
<p>In deep learning, whether the large amount of data can be prepared greatly affects the generalization performance of the model. A <strong>technique (data augmentation) that applies various transformations to images and accompanying labels without changing the meaning of the data so as to increase data in a pseudo manner</strong> is a method to inflate learning data.</p>
<p>In the following, you define a class describing the conversion process you want to apply to each data point in the training data set. The conversion to be done <code class="docutils literal notranslate"><span class="pre">__call__</span></code> is five described in the method. For example, you can change the color, flip it horizontally, enlarge, or shrink as long as the meaning of the image does not change much. Please note that correct answer labels need to be converted properly in those cases. For example, if you invert it in the horizontal direction, the correct
solution is the one with the correct answer label inverted in the horizontal direction. Also, it is an effective technique to mask and hide part of the image. This makes it possible to recognize based on various information without relying on only one information in recognition.</p>
<p>Let’s execute the following cells.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">copy</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">chainercv</span> <span class="kn">import</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">chainercv.links.model.ssd</span> <span class="kn">import</span> <span class="n">random_crop_with_bbox_constraints</span>
<span class="kn">from</span> <span class="nn">chainercv.links.model.ssd</span> <span class="kn">import</span> <span class="n">random_distort</span>
<span class="kn">from</span> <span class="nn">chainercv.links.model.ssd</span> <span class="kn">import</span> <span class="n">resize_with_random_interpolation</span>


<span class="k">class</span> <span class="nc">Transform</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">coder</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">mean</span><span class="p">):</span>
        <span class="c1"># to send cpu, make a copy</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coder</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">coder</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coder</span><span class="o">.</span><span class="n">to_cpu</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">mean</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_data</span><span class="p">):</span>
        <span class="c1"># There are five data augmentation steps</span>
        <span class="c1"># 1. Color augmentation</span>
        <span class="c1"># 2. Random expansion</span>
        <span class="c1"># 3. Random cropping</span>
        <span class="c1"># 4. Resizing with random interpolation</span>
        <span class="c1"># 5. Random horizontal flipping</span>

        <span class="n">img</span><span class="p">,</span> <span class="n">bbox</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">in_data</span>

        <span class="c1"># 1. Color augmentation</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">random_distort</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>

        <span class="c1"># 2. Random expansion</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
            <span class="n">img</span><span class="p">,</span> <span class="n">param</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">random_expand</span><span class="p">(</span>
                <span class="n">img</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span> <span class="n">return_param</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            <span class="n">bbox</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">translate_bbox</span><span class="p">(</span>
                <span class="n">bbox</span><span class="p">,</span> <span class="n">y_offset</span><span class="o">=</span><span class="n">param</span><span class="p">[</span><span class="s1">&#39;y_offset&#39;</span><span class="p">],</span> <span class="n">x_offset</span><span class="o">=</span><span class="n">param</span><span class="p">[</span><span class="s1">&#39;x_offset&#39;</span><span class="p">])</span>

        <span class="c1"># 3. Random cropping</span>
        <span class="n">img</span><span class="p">,</span> <span class="n">param</span> <span class="o">=</span> <span class="n">random_crop_with_bbox_constraints</span><span class="p">(</span>
            <span class="n">img</span><span class="p">,</span> <span class="n">bbox</span><span class="p">,</span> <span class="n">return_param</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">bbox</span><span class="p">,</span> <span class="n">param</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">crop_bbox</span><span class="p">(</span>
            <span class="n">bbox</span><span class="p">,</span> <span class="n">y_slice</span><span class="o">=</span><span class="n">param</span><span class="p">[</span><span class="s1">&#39;y_slice&#39;</span><span class="p">],</span> <span class="n">x_slice</span><span class="o">=</span><span class="n">param</span><span class="p">[</span><span class="s1">&#39;x_slice&#39;</span><span class="p">],</span>
            <span class="n">allow_outside_center</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">return_param</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">label</span> <span class="o">=</span> <span class="n">label</span><span class="p">[</span><span class="n">param</span><span class="p">[</span><span class="s1">&#39;index&#39;</span><span class="p">]]</span>

        <span class="c1"># 4. Resizing with random interpolatation</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">resize_with_random_interpolation</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">))</span>
        <span class="n">bbox</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">resize_bbox</span><span class="p">(</span><span class="n">bbox</span><span class="p">,</span> <span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">),</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">))</span>

        <span class="c1"># 5. Random horizontal flipping</span>
        <span class="n">img</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">random_flip</span><span class="p">(</span>
            <span class="n">img</span><span class="p">,</span> <span class="n">x_random</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">return_param</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">bbox</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">flip_bbox</span><span class="p">(</span>
            <span class="n">bbox</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">),</span> <span class="n">x_flip</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;x_flip&#39;</span><span class="p">])</span>

        <span class="c1"># Preparation for SSD network</span>
        <span class="n">img</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span>
        <span class="n">mb_loc</span><span class="p">,</span> <span class="n">mb_label</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coder</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">bbox</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">img</span><span class="p">,</span> <span class="n">mb_loc</span><span class="p">,</span> <span class="n">mb_label</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="6.7.-The-start-of-the-learning">
<h2>6.7. 6.7. The start of the learning<a class="headerlink" href="#6.7.-The-start-of-the-learning" title="Permalink to this headline">¶</a></h2>
<p>In the following, <code class="docutils literal notranslate"><span class="pre">TransformDataset</span></code> we will use the dataset class provided by Chainer <code class="docutils literal notranslate"><span class="pre">Transform</span></code> to apply the transformation we just defined for each data.</p>
<p>Since the basic flow is common to many of the network training methods which do image classification and segmentation already learned, detailed explanation is omitted here.</p>
<p>First, import the necessary modules. I will adopt it in the neural network which learns SSD 300 provided by ChainerCV and will use its implementation.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">chainer.datasets</span> <span class="kn">import</span> <span class="n">TransformDataset</span>
<span class="kn">from</span> <span class="nn">chainer.optimizer_hooks</span> <span class="kn">import</span> <span class="n">WeightDecay</span>
<span class="kn">from</span> <span class="nn">chainer</span> <span class="kn">import</span> <span class="n">serializers</span>
<span class="kn">from</span> <span class="nn">chainer</span> <span class="kn">import</span> <span class="n">training</span>
<span class="kn">from</span> <span class="nn">chainer.training</span> <span class="kn">import</span> <span class="n">extensions</span>
<span class="kn">from</span> <span class="nn">chainer.training</span> <span class="kn">import</span> <span class="n">triggers</span>
<span class="kn">from</span> <span class="nn">chainercv.extensions</span> <span class="kn">import</span> <span class="n">DetectionVOCEvaluator</span>
<span class="kn">from</span> <span class="nn">chainercv.links.model.ssd</span> <span class="kn">import</span> <span class="n">GradientScaling</span>

<span class="n">chainer</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_max_workspace_size</span><span class="p">(</span><span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">)</span>
<span class="n">chainer</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">autotune</span> <span class="o">=</span> <span class="bp">True</span>
</pre></div>
</div>
</div>
<p>Next, assign the following setting items to variables here for easy change later.</p>
<ul class="simple">
<li>Batch size</li>
<li>ID of the GPU to be used</li>
<li>Result output directory name</li>
<li>Initial value of learning rate</li>
<li>Number of epochs to be learned</li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="n">batchsize</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">gpu_id</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">out</span> <span class="o">=</span> <span class="s1">&#39;results&#39;</span>
<span class="n">initial_lr</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">training_epoch</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">log_interval</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="s1">&#39;epoch&#39;</span>
<span class="n">lr_decay_rate</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">lr_decay_timing</span> <span class="o">=</span> <span class="p">[</span><span class="mi">200</span><span class="p">,</span> <span class="mi">250</span><span class="p">]</span>
</pre></div>
</div>
</div>
<p>Next, we create dataset classes and iterators. This is the same as in the case of image classification already learned. The data points extracted from the data set are <code class="docutils literal notranslate"><span class="pre">Transform</span></code> converted by the conversion process defined in each predefined class.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="n">transformed_train_dataset</span> <span class="o">=</span> <span class="n">TransformDataset</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">Transform</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">coder</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">insize</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">mean</span><span class="p">))</span>

<span class="n">train_iter</span> <span class="o">=</span> <span class="n">chainer</span><span class="o">.</span><span class="n">iterators</span><span class="o">.</span><span class="n">MultiprocessIterator</span><span class="p">(</span><span class="n">transformed_train_dataset</span><span class="p">,</span> <span class="n">batchsize</span><span class="p">)</span>
<span class="n">valid_iter</span> <span class="o">=</span> <span class="n">chainer</span><span class="o">.</span><span class="n">iterators</span><span class="o">.</span><span class="n">SerialIterator</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">,</span> <span class="n">batchsize</span><span class="p">,</span> <span class="n">repeat</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Next I will create Optimizer. In this time we will optimize the parameters of the model using the technique called Momentum SGD. In doing so, <code class="docutils literal notranslate"><span class="pre">update_rule</span></code> we set hooks so that the slope is twice as large as the bias parameter of the linear transformation in the model . Also, in the case of bias parameters, weight decay is not performed, and weight decay is set for parameters other than bias parameters. These are techniques often used to stabilize learning and so on.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">chainer</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">MomentumSGD</span><span class="p">()</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">train_chain</span><span class="p">)</span>
<span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">train_chain</span><span class="o">.</span><span class="n">params</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;b&#39;</span><span class="p">:</span>
        <span class="n">param</span><span class="o">.</span><span class="n">update_rule</span><span class="o">.</span><span class="n">add_hook</span><span class="p">(</span><span class="n">GradientScaling</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">param</span><span class="o">.</span><span class="n">update_rule</span><span class="o">.</span><span class="n">add_hook</span><span class="p">(</span><span class="n">WeightDecay</span><span class="p">(</span><span class="mf">0.0005</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>Next we will create an updater object. This time <code class="docutils literal notranslate"><span class="pre">StandardUpdater</span></code> I used the simplest to the updater . We use this updater when learning using CPU or single GPU.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="n">updater</span> <span class="o">=</span> <span class="n">training</span><span class="o">.</span><span class="n">updaters</span><span class="o">.</span><span class="n">StandardUpdater</span><span class="p">(</span>
    <span class="n">train_iter</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">gpu_id</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Finally, we create a Trainer object.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="n">trainer</span> <span class="o">=</span> <span class="n">training</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span>
    <span class="n">updater</span><span class="p">,</span>
    <span class="p">(</span><span class="n">training_epoch</span><span class="p">,</span> <span class="s1">&#39;epoch&#39;</span><span class="p">),</span> <span class="n">out</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>There are no new ones such as addition of Trainer Extension from the one described in the previous chapter, but the <code class="docutils literal notranslate"><span class="pre">ManualScheduleTrigger</span></code> method of designating the new attenuation timing is used for attenuation of learning rate using ExponentialShift below . Simply, if you pass a list of numbers that show the timing when you want to start that Extention, and that unit (here ), it will invoke that extension only at the specified timing. In the following code, since above is assigned to above,
at the time of 200 epochs and 250 epochs, ExponentialShift is invoked, doubling the learning rate , that is, as set above,<code class="docutils literal notranslate"><span class="pre">[200,</span> <span class="pre">250]</span></code> <code class="docutils literal notranslate"><span class="pre">epochlr_decay_timing</span></code> <code class="docutils literal notranslate"><span class="pre">[200,</span> <span class="pre">250]</span></code> <code class="docutils literal notranslate"><span class="pre">lr_decay_rate0</span></code>. 1It is supposed to be doubled.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span>
    <span class="n">extensions</span><span class="o">.</span><span class="n">ExponentialShift</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">lr_decay_rate</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="n">initial_lr</span><span class="p">),</span>
    <span class="n">trigger</span><span class="o">=</span><span class="n">triggers</span><span class="o">.</span><span class="n">ManualScheduleTrigger</span><span class="p">(</span><span class="n">lr_decay_timing</span><span class="p">,</span> <span class="s1">&#39;epoch&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="6.7.1.-Evaluation">
<h3>6.7.1. 6.7.1. Evaluation<a class="headerlink" href="#6.7.1.-Evaluation" title="Permalink to this headline">¶</a></h3>
<p>For object detection, the <strong>case where the bbox (bbox to which confidence exceeding a certain value was judged) that the model judged to be “detected” is actually true bbox and IoU&gt; 0.5 or more is regarded as True Positive</strong>, and the <strong>average conformity rate Average precision (AP)</strong> is generally used for evaluation. In addition, Mean average precision (mAP) which calculates this for each class and takes an average as a whole is also used. IoU is described in the explanation about the semantic
segmentation in the previous chapter, but the IoU in object detection is the same as well, and the size of the region enclosed by either or both of the predicted rectangle and the correct rectangle is common The size of the enclosing area is divided by the size.</p>
<p>The extension provided by ChainerCV <code class="docutils literal notranslate"><span class="pre">DetectionVOCEvaluator</span></code> calculates the AP and the whole mAP for each class while learning, using the passed iterator (an iterator val_iter created for the validation dataset here). Again we will use this Extension.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span>
    <span class="n">DetectionVOCEvaluator</span><span class="p">(</span>
        <span class="n">valid_iter</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">use_07_metric</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="n">label_names</span><span class="o">=</span><span class="n">bccd_labels</span><span class="p">),</span>
    <span class="n">trigger</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;epoch&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>Let’s add another commonly used extension. For this time, I will save the results of learning every 10 epochs.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">extensions</span><span class="o">.</span><span class="n">LogReport</span><span class="p">(</span><span class="n">trigger</span><span class="o">=</span><span class="n">log_interval</span><span class="p">))</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">extensions</span><span class="o">.</span><span class="n">observe_lr</span><span class="p">(),</span> <span class="n">trigger</span><span class="o">=</span><span class="n">log_interval</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">extensions</span><span class="o">.</span><span class="n">PrintReport</span><span class="p">(</span>
    <span class="p">[</span><span class="s1">&#39;epoch&#39;</span><span class="p">,</span> <span class="s1">&#39;iteration&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">,</span>
     <span class="s1">&#39;main/loss&#39;</span><span class="p">,</span> <span class="s1">&#39;main/loss/loc&#39;</span><span class="p">,</span> <span class="s1">&#39;main/loss/conf&#39;</span><span class="p">,</span>
     <span class="s1">&#39;validation/main/map&#39;</span><span class="p">,</span> <span class="s1">&#39;elapsed_time&#39;</span><span class="p">]),</span>
    <span class="n">trigger</span><span class="o">=</span><span class="n">log_interval</span><span class="p">)</span>
<span class="k">if</span> <span class="n">extensions</span><span class="o">.</span><span class="n">PlotReport</span><span class="o">.</span><span class="n">available</span><span class="p">():</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span>
        <span class="n">extensions</span><span class="o">.</span><span class="n">PlotReport</span><span class="p">(</span>
            <span class="p">[</span><span class="s1">&#39;main/loss&#39;</span><span class="p">,</span> <span class="s1">&#39;main/loss/loc&#39;</span><span class="p">,</span> <span class="s1">&#39;main/loss/conf&#39;</span><span class="p">],</span>
            <span class="s1">&#39;epoch&#39;</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s1">&#39;loss.png&#39;</span><span class="p">))</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span>
        <span class="n">extensions</span><span class="o">.</span><span class="n">PlotReport</span><span class="p">(</span>
            <span class="p">[</span><span class="s1">&#39;validation/main/map&#39;</span><span class="p">],</span>
            <span class="s1">&#39;epoch&#39;</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s1">&#39;accuracy.png&#39;</span><span class="p">))</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">extensions</span><span class="o">.</span><span class="n">snapshot</span><span class="p">(</span>
    <span class="n">filename</span><span class="o">=</span><span class="s1">&#39;snapshot_epoch_{.updater.epoch}.npz&#39;</span><span class="p">),</span> <span class="n">trigger</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="s1">&#39;epoch&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>Well, originally, here</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
<p>f you run it, learning will begin immediately, but it will take about 100 minutes. So I just executed this script beforehand and saved the result of learning up to 290 epochs so let’s read this and learn only the last 10 epochs. First, download snapshot which is halfway through learning up to 290 epoch points.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[24]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="o">!</span>wget https://github.com/japan-medical-ai/medical-ai-course-materials/releases/download/v0.1/detection_snapshot_epoch_290.npz
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
--2018-12-16 13:36:44--  https://github.com/japan-medical-ai/medical-ai-course-materials/releases/download/v0.1/detection_snapshot_epoch_290.npz
Resolving github.com (github.com)... 140.82.118.3, 140.82.118.4
Connecting to github.com (github.com)|140.82.118.3|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/153412006/8191fa00-e78e-11e8-8a9b-3b2647ec012b?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20181216%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20181216T133644Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=8db368451cd08ed3f63daaf1a71d6fc8e00d5e1d60c84eeee422ef7d79c57fe0&amp;X-Amz-SignedHeaders=host&amp;actor_id=0&amp;response-content-disposition=attachment%3B%20filename%3Ddetection_snapshot_epoch_290.npz&amp;response-content-type=application%2Foctet-stream [following]
--2018-12-16 13:36:44--  https://github-production-release-asset-2e65be.s3.amazonaws.com/153412006/8191fa00-e78e-11e8-8a9b-3b2647ec012b?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20181216%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20181216T133644Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=8db368451cd08ed3f63daaf1a71d6fc8e00d5e1d60c84eeee422ef7d79c57fe0&amp;X-Amz-SignedHeaders=host&amp;actor_id=0&amp;response-content-disposition=attachment%3B%20filename%3Ddetection_snapshot_epoch_290.npz&amp;response-content-type=application%2Foctet-stream
Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.136.83
Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.136.83|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 179653491 (171M) [application/octet-stream]
Saving to: ‘detection_snapshot_epoch_290.npz’

detection_snapshot_ 100%[===================&gt;] 171.33M  23.7MB/s    in 11s

2018-12-16 13:36:55 (16.1 MB/s) - ‘detection_snapshot_epoch_290.npz’ saved [179653491/179653491]

</pre></div></div>
</div>
<p>Next, <code class="docutils literal notranslate"><span class="pre">detection_snapshot_epoch_250.npz</span></code> let’s load this downloaded file into the Trainer object created earlier.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="n">chainer</span><span class="o">.</span><span class="n">serializers</span><span class="o">.</span><span class="n">load_npz</span><span class="p">(</span><span class="s1">&#39;detection_snapshot_epoch_290.npz&#39;</span><span class="p">,</span> <span class="n">trainer</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Let’s learn only the last 10 epochs. Please execute the following cell and wait for a while.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[26]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
epoch       iteration   lr          main/loss   main/loss/loc  main/loss/conf  validation/main/map  elapsed_time
10          65          0.001       6.75134     2.08291        4.66843         0.118168             230.543
20          129         0.001       4.12112     1.58375        2.53737         0.181493             435.038
30          193         0.001       3.59885     1.31919        2.27966         0.279919             635.634
40          257         0.001       3.1998      1.07375        2.12605         0.573733             835.256
50          321         0.001       2.94131     0.926096       2.01522         0.657611             1034.6
60          385         0.001       2.86323     0.887698       1.97553         0.670849             1233.12
70          449         0.001       2.73648     0.819021       1.91746         0.696257             1428.25
80          513         0.001       2.63796     0.765831       1.87212         0.692361             1625.98
90          577         0.001       2.55598     0.738259       1.81773         0.711002             1821.58
100         641         0.001       2.49245     0.701536       1.79092         0.713163             2019.14
110         705         0.001       2.46662     0.68411        1.78251         0.719259             2215.34
120         769         0.001       2.42422     0.668462       1.75576         0.716902             2410.75
130         833         0.001       2.38509     0.651328       1.73376         0.72674              2609.16
140         897         0.001       2.32725     0.62762        1.69963         0.734795             2809.84
150         961         0.001       2.28612     0.609401       1.67672         0.731203             3012.42
160         1025        0.001       2.26408     0.602341       1.66174         0.737827             3208.94
170         1090        0.001       2.26435     0.602011       1.66234         0.739109             3415.94
180         1154        0.001       2.20838     0.580387       1.62799         0.73633              3619
190         1218        0.001       2.1549      0.558059       1.59684         0.738508             3823.92
200         1282        0.001       2.1479      0.557085       1.59082         0.735312             4022.46
210         1346        0.0001      2.15193     0.566057       1.58587         0.743703             4218.82
220         1410        0.0001      2.06368     0.525004       1.53867         0.746575             4421.17
230         1474        0.0001      2.03127     0.510777       1.52049         0.748318             4629.21
240         1538        0.0001      2.03743     0.517596       1.51984         0.748923             4836.61
250         1602        0.0001      2.01771     0.50665        1.51106         0.74621              5044.15
260         1666        1e-05       1.9999      0.500324       1.49958         0.750594             5251.47
270         1730        1e-05       2.0164      0.502952       1.51345         0.749446             5459.1
280         1794        1e-05       2.0113      0.504592       1.50671         0.750496             5667.54
290         1858        1e-05       2.0113      0.507134       1.50417         0.750217             5871.16
300         1922        1e-05       2.0002      0.496281       1.50392         0.749795             6107.78
</pre></div></div>
</div>
<p>Learning is completed. From the next section we try <strong>to infer</strong> the <strong>unknown data</strong> using the new snapshot obtained as a result of this learning .</p>
</div>
</div>
<div class="section" id="6.8.-Inference-using-learning">
<h2>6.8. 6.8. Inference using learning<a class="headerlink" href="#6.8.-Inference-using-learning" title="Permalink to this headline">¶</a></h2>
<p>The parameters of the model obtained as a result of learning are <code class="docutils literal notranslate"><span class="pre">extensions.snapshot()</span></code> saved in the file by the Trainer extension called. By default, the save destination is <code class="docutils literal notranslate"><span class="pre">out</span></code> less than or equal to the directory specified by the argument that it was passed at the time of Trainer object initialization . This time <code class="docutils literal notranslate"><span class="pre">results</span></code> it should be below.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[27]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="o">!</span>ls -la results/
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
total 175520
drwxr-xr-x 2 root root      4096 Dec 16 13:41 .
drwxr-xr-x 1 root root      4096 Dec 16 13:36 ..
-rw-r--r-- 1 root root     16448 Dec 16 13:40 accuracy.png
-rw-r--r-- 1 root root     14213 Dec 16 13:40 log
-rw-r--r-- 1 root root     19216 Dec 16 13:40 loss.png
-rw-r--r-- 1 root root 179665430 Dec 16 13:41 snapshot_epoch_300.npz
</pre></div></div>
</div>
<p>As a result of executing the above shell command, <code class="docutils literal notranslate"><span class="pre">snapshot_epoch_300.npz</span></code> it should be found a file called. This is a collection of the parameters necessary for restarting the learning that was in Trainer during learning. For that reason, parameters other than the parameters that the model itself had inside, such as the parameters that the Optimizer has inside, are stored together. Therefore, this time we will use only the parameters of the model necessary for inference from this file and use
it.</p>
<p>As a way to retrieve the model parameters of, <code class="docutils literal notranslate"><span class="pre">chainer.serializers.load_npz</span></code> using <code class="docutils literal notranslate"><span class="pre">.npz</span></code> the time to load the file into the model object, <code class="docutils literal notranslate"><span class="pre">.npz</span></code> the file of the key to specify that see only what the following hierarchy that there is a way. When taking a snapshot of the entire Trainer object, other than the parameters inside the model, such as information on the iteration count of the Optimizer, are also stored, but <code class="docutils literal notranslate"><span class="pre">updater/model:main/model</span></code> if you pass the prefix, you can retrieve only
the parameter part of the model I can.</p>
<p>In the place different from the one used for learning, assuming a situation where only the snapshot and the code of the definition of the model are passed, a new model object is created, and the learned parameters are loaded there Let’s look.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Create a model object</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SSD300</span><span class="p">(</span><span class="n">n_fg_class</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">bccd_labels</span><span class="p">),</span> <span class="n">pretrained_model</span><span class="o">=</span><span class="s1">&#39;imagenet&#39;</span><span class="p">)</span>

<span class="c1"># Load parameters to the model</span>
<span class="n">chainer</span><span class="o">.</span><span class="n">serializers</span><span class="o">.</span><span class="n">load_npz</span><span class="p">(</span>
    <span class="s1">&#39;results/snapshot_epoch_300.npz&#39;</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">path</span><span class="o">=</span><span class="s1">&#39;updater/model:main/model/&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Let’s try to detect the cells with one of the test images using the model loaded with the learned weights. In the following code, we read images, execute inference, and visualize the results in order using ChainerCV.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[29]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">chainercv</span> <span class="kn">import</span> <span class="n">utils</span>

<span class="k">def</span> <span class="nf">inference</span><span class="p">(</span><span class="n">image_filename</span><span class="p">):</span>
    <span class="c1"># Load a test image</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">read_image</span><span class="p">(</span><span class="n">image_filename</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="c1"># Perform inference</span>
    <span class="n">bboxes</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">img</span><span class="p">])</span>

    <span class="c1"># Extract the results</span>
    <span class="n">bbox</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">score</span> <span class="o">=</span> <span class="n">bboxes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">labels</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">scores</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Visualize the detection results</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">vis_bbox</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">bbox</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">label_names</span><span class="o">=</span><span class="n">bccd_labels</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">figure</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="n">inference</span><span class="p">(</span><span class="s1">&#39;BCCD_Dataset/BCCD/JPEGImages/BloodImage_00007.jpg&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Blood_Cell_Detection_66_0.png" src="../_images/notebooks_Blood_Cell_Detection_66_0.png" />
</div>
</div>
<p>Let’s further infer some images and take a look at the results.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[30]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">image_filename</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s1">&#39;BCCD_Dataset/BCCD/ImageSets/Main/test.txt&#39;</span><span class="p">)):</span>
    <span class="k">print</span><span class="p">(</span><span class="n">image_filename</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
    <span class="n">inference</span><span class="p">(</span><span class="s1">&#39;BCCD_Dataset/BCCD/JPEGImages/&#39;</span> <span class="o">+</span> <span class="n">image_filename</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="o">+</span> <span class="s1">&#39;.jpg&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">5</span><span class="p">:</span>  <span class="c1"># 5+1個表示したら終わる</span>
        <span class="k">break</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
BloodImage_00007

</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>&lt;matplotlib.figure.Figure at 0x7fbfcaac8710&gt;
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Blood_Cell_Detection_68_2.png" src="../_images/notebooks_Blood_Cell_Detection_68_2.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
BloodImage_00011

</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>&lt;matplotlib.figure.Figure at 0x7fbfcee0d2e8&gt;
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Blood_Cell_Detection_68_5.png" src="../_images/notebooks_Blood_Cell_Detection_68_5.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
BloodImage_00015

</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>&lt;matplotlib.figure.Figure at 0x7fbfcefd5c50&gt;
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Blood_Cell_Detection_68_8.png" src="../_images/notebooks_Blood_Cell_Detection_68_8.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
BloodImage_00016

</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>&lt;matplotlib.figure.Figure at 0x7fbfceeca908&gt;
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Blood_Cell_Detection_68_11.png" src="../_images/notebooks_Blood_Cell_Detection_68_11.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
BloodImage_00018

</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>&lt;matplotlib.figure.Figure at 0x7fbfced83c50&gt;
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Blood_Cell_Detection_68_14.png" src="../_images/notebooks_Blood_Cell_Detection_68_14.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
BloodImage_00019

</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>&lt;matplotlib.figure.Figure at 0x7fbfced1ca58&gt;
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Blood_Cell_Detection_68_17.png" src="../_images/notebooks_Blood_Cell_Detection_68_17.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
BloodImage_00021

</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>&lt;matplotlib.figure.Figure at 0x7fbfceea6668&gt;
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Blood_Cell_Detection_68_20.png" src="../_images/notebooks_Blood_Cell_Detection_68_20.png" />
</div>
</div>
</div>
<div class="section" id="6.9.-Evaluating-learned">
<h2>6.9. 6.9. Evaluating learned<a class="headerlink" href="#6.9.-Evaluating-learned" title="Permalink to this headline">¶</a></h2>
<p>After learning, evaluate the obtained model with the test dataset. Validation data set (validation dataset) is not used directly to calculate parameter update amount during learning, but in order to <strong>adjust</strong> hyperparameter such as learning rate and ratio / timing of learning rate attenuation <strong>It is used</strong> for, <strong>not what you would call strictly speaking, not using at the time of learning data</strong>. Therefore, in order to obtain an indication of the extent of the generalization performance of the
finally obtained model, it is necessary to <strong>evaluate using the third data set not included in any of the learning / verification data set You need to do.</strong></p>
<p>Evaluator, one of Chainer ’s Trainer Extensions, can actually be used alone, not with Trainer. It is <code class="docutils literal notranslate"><span class="pre">DetectionVOCEvaluatoran</span></code> extension version Evaluator which ChainerCV provides but inherited Chainer’s Evaluator, so it can be used for evaluation only, irrespective of Trainer as well.</p>
<p>So, the beginning had been prepared in the direction of <code class="docutils literal notranslate"><span class="pre">test_dataset</span></code> making the First iterator using, it <code class="docutils literal notranslate"><span class="pre">DetectionVOCEvaluator</span></code> passes along with the learned model using even earlier, the final performance evaluation using the test data set to go the Let’s.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[31]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="n">test_batchsize</span> <span class="o">=</span> <span class="mi">256</span>

<span class="n">model</span><span class="o">.</span><span class="n">to_gpu</span><span class="p">()</span>

<span class="n">test_iter</span> <span class="o">=</span> <span class="n">chainer</span><span class="o">.</span><span class="n">iterators</span><span class="o">.</span><span class="n">SerialIterator</span><span class="p">(</span>
    <span class="n">test_dataset</span><span class="p">,</span> <span class="n">test_batchsize</span><span class="p">,</span> <span class="n">repeat</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">test_evaluator</span> <span class="o">=</span> <span class="n">DetectionVOCEvaluator</span><span class="p">(</span>
    <span class="n">test_iter</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">use_07_metric</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">label_names</span><span class="o">=</span><span class="n">bccd_labels</span><span class="p">)</span>

<span class="n">test_evaluator</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[31]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>{&#39;main/ap/platelets&#39;: 0.43220927662530395,
 &#39;main/ap/rbc&#39;: 0.760081977582848,
 &#39;main/ap/wbc&#39;: 0.9651693947468596,
 &#39;main/map&#39;: 0.7191535496516704}
</pre></div>
</div>
</div>
<p>Looking at the results displayed here, we found that prediction for white blood cells is most accurate, then red blood cells, while platelets predictions are much lower than those of the other two. In these cases, it is necessary to check whether platelets, erythrocytes, white blood cells appear in the data set at the same frequency. If the frequency varies from class to class, the model seems to be able to observe less frequent classes less frequently than frequent classes. It is not the best
way to handle learning by treating them completely (without distinction).</p>
<p>Even when training an object detector with actual application, first try learning using a famous model and make a result, then match the result with the data, and use the prediction trend of the model and the data set itself It is important to examine the features etc.</p>
<p>As for the problem of Class imbalance, the method called <a class="reference external" href="https://arxiv.org/abs/1708.02002">Focal loss</a> proposes a simple and powerful proposal. It may be helpful.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>
</pre></div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="DNA_Sequence_Data_Analysis.html" class="btn btn-neutral float-right" title="7. 7. practical part: sequence analysis using deep learning" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="Image_Segmentation.html" class="btn btn-neutral" title="5. 5. Practice: Segmentation of MRI" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Preferred Networks &amp; キカガク

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>