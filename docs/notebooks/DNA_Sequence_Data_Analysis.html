

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>7. Practical part: sequence analysis using deep learning &mdash; メディカルAI専門コース オンライン講義資料  documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="8. Practical part: Time series analysis of monitoring data using the deep learning" href="Sequential_Data_Analysis_with_Deep_Learning.html" />
    <link rel="prev" title="6. Practice section: Detection of cells from microscope images of blood" href="Blood_Cell_Detection.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-797798-11"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'UA-797798-11');
  </script>

  <meta name="description" content="メディカルAI学会公認資格向けオンライン講義資料。機械学習に必要な数学の基礎の解説から深層学習（ディープラーニング）を用いた実践的な内容までGoogle Colaboratory上でGPUを用いて実際にコードを実行可能な形式にしオンライン資料として無料公開。">
  <meta property="og:title" content="メディカルAI専門コース オンライン講義資料">
  <meta property="og:description" content="メディカルAI学会公認資格向けオンライン講義資料。機械学習に必要な数学の基礎の解説から深層学習（ディープラーニング）を用いた実践的な内容までGoogle Colaboratory上でGPUを用いて実際にコードを実行可能な形式にしオンライン資料として無料公開。">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://japan-medical-ai.github.io/medical-ai-course-materials/">
  <meta property="og:image" content="https://raw.githubusercontent.com/japan-medical-ai/medical-ai-course-materials/master/notebooks/images/medical_ai.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@PreferredNetJP">
  <meta name="twitter:creator" content="@PreferredNetJP">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> メディカルAI専門コース オンライン講義資料
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Basic_Math_for_ML.html">1. Basis of the mathematics required to machine learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Introduction_to_ML_libs.html">2. Basics of machine learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Introduction_to_Neural_Network.html">3. Basics of neural network</a></li>
<li class="toctree-l1"><a class="reference internal" href="Introduction_to_Chainer.html">4. Introduction to Deep Learning Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="Image_Segmentation.html">5. Practice: Segmentation of MRI</a></li>
<li class="toctree-l1"><a class="reference internal" href="Blood_Cell_Detection.html">6. Practice section: Detection of cells from microscope images of blood</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">7. Practical part: sequence analysis using deep learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Environment">7.1. Environment</a></li>
<li class="toctree-l2"><a class="reference internal" href="#For-the-sequence-analysis">7.2. For the sequence analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Data-Set">7.3. Data Set</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Dilated-Convolution-analysis-using">7.4. Dilated Convolution analysis using</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Strategy-of-sequence-analysis">7.4.1. Strategy of sequence analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Dilated-Convolution">7.4.2. Dilated Convolution</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Sequential_Data_Analysis_with_Deep_Learning.html">8. Practical part: Time series analysis of monitoring data using the deep learning</a></li>
</ul>

            
          
          <div style="padding-right:20px; bottom:10px;">
            <a href="https://short-term.kikagaku.co.jp/dnn-seminar/">
              <img src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/img_handson.png" />
              <p style="padding:5px; font-size:small; line-height: 150%">ディープラーニングの詳しい解説や画像・自然言語の取り扱い、クラウド上のGPUを使った実践的な演習をご希望の方はこちらがおすすめです</p>
            </a>
          </div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">メディカルAI専門コース オンライン講義資料</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>7. Practical part: sequence analysis using deep learning</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/notebooks/DNA_Sequence_Data_Analysis.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 5ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    background: #f5f5f5;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<p><a class="reference external" href="https://colab.research.google.com/github/preferred-medicine/medical-ai-course-materials/blob/master/notebooks/DNA_Sequence_Data_Analysis.ipynb"><img alt="colab-logo" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<div class="section" id="Practical-part:-sequence-analysis-using-deep-learning">
<h1>7. Practical part: sequence analysis using deep learning<a class="headerlink" href="#Practical-part:-sequence-analysis-using-deep-learning" title="Permalink to this headline">¶</a></h1>
<p>In recent years, with the development of the Next Generation Sequencer (NGS), nucleotide sequences of genes have been read at high speed, in large amounts, and at low cost.</p>
<p>Here we will address the issue of predicting epigenetic effects and transcriptional control from DNA sequences using deep learning. Deep learning can express a complicated model, it can also take into account the effects of long distances and it can be expected to predict with higher accuracy.</p>
<div class="section" id="Environment">
<h2>7.1. Environment<a class="headerlink" href="#Environment" title="Permalink to this headline">¶</a></h2>
<p>The library used here are:</p>
<ul class="simple">
<li><p>Chainer</p></li>
<li><p>Cupy</p></li>
<li><p>matplotlib</p></li>
</ul>
<p>On Google Colab, you can install as follows. Execute the following cell (Shit + Enter).</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>!curl https://colab.chainer.org/install | sh -
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  1580  100  1580    0     0   6531      0 --:--:-- --:--:-- --:--:--  6556
sh: line 9: nvidia-smi: command not found
********************************************************************************
GPU is not enabled!
Open &#34;Runtime&#34; &gt; &#34;Change runtime type&#34; and set &#34;Hardware accelerator&#34; to &#34;GPU&#34;.
********************************************************************************
</pre></div></div>
</div>
<p>When installation is completed, run the following cell and check the version of each library.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>import chainer
import cupy
import matplotlib

chainer.print_runtime_info()
print(&#39;matplotlib:&#39;, matplotlib.__version__)
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/Users/ytakeda/venvs/py37/lib/python3.7/site-packages/chainer/_environment_check.py:37: UserWarning: Accelerate has been detected as a NumPy backend library.
vecLib, which is a part of Accelerate, is known not to work correctly with Chainer.
We recommend using other BLAS libraries such as OpenBLAS.
For details of the issue, please see
https://docs.chainer.org/en/stable/tips.html#mnist-example-does-not-converge-in-cpu-mode-on-mac-os-x.

Please be aware that Mac OS X is not an officially supported OS.

  &#39;&#39;&#39;)  # NOQA
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ModuleNotFoundError</span>                       Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-2-e4d91d41dbf0&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-intense-fg ansi-bold">      1</span> <span class="ansi-green-fg">import</span> chainer
<span class="ansi-green-fg">----&gt; 2</span><span class="ansi-red-fg"> </span><span class="ansi-green-fg">import</span> cupy
<span class="ansi-green-intense-fg ansi-bold">      3</span> <span class="ansi-green-fg">import</span> matplotlib
<span class="ansi-green-intense-fg ansi-bold">      4</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span> chainer<span class="ansi-blue-fg">.</span>print_runtime_info<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>

<span class="ansi-red-fg">ModuleNotFoundError</span>: No module named &#39;cupy&#39;
</pre></div></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Platform</span><span class="p">:</span> <span class="n">Linux</span><span class="o">-</span><span class="mf">4.14</span><span class="o">.</span><span class="mi">65</span><span class="o">+-</span><span class="n">x86_64</span><span class="o">-</span><span class="k">with</span><span class="o">-</span><span class="n">Ubuntu</span><span class="o">-</span><span class="mf">18.04</span><span class="o">-</span><span class="n">bionic</span>
<span class="n">Chainer</span><span class="p">:</span> <span class="mf">5.1</span><span class="o">.</span><span class="mi">0</span>
<span class="n">NumPy</span><span class="p">:</span> <span class="mf">1.14</span><span class="o">.</span><span class="mi">6</span>
<span class="n">CuPy</span><span class="p">:</span>
  <span class="n">CuPy</span> <span class="n">Version</span>          <span class="p">:</span> <span class="mf">5.1</span><span class="o">.</span><span class="mi">0</span>
  <span class="n">CUDA</span> <span class="n">Root</span>             <span class="p">:</span> <span class="o">/</span><span class="n">usr</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">cuda</span>
  <span class="n">CUDA</span> <span class="n">Build</span> <span class="n">Version</span>    <span class="p">:</span> <span class="mi">9020</span>
  <span class="n">CUDA</span> <span class="n">Driver</span> <span class="n">Version</span>   <span class="p">:</span> <span class="mi">9020</span>
  <span class="n">CUDA</span> <span class="n">Runtime</span> <span class="n">Version</span>  <span class="p">:</span> <span class="mi">9020</span>
  <span class="n">cuDNN</span> <span class="n">Build</span> <span class="n">Version</span>   <span class="p">:</span> <span class="mi">7301</span>
  <span class="n">cuDNN</span> <span class="n">Version</span>         <span class="p">:</span> <span class="mi">7301</span>
  <span class="n">NCCL</span> <span class="n">Build</span> <span class="n">Version</span>    <span class="p">:</span> <span class="mi">2307</span>
<span class="n">iDeep</span><span class="p">:</span> <span class="mf">2.0</span><span class="o">.</span><span class="mf">0.</span><span class="n">post3</span>
<span class="p">(</span><span class="s1">&#39;matplotlib:&#39;</span><span class="p">,</span> <span class="s1">&#39;2.1.2&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="For-the-sequence-analysis">
<h2>7.2. For the sequence analysis<a class="headerlink" href="#For-the-sequence-analysis" title="Permalink to this headline">¶</a></h2>
<p>Along with the development and popularization of next-generation sequencers, a large number of gene sequences have been read. Under such circumstances, GWAS (Genome Wide Association Study) that estimates the relationship between the genotype expressed in the base sequence and the phenotype such as disease and morphology has been performed, It has been learned that mutation alone can not explain all phenotypic changes. In particular, various experimental results have shown that untranslated
regions influence gene expression and cause phenotypic changes. Various methods have been proposed to investigate how the surrounding area affects gene expression.</p>
<p><img alt="Epigenome analysis schematic（cited in Encode Project）" src="https://www.encodeproject.org/images/c45f4d8c-0340-4fcb-abe3-e4ff0bb919be/download/attachment/EncodeDatatypes2013-7.png" /></p>
<p>For example, ChIP-Seq (chromatin immunoprecipitation sequence) is a method to identify histone modification status and the binding site of transcriptional regulatory elements exhaustively (genome-wide) by combining ChIP (chromatin immunoprecipitation) and high-speed DNA sequence. As a result, it is possible to identify the binding site of histone modification and DNA binding protein which controls the transcriptional regulatory function throughout the genome, it becomes possible to obtain vast
amounts of information related to cellular phenotype which can not be fully explained by gene mutation alone.</p>
<p>Therefore, in this section, by learning patterns of DNA base sequences corresponding to binding sites of transcriptional regulatory factors obtained by ChIP-Seq by deep learning, we will predict the possibility of binding to an arbitrary DNA base sequence with a specific transcriptional regulatory factor. This method can deal with a wide range of biological phenomena such as prediction of the histone modification site of the whole genome and prediction of the open chromatin region and gives
useful insights.</p>
<p>One of the technical difficulties in dealing with this task with machine learning is a phenomenon called long-distance interaction of DNA base sequence. This is because DNA in the nucleus exists in a complicated folded manner, two distant regions as a sequence on the base sequence are spatially close to each other, and it may affect the binding of the transcriptional regulatory factor. Therefore, we will construct an efficient model that enables training with this long distance interaction taken
into account, and receive a DNA base sequence having a length exceeding 100,000 bp (base pair: a unit that counts bases constituting DNA) as an input, then we will try to predict if a certain region in the DNA base sequence can be the binding site for the transcriptional regulatory factor.</p>
<p>In this case, we will take DNA base sequence, obtained from dataset of thousands of ChIP-Seq, DNase-seq (one method of comprehensive analysis of open chromatin region) obtained from hundreds of human cell types, as input, then examine the problem of estimating the expression level of mRNA measured from the result of CAGE (Cap Analysis of Gene Expression)[1].</p>
</div>
<div class="section" id="Data-Set">
<h2>7.3. Data Set<a class="headerlink" href="#Data-Set" title="Permalink to this headline">¶</a></h2>
<p>Here we use some of the experimental data set used in Basenji [1], which is a data set obtained by performing sequence analysis such as CAGE.</p>
<p>Please run the cell below and download the data.</p>
<p>Each of these arrays consists of 131072 bp in length, and its coverage value is recorded every 128 bp. The array length of this coverage value is 131072/128 = 1024.</p>
<p>The goal of this problem is to estimate the coverage value every 128 bp when receiving an array of length 131072 bp as input.</p>
<p>This time we will deal with the problem of simultaneously predicting coverage values of 10 different experiments.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>!wget https://github.com/japan-medical-ai/medical-ai-course-materials/releases/download/v0.1/seq.h5
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
--2019-05-06 10:49:35--  https://github.com/japan-medical-ai/medical-ai-course-materials/releases/download/v0.1/seq.h5
Resolving github.com (github.com)... 192.30.255.113
Connecting to github.com (github.com)|192.30.255.113|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/153412006/c79a0800-f713-11e8-8d6c-255563d45b1b?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20190506%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20190506T174936Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=7b95541ce0c07323709274af077569f3e5cbd0883349be7025703fa5f0de3c21&amp;X-Amz-SignedHeaders=host&amp;actor_id=0&amp;response-content-disposition=attachment%3B%20filename%3Dseq.h5&amp;response-content-type=application%2Foctet-stream [following]
--2019-05-06 10:49:35--  https://github-production-release-asset-2e65be.s3.amazonaws.com/153412006/c79a0800-f713-11e8-8d6c-255563d45b1b?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20190506%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20190506T174936Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=7b95541ce0c07323709274af077569f3e5cbd0883349be7025703fa5f0de3c21&amp;X-Amz-SignedHeaders=host&amp;actor_id=0&amp;response-content-disposition=attachment%3B%20filename%3Dseq.h5&amp;response-content-type=application%2Foctet-stream
Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.233.35
Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.233.35|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 594118876 (567M) [application/octet-stream]
Saving to: ‘seq.h5.1’

seq.h5.1            100%[===================&gt;] 566.60M  21.1MB/s    in 44s

2019-05-06 10:50:19 (13.0 MB/s) - ‘seq.h5.1’ saved [594118876/594118876]

</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>!ls -lh
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
total 2363288
-rw-r--r--   1 ytakeda  staff    62K Apr  1 18:10 Basic_Math_for_ML.ipynb
-rw-r--r--   1 ytakeda  staff   442K Apr 24 19:03 Blood_Cell_Detection.ipynb
-rw-r--r--   1 ytakeda  staff    80K May  1 16:10 DNA_Sequence_Data_Analysis.ipynb
-rw-r--r--   1 ytakeda  staff   221K Apr 19 10:47 Image_Segmentation.ipynb
-rw-r--r--   1 ytakeda  staff   503K Apr 30 14:41 Introduction_to_Chainer.ipynb
-rw-r--r--   1 ytakeda  staff    73K Apr  1 18:10 Introduction_to_ML_libs.ipynb
-rw-r--r--   1 ytakeda  staff    57K Apr  8 12:46 Introduction_to_Neural_Network.ipynb
-rw-r--r--   1 ytakeda  staff   246K Apr 24 19:03 Sequential_Data_Analysis_with_Deep_Learning.ipynb
drwxr-xr-x  15 ytakeda  staff   480B Apr 19 11:07 <span class="ansi-cyan-intense-fg ansi-bold">images</span>
-rw-r--r--   1 ytakeda  staff   567M Dec  2 22:54 seq.h5
-rw-r--r--   1 ytakeda  staff   567M Dec  2 22:54 seq.h5.1
</pre></div></div>
</div>
<p>Please check that the file seq.h5 is downloaded correctly. The size is 567 MB.</p>
<p>seq.h 5 is a file that stores data in HDF5 format. Like the file system, the HDF5 file can store data hierarchically, and can store matrix and tensor data named at each position.</p>
<p>There is a library called h5py to manipulate HDF5 format files. Open the file with h5py’s File() function and enumerate the keys contained in it with the keys() function. By specifying the acquired key within ‘[]’, you can refer to each stored data linked to that key.</p>
<p>Like the numpy, you can get size of tensor data with the attribute called shape.</p>
<p>Run the following cell and check the stored data.</p>
<p>A prefix of train (training), validate (validation), test (test) is attached to the name of each data, in corresponds to the base sequence of the input, and out corresponds to the coverage value of the output.</p>
<p>For example, ‘train_in’ is input data for training and has a size of (5000, 131072, 4). This is an array with 5000 arrays of length 130172, where the corresponding dimension values of A, T, C and G are 1 and the others are 0.</p>
<p>‘Train_out’ is output data for training and has a size of ‘(5000, 1024, 10’). There are 5000 arrays of length 1024, and each contains the coverage value of 10 different ChIP-Seq results.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>import h5py
import numpy as np

with h5py.File(&#39;seq.h5&#39;, &#39;r&#39;) as hf:
    for key in hf.keys():
        print(key, hf[key].shape, hf[key].dtype)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ModuleNotFoundError</span>                       Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-6-3c26f0ebd2c1&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> </span><span class="ansi-green-fg">import</span> h5py
<span class="ansi-green-intense-fg ansi-bold">      2</span> <span class="ansi-green-fg">import</span> numpy <span class="ansi-green-fg">as</span> np
<span class="ansi-green-intense-fg ansi-bold">      3</span>
<span class="ansi-green-intense-fg ansi-bold">      4</span> <span class="ansi-green-fg">with</span> h5py<span class="ansi-blue-fg">.</span>File<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">&#39;seq.h5&#39;</span><span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">&#39;r&#39;</span><span class="ansi-blue-fg">)</span> <span class="ansi-green-fg">as</span> hf<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span>     <span class="ansi-green-fg">for</span> key <span class="ansi-green-fg">in</span> hf<span class="ansi-blue-fg">.</span>keys<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>

<span class="ansi-red-fg">ModuleNotFoundError</span>: No module named &#39;h5py&#39;
</pre></div></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="sa">u</span><span class="s1">&#39;target_labels&#39;</span><span class="p">,</span> <span class="p">(</span><span class="mi">10</span><span class="p">,),</span> <span class="n">dtype</span><span class="p">(</span><span class="s1">&#39;S29&#39;</span><span class="p">))</span>
<span class="p">(</span><span class="sa">u</span><span class="s1">&#39;test_in&#39;</span><span class="p">,</span> <span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">131072</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="p">(</span><span class="s1">&#39;bool&#39;</span><span class="p">))</span>
<span class="p">(</span><span class="sa">u</span><span class="s1">&#39;test_out&#39;</span><span class="p">,</span> <span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="p">(</span><span class="s1">&#39;&lt;f2&#39;</span><span class="p">))</span>
<span class="p">(</span><span class="sa">u</span><span class="s1">&#39;train_in&#39;</span><span class="p">,</span> <span class="p">(</span><span class="mi">5000</span><span class="p">,</span> <span class="mi">131072</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="p">(</span><span class="s1">&#39;bool&#39;</span><span class="p">))</span>
<span class="p">(</span><span class="sa">u</span><span class="s1">&#39;train_out&#39;</span><span class="p">,</span> <span class="p">(</span><span class="mi">5000</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="p">(</span><span class="s1">&#39;&lt;f2&#39;</span><span class="p">))</span>
<span class="p">(</span><span class="sa">u</span><span class="s1">&#39;valid_in&#39;</span><span class="p">,</span> <span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">131072</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="p">(</span><span class="s1">&#39;bool&#39;</span><span class="p">))</span>
<span class="p">(</span><span class="sa">u</span><span class="s1">&#39;valid_out&#39;</span><span class="p">,</span> <span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="p">(</span><span class="s1">&#39;&lt;f2&#39;</span><span class="p">))</span>
</pre></div>
</div>
<p>To handle h5py format files as numpy data, you need to copy them. The following code reads the tensor data corresponding to the key ‘train_in’ as numpy data and displays part of that data.</p>
<p>Let’s try to extract the first data and display the output value of it.</p>
<p>Please run the cell below. It outputs three values of the first data output as a line graph. (Please run the cells up to here).</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>%matplotlib inline
import matplotlib.pyplot as plt

with h5py.File(&#39;seq.h5&#39;) as hf:
    y = hf[&#39;train_out&#39;][:100]
    fig_size = plt.rcParams[&quot;figure.figsize&quot;]
    fig_size[0] = 20
    fig_size[1] = 5
    for i in range(3):
        plt.bar(range(y.shape[1]), y[0,:,i])
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_DNA_Sequence_Data_Analysis_14_0.png" src="../_images/notebooks_DNA_Sequence_Data_Analysis_14_0.png" />
</div>
</div>
</div>
<div class="section" id="Dilated-Convolution-analysis-using">
<h2>7.4. Dilated Convolution analysis using<a class="headerlink" href="#Dilated-Convolution-analysis-using" title="Permalink to this headline">¶</a></h2>
<div class="section" id="Strategy-of-sequence-analysis">
<h3>7.4.1. Strategy of sequence analysis<a class="headerlink" href="#Strategy-of-sequence-analysis" title="Permalink to this headline">¶</a></h3>
<p>Let’s handle the problem that array data is input.</p>
<p>There are three major strategies to handle array data.</p>
<p>The first is to discard the sequence information in the array and to consider the array as a set of its features. This is called Bag of Words (BoW) expression. This BoW expression is a powerful technique if enough information is included in the feature, but it is difficult to capture its features only with an array of four kinds of characters such as a DNA sequence or a partial sequence thereof.</p>
<p>The second method is to read elements in the array from left to right in order and calculate. This is analyzed using RNN which I mentioned a little in Chapter 4 as well. RNN reads input one by one at each time and updates the internal state. The problem with RNN is that its calculation is sequential and the amount of calculation is proportional to the length of the array. The current computer achieves high speed by parallelizing the calculation, but it is difficult for RNN to parallelize the
calculation. Another problem is that it is difficult to capture relationships between long distances. Because of the calculation method, RNN needs to store all intermediate results in a fixed-length internal state vector. When trying to capture relationships between long distances, we need to remember a lot of information, but since state vector size is finite, it becomes difficult to capture relationships between long distances.</p>
<p>The third method is to analyze array data as one dimensional image and analyze it using CNN as in the case of image processing. Unlike the case of RNN, CNN can process each position independently, so it can process in parallel.</p>
<p>In this time we will adopt this third strategy, a method to analyze using CNN. In addition, by using Dilated Convolution, processing at each position can directly read information at a long distance. In the next chapter we will look at Dilated Convolution in detail.</p>
</div>
<div class="section" id="Dilated-Convolution">
<h3>7.4.2. Dilated Convolution<a class="headerlink" href="#Dilated-Convolution" title="Permalink to this headline">¶</a></h3>
<p>Consider the case of sequence analysis using the conventional convolutional layer. As shown in the figure below, the input information of a certain position is read only from the adjacent position in each layer. The kernel size determines how much information is acquired from the distance, and if the kernel size is K, the D / K layer is required to acquire the information that is at a distance D from the kernel size. In the case of this problem D is hundreds to tens of thousands and K is the
value of 3 or 5 so it is not realistic that the number of necessary layers will also be from one hundred to ten thousand.</p>
<p><img alt="Conventional convolutional layer calculation" src="http://musyoku.github.io/images/post/2016-09-17/naive_conv.png" /></p>
<p>Cited from <a class="reference external" href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/">WaveNet: A Generative Model for Raw Audio</a></p>
<p>On the other hand, Dilated Convolution (also called atrous convolution and convolution weith holes) receives from the place where the reading place shifts. For example, if Dilation = 4, we will receive information from a distance of 4. If we double this Dilated and set the kernel size to 2, only log2𝐷 layer is needed to receive information separated by D. If this time D is hundreds to tens of thousands, only about 10 to 20 layers will be enough.</p>
<p>This time, using this Dilated Convolution, we will create a model that can take distant information into account.</p>
<p><img alt="Calculation image of Dilated Convolution" src="https://storage.googleapis.com/deepmind-live-cms/documents/BlogPost-Fig2-Anim-160908-r01.gif" /></p>
<p><a class="reference external" href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/">WAVENET: A GENERATIVE MODEL FOR RAW AUDIO, blog</a> blogpost</p>
<p>Let’s start by designing the whole network. This network consists of two blocks.</p>
<p>The first block takes an array from length <span class="math notranslate nohighlight">\(2^{17}\)</span> and outputs a vector of length <span class="math notranslate nohighlight">\(2^{10}\)</span>. This makes the input 128(=<span class="math notranslate nohighlight">\(2^{17}\)</span>/<span class="math notranslate nohighlight">\(2^{10}\)</span>) bp corresponds to one position of the output. This is realized by SqueezeBlock. That is, SqueezeBlock accepts the base sequence of the DNA consisting of the length 131072 bp as an input and performs convolution processing so that the information of every 128 bp corresponding to the length of each fragment becomes one value. As a
result, a vector sequence with length of 131072/128 = 1024 is output. This vector sequence can be regarded as the feature of each fragment compressed into one vector.</p>
<p>The second block is the part which calculates the value of each vector in consideration of the information in the long distance, and is handled by DilatedBlock. DilatedBlock receives a vector sequence of 1024 length output from SqueezeBlock and uses the mechanism of Dilated Convolution to efficiently consider the information at mutually separated positions and process it and returns the same lenth of output as the input. We will proceed with training so that this output agrees with the numerical
value (coverage value) representing the possibility of binding of DNA-related protein given for each fragment.</p>
<p>Let’s run the following code.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>import chainer
import chainer.functions as F
import chainer.links as L
import cupy as cp

bc = 24 # base channel

default_squeeze_params = [
    # out_ch, kernel, stride, dropout
    [bc*2, 21, 2, 0], #1 128 -&gt; 64
    [int(bc*2.5), 7, 4, 0.05], #2  64 -&gt; 16
    [int(bc*3.2), 7, 4, 0.05], #3  16 -&gt; 4
    [bc*4, 7, 4, 0.05]  #4  4 -&gt; 1
]


default_dilated_params = [
# out_ch, kernel, dilated
  [bc, 3, 1, 0.1],
  [bc, 3, 2, 0.1],
  [bc, 3, 4, 0.1],
  [bc, 3, 8, 0.1],
  [bc, 3, 16, 0.1],
  [bc, 3, 32, 0.1],
  [bc, 3, 64, 0.1]
]


class Net(chainer.Chain):

    def __init__(self, squeeze_params=default_squeeze_params, dilated_params=default_dilated_params, n_targets=10):
        super(Net, self).__init__()
        self._n_squeeze = len(squeeze_params)
        self._n_dilated = len(dilated_params)
        with self.init_scope():
            in_ch = 4
            for i, param in enumerate(squeeze_params):
                out_ch, kernel, stride, do_rate = param
                setattr(self, &quot;s_{}&quot;.format(i), SqueezeBlock(in_ch, out_ch, kernel, stride, do_rate))
                in_ch = out_ch
            for i, param in enumerate(dilated_params):
                out_ch, kernel, dilated, do_rate = param
                setattr(self, &quot;d_{}&quot;.format(i), DilatedBlock(in_ch, out_ch, kernel, dilated, do_rate))
                in_ch += out_ch
            self.l = L.ConvolutionND(1, None, n_targets, 1)

    def forward(self, x):
        # x : (B, X, 4)
        xp = cp.get_array_module(x)
        h = xp.transpose(x, (0, 2, 1))
        h = h.astype(xp.float32)

        for i in range(self._n_squeeze):
            h = self[&quot;s_{}&quot;.format(i)](h)

        hs = [h]
        for i in range(self._n_dilated):
            h = self[&quot;d_{}&quot;.format(i)](hs)
            hs.append(h)

        h = self.l(F.concat(hs, axis=1))
        h = xp.transpose(h, (0, 2, 1))
        return h
</pre></div>
</div>
</div>
<p>This network receives parameters related to SqueezeBlock and parameters related to DilatedBlock as initialization arguments.</p>
<p>Each receives a list of triples of output channels, kernel sizes, and pooling, and a list of triples of output channels, kernel sizes, and dilated sizes.</p>
<p>Next, we define the block.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>import chainer
import chainer.functions as F
import chainer.links as L
import cupy as cp

class WNConvolutionND(L.ConvolutionND):
    def __init__(self, *args, **kwargs):
        super(WNConvolutionND, self).__init__(*args, **kwargs)
        self.add_param(&#39;g&#39;, self.W.data.shape[0])
        norm = np.linalg.norm(self.W.data.reshape(
            self.W.data.shape[0], -1), axis=1)
        self.g.data[...] = norm

    def __call__(self, x):
        norm = F.batch_l2_norm_squared(self.W) ** 0.5
        channel_size = self.W.data.shape[0]
        norm_broadcasted = F.broadcast_to(
            F.reshape(norm, (channel_size, 1, 1)), self.W.data.shape)
        g_broadcasted = F.broadcast_to(
            F.reshape(self.g, (channel_size, 1, 1)), self.W.data.shape)
        return F.convolution_nd(
            x, g_broadcasted * self.W / norm_broadcasted, self.b, self.stride,
            self.pad, self.cover_all, self.dilate)

class SqueezeBlock(chainer.Chain):
    def __init__(self, in_ch, out_ch, kernel, stride, do_rate):
        super(SqueezeBlock, self).__init__()

        self.do_rate = do_rate
        with self.init_scope():
            pad = kernel // 2
            self.conv = WNConvolutionND(1, in_ch, out_ch*2, kernel, pad=pad, stride=stride)

    def forward(self, x):
        h = self.conv(x)
        h, g = F.split_axis(h, 2, 1)
        h = F.dropout(h * F.sigmoid(g), self.do_rate)
        return h

class DilatedBlock(chainer.Chain):
     def __init__(self, in_ch, out_ch, kernel, dilate, do_rate):
        super(DilatedBlock, self).__init__()
        self.do_rate = do_rate
        with self.init_scope():
            self.conv = WNConvolutionND(1, in_ch, out_ch*2, kernel, pad=dilate, dilate=dilate)

     def forward(self, xs):
        x = F.concat(xs, axis=1)
        h = self.conv(x)
        h, g = F.split_axis(h, 2, 1)
        h = F.dropout(h * F.sigmoid(g), self.do_rate)
        return h

</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ModuleNotFoundError</span>                       Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-7-1a46fb495a52&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-intense-fg ansi-bold">      2</span> <span class="ansi-green-fg">import</span> chainer<span class="ansi-blue-fg">.</span>functions <span class="ansi-green-fg">as</span> F
<span class="ansi-green-intense-fg ansi-bold">      3</span> <span class="ansi-green-fg">import</span> chainer<span class="ansi-blue-fg">.</span>links <span class="ansi-green-fg">as</span> L
<span class="ansi-green-fg">----&gt; 4</span><span class="ansi-red-fg"> </span><span class="ansi-green-fg">import</span> cupy <span class="ansi-green-fg">as</span> cp
<span class="ansi-green-intense-fg ansi-bold">      5</span>
<span class="ansi-green-intense-fg ansi-bold">      6</span> <span class="ansi-green-fg">class</span> WNConvolutionND<span class="ansi-blue-fg">(</span>L<span class="ansi-blue-fg">.</span>ConvolutionND<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>

<span class="ansi-red-fg">ModuleNotFoundError</span>: No module named &#39;cupy&#39;
</pre></div></div>
</div>
<p><img alt="Network Structure" src="https://raw.githubusercontent.com/preferred-medicine/medical-ai-course-materials/master/notebooks/images/7/network.png" /></p>
<p>WeightNormalization [2] is a method of expressing parameter representation in length and direction, and is a normalization method used in cases like this series problem. In the code WeightNormalization <code class="docutils literal notranslate"><span class="pre">WNConvolutionND</span></code> is defined as the convolved layer applied .</p>
<p>SqueezeBlock is a block to shorten the array with a length of <span class="math notranslate nohighlight">\(2^{17}\)</span> to the sequence of <span class="math notranslate nohighlight">\(2^{10}\)</span> (upper figure). We use WNConvolutionND to handle a one-dimensional array, and specify 1, which indicates that it is a one-dimensional array, for the first argument. Also, in the activation function, we use the Gated Linear Unit [3] which is denoted as ℎ=𝑊𝑥∗𝑠𝑖𝑔𝑚𝑜𝑖𝑑(𝑈𝑥). For efficiency in computation, instead of calculating Wx and Ux separately, applying Convolution with twice the number
of output channels, and then divide the output result into two in the channel direction(𝑊𝑥,𝑈𝑥), and then multiply them by element after applying the sigmoid function to one side.</p>
<p>DilatedBlock is a block that calculates using long distance information using Dilated Convolution for an array whose length has already become 1024 (upper figure). It takes dilated as an argument. When using Dilated Convolution you can calculate by simply adding dilated to the argument of the usual Convolution layer (this time ConvolutionND but also Convolution 2D).</p>
<p>Also, In DilatedBlock, we adopts a method called DenseNet [4], in which all previous intermediate results are used as inputs for the next layer (corresponding to <code class="docutils literal notranslate"><span class="pre">concat</span></code> in forward() within DilatedBlock). This is based on making a lot of skip connections in the neural network so that the gradient does not decay and the training becomes easy even if the number of layers increases.</p>
<p>Let’s build a network on a trial and let the sample data flow there.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>import numpy as np
n = Net()
size = 131072 # 128 * 1024
batchsize = 4
x = np.empty((batchsize, size, 4), dtype=np.bool)
y = n.forward(x)
print(y.shape)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
(4, 1024, 10)
</pre></div></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
<p>Here, the array which was originally of batch size (B) = 4, input length (L) = 131072, number of input channels (C) = 4 became the array of B = 4, L = 1024, C = 10 after calculation</p>
<p>The coverage value predicted this time can be regarded as count data showing how frequently the DNA related protein binds to each fragment. Therefore, we use logarithmic Poisson loss function which is loss function for count data in training.</p>
<p>When using the logarithmic Poisson loss function, the model predicts the mean which is the only parameter of the Poisson distribution, and calculates the likelihood of the training data when using the Poisson distribution with the predicted mean. Then maximize its likelihood and minimize the same negative logarithmic likelihood. In this case, ignoring the term which does not include the parameter to be trained on the program. Note that the minimum value of this function will not become 0 as it
is, so subtract the minimum value <span class="math notranslate nohighlight">\(t \log t\)</span> beforehand so that the minimum value of the loss function will be 0.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>import chainer.functions as F
import math
import sklearn
import numpy as np

def log_poisson_loss(log_x, t):
    loss =  F.mean(F.exp(log_x) - t * log_x)
    t = chainer.cuda.to_cpu(t.astype(np.float32))
    offset = F.mean(cp.array(t - t * np.ma.log(t)))
    return loss - offset


def log_r2_score(log_x, t):
    return F.r2_score(F.exp(log_x), t)
</pre></div>
</div>
</div>
<p>We also use CosineScheduler to adjust the learning rate. In training of neural networks, we know that we can find solutions with higher generalization performance gradually as we gradually reduce the learning rate. Since the objective function of the training of the neural network has many poor performance local solutions, the first is to increase the learning rate so that it does not fit into the local solution and find a good solution in the whole, and in the second half, the learning rate is
gradually brought close to 0 and converged. CosineScheduler changes the learning rate like the change of the Cosine function from 0 degrees to 90 degrees. Since training is initially unstable, it is also common to increase the learning rate linearly from 0 to the initial learning rate for the first n_warmup cycles. Since learning rate is low and training is stable this time, n_warmup is set to 0.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>from chainer import training
import numpy as np
import math

class CosineScheduler(training.Extension):

    def __init__(self, attr=&#39;lr&#39;, init_val=0.0001, n_decays=200, n_warmups=3, target=None, optimizer=None):
        self._attr = attr
        self._target = target
        self._optimizer = optimizer
        self._min_loss = None
        self._last_value = None
        self._init_val = init_val
        self._n_decays = n_decays - n_warmups
        self._decay_count = 0
        self._n_warmups = n_warmups

    def __call__(self, trainer):
        updater = trainer.updater
        optimizer = self._get_optimizer(trainer)
        epoch = updater.epoch
        if epoch &lt; self._n_warmups:
            value = self._init_val / (self._n_warmups + 1) * (epoch + 1)
        else:
            value = 0.5 * self._init_val * (1 + math.cos(math.pi * (epoch - self._n_warmups) / self._n_decays))
        self._update_value(optimizer, value)


    def _get_optimizer(self, trainer):
        return self._optimizer or trainer.updater.get_optimizer(&#39;main&#39;)

    def _update_value(self, optimizer, value):
        setattr(optimizer, self._attr, value)
        self._last_value = value
</pre></div>
</div>
</div>
<p>Finally, we apply Data Augmentation which makes a change which does not change meaning to training data during training. This is the same as rotating and translating in the image. Although coverage value is predicted every 128 bp this time, it is expected that the coverage value will be about the same even if it moves several bases (eg 4 to 8). So we shift the array back and forth by max_shift max. (Putting a completely random base sequence in the remaining part may change from the actual base
sequence distribution, so here we roll-shift the roll() function).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>import chainer
import random

class PreprocessedDataset(chainer.dataset.DatasetMixin):

    def __init__(self, xs, ys, max_shift):
        self.xs = xs
        self.ys = ys
        self.max_shift = max_shift

    def __len__(self):
        return len(self.xs)

    def get_example(self, i):
        # It applies following preprocesses:
        #     - Cropping
        #     - Random flip

        x = self.xs[i]
        y = self.ys[i]


        s = random.randint(-self.max_shift, self.max_shift)
        x = np.roll(x, s, axis=0)
        return x, y
</pre></div>
</div>
</div>
<p>All the preparations are complete now. The rest is to remodel the Chainer’s trainer and train it. Run the following code.</p>
<p>Since training takes time in the entire original data, only data / <code class="docutils literal notranslate"><span class="pre">ratio</span></code> portion is used as training and validation data. This time <code class="docutils literal notranslate"><span class="pre">ratio</span></code> is set to 1. In this case, training is completed in about 30 minutes. If you want to try it in a short time, experiment with ratio = 1 as ratio = 10 or ratio = 20.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>import chainer
import chainer.functions as F
import chainer.links as L
import numpy as np
from chainer.training import extensions
from chainer import training
import h5py

ml_h5 = h5py.File(&#39;seq.h5&#39;)

train_x = ml_h5[&#39;train_in&#39;]
train_y = ml_h5[&#39;train_out&#39;]

valid_x = ml_h5[&#39;valid_in&#39;]
valid_y = ml_h5[&#39;valid_out&#39;]

test_x = ml_h5[&#39;test_in&#39;]
test_y = ml_h5[&#39;test_out&#39;]

ratio = 1
train_x = train_x[:len(train_x)//ratio]
train_y = train_y[:len(train_y)//ratio]
valid_x = valid_x[:len(valid_x)//ratio]
valid_y = valid_y[:len(valid_y)//ratio]


max_shift_for_data_augmentation = 5
train = PreprocessedDataset(train_x, train_y, max_shift_for_data_augmentation)
val = chainer.datasets.TupleDataset(valid_x, valid_y)

batchsize = 8

train_iter = chainer.iterators.SerialIterator(train, batchsize)
val_iter = chainer.iterators.SerialIterator(val, batchsize, repeat=False, shuffle=False)

model = L.Classifier(Net(), lossfun=log_poisson_loss, accfun=log_r2_score)

lr = 0.001
optimizer = chainer.optimizers.Adam(alpha=lr, beta1=0.97, beta2=0.98)
optimizer.setup(model)
optimizer.add_hook(chainer.optimizer_hooks.GradientClipping(threshold=0.01))


updater = training.updaters.StandardUpdater(
     train_iter, optimizer, device=0)

n_epochs = 10
n_warmups = 0
out = &quot;out&quot;
trainer = training.Trainer(updater, (n_epochs, &#39;epoch&#39;), out=out)
trainer.extend(CosineScheduler(attr=&#39;alpha&#39;, init_val=lr, n_decays=n_epochs, n_warmups=n_warmups), trigger=(1, &#39;epoch&#39;))

trainer.extend(extensions.Evaluator(val_iter, model, device = 0))
trainer.extend(extensions.LogReport(trigger=(0.2, &#39;epoch&#39;)))
trainer.extend(extensions.snapshot_object(model, &#39;model_epoch_{.updater.epoch}&#39;), trigger=(1, &#39;epoch&#39;))

trainer.extend(extensions.PrintReport(
          [&#39;epoch&#39;, &#39;main/loss&#39;, &#39;validation/main/loss&#39;, &#39;elapsed_time&#39;]), trigger = (0.1, &#39;epoch&#39;))

# trainer.extend(extensions.ProgressBar())

trainer.run()

</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
epoch       main/loss   validation/main/loss  elapsed_time
0           2.48903                           67.7519
0           1.84639                           117.127
0           1.89686                           166.72
0           1.81704                           215.449
1           1.85827     1.85512               274.106
1           1.81286                           323.281
1           1.74802                           372.488
1           1.80567                           421.261
1           1.7467                            470.755
2           1.70371     1.78047               528.83
2           1.77928                           577.477
2           1.67051                           626.814
2           1.6415                            675.927
2           1.67238                           725.017
3           1.69656     1.70897               782.987
3           1.63935                           831.673
3           1.64996                           881.092
3           1.63925                           930.107
3           1.71683                           979.111
4           1.63116     1.71748               1036.98
4           1.64786                           1085.9
4           1.6442                            1134.54
4           1.57821                           1183.92
4           1.62886                           1232.91
5           1.61523     1.66392               1290.8
5           1.65216                           1339.78
5           1.61142                           1388.37
5           1.61483                           1437.71
5           1.57835                           1486.61
6           1.56529     1.63406               1544.53
6           1.59062                           1593.49
6           1.61102                           1642.09
6           1.60003                           1691.49
6           1.57222                           1740.46
7           1.55098     1.62176               1798.31
7           1.54207                           1847.28
7           1.5653                            1895.92
7           1.57523                           1944.68
7           1.61043                           1993.73
8           1.57391     1.62377               2051.65
8           1.51835                           2100.61
8           1.58225                           2149.57
8           1.59289                           2198.5
8           1.56643                           2247.32
9           1.55151     1.62115               2305.72
9           1.53593                           2354.7
9           1.57812                           2403.76
9           1.54277                           2452.85
9           1.55514                           2501.51
</pre></div></div>
</div>
<p>If training is successful, the model trained should be output under directory out. Let’s see if the model is actually being output.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>!ls -l out/
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
total 14172
-rw-r--r-- 1 root root   10080 Dec 16 05:24 log
-rw-r--r-- 1 root root 1445890 Dec 16 04:47 model_epoch_1
-rw-r--r-- 1 root root 1447626 Dec 16 05:25 model_epoch_10
-rw-r--r-- 1 root root 1446428 Dec 16 04:51 model_epoch_2
-rw-r--r-- 1 root root 1446742 Dec 16 04:55 model_epoch_3
-rw-r--r-- 1 root root 1447061 Dec 16 04:59 model_epoch_4
-rw-r--r-- 1 root root 1447268 Dec 16 05:04 model_epoch_5
-rw-r--r-- 1 root root 1447473 Dec 16 05:08 model_epoch_6
-rw-r--r-- 1 root root 1447585 Dec 16 05:12 model_epoch_7
-rw-r--r-- 1 root root 1447649 Dec 16 05:16 model_epoch_8
-rw-r--r-- 1 root root 1447650 Dec 16 05:21 model_epoch_9
</pre></div></div>
</div>
<p>Next, let us also predict test data using the trained model. Let’s load the model after training as follows and apply the model to the test data.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>import chainer
import chainer.links as L
%matplotlib inline
import matplotlib.pyplot as plt

model_n_epoch = 10
out_dir = &#39;out&#39;
model = L.Classifier(Net())
chainer.serializers.load_npz(&#39;{}/model_epoch_{}&#39;.format(out_dir, model_n_epoch), model)
predictor = model.predictor

print(len(test_x))
with chainer.no_backprop_mode():
    test_y_estimated = F.exp(predictor(test_x[:1]))

test_y = test_y[:1]

print(test_y_estimated.shape)
print(test_y_estimated[0,:,0])


</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
500
(1, 1024, 10)
variable([1.8674504  2.004048   1.68377    ... 0.81418294 0.7608197
          0.8720923 ])
</pre></div></div>
</div>
<p>Let’s excerpt the result and display it. Here, the correct answer and estimation result are output for the first (i = 0) output. Even in this case, although we narrowed down the training data (the number of classes was set to 10) and the number of training is also small, you can see that the peak is captured.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>y = test_y_estimated.data
fig_size = plt.rcParams[&quot;figure.figsize&quot;]
fig_size[0] = 20
fig_size[1] = 10
i = 0
b1 = plt.bar(range(y.shape[1]), y[0,:,i])
b2 = plt.bar(range(y.shape[1]), test_y[0,:,i])
plt.legend((b1, b2), (&#39;estimated&#39;, &#39;observed&#39;))

</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>&lt;matplotlib.legend.Legend at 0x7f037ea9f6d8&gt;
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_DNA_Sequence_Data_Analysis_44_1.png" src="../_images/notebooks_DNA_Sequence_Data_Analysis_44_1.png" />
</div>
</div>
<p>If you have time, you can find out if more accurate models would be trained by increasing n_epochs of training from 10 to somewhere in between 30 and 50, or increasing the number of layers, or increasing the number of channels.</p>
<ul class="simple">
<li><p>[1] “Sequential regulatory activity prediction across chromosomes with convolutional neural networks”, D. R. Kelly and et al., Genome Res. 2018. 28: 739-750</p></li>
<li><p>[2] “Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks”, T. Salimans and et al., arXiv:1602.07868</p></li>
<li><p>[3] “Language Modeling with Gated Convolutional Networks”, Y. N. Dauphin and et al., arXiv:1612.08083</p></li>
<li><p>[4] “Densely Connected Convolutional Networks”, G. Huang, and et al., CVPR 2017</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>
</pre></div>
</div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="Sequential_Data_Analysis_with_Deep_Learning.html" class="btn btn-neutral float-right" title="8. Practical part: Time series analysis of monitoring data using the deep learning" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="Blood_Cell_Detection.html" class="btn btn-neutral" title="6. Practice section: Detection of cells from microscope images of blood" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Preferred Networks &amp; キカガク

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>