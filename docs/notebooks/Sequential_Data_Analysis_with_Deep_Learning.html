

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>8. Practical part: Time series analysis of monitoring data using the deep learning &mdash; メディカルAI専門コース オンライン講義資料  documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="7. Practical part: sequence analysis using deep learning" href="DNA_Sequence_Data_Analysis.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-797798-11"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'UA-797798-11');
  </script>

  <meta name="description" content="メディカルAI学会公認資格向けオンライン講義資料。機械学習に必要な数学の基礎の解説から深層学習（ディープラーニング）を用いた実践的な内容までGoogle Colaboratory上でGPUを用いて実際にコードを実行可能な形式にしオンライン資料として無料公開。">
  <meta property="og:title" content="メディカルAI専門コース オンライン講義資料">
  <meta property="og:description" content="メディカルAI学会公認資格向けオンライン講義資料。機械学習に必要な数学の基礎の解説から深層学習（ディープラーニング）を用いた実践的な内容までGoogle Colaboratory上でGPUを用いて実際にコードを実行可能な形式にしオンライン資料として無料公開。">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://japan-medical-ai.github.io/medical-ai-course-materials/">
  <meta property="og:image" content="https://raw.githubusercontent.com/japan-medical-ai/medical-ai-course-materials/master/notebooks/images/medical_ai.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@PreferredNetJP">
  <meta name="twitter:creator" content="@PreferredNetJP">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> メディカルAI専門コース オンライン講義資料
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Basic_Math_for_ML.html">1. Basis of the mathematics required to machine learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Introduction_to_ML_libs.html">2. Basics of machine learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Introduction_to_Neural_Network.html">3. Basics of neural network</a></li>
<li class="toctree-l1"><a class="reference internal" href="Introduction_to_Chainer.html">4. Introduction to Deep Learning Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="Image_Segmentation.html">5. Practice: Segmentation of MRI</a></li>
<li class="toctree-l1"><a class="reference internal" href="Blood_Cell_Detection.html">6. Practice section: Detection of cells from microscope images of blood</a></li>
<li class="toctree-l1"><a class="reference internal" href="DNA_Sequence_Data_Analysis.html">7. Practical part: sequence analysis using deep learning</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">8. Practical part: Time series analysis of monitoring data using the deep learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Environment">8.1. Environment</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Electrocardiogram-(ECG)-and-arrhythmia-diagnosis">8.2. Electrocardiogram (ECG) and arrhythmia diagnosis</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Data-sets">8.3. Data sets</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Data-pre-processing">8.4. Data pre-processing</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Series-data-analysis-when-using-the-deep-learning">8.5. Series data analysis when using the deep learning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Learning">8.5.1. Learning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Evaluation">8.6. Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Towards-the-accuracy">8.7. Towards the accuracy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Corresponds-to-the-class-imbalance-data">8.7.1. Corresponds to the class imbalance data</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Sampling">8.7.1.1. Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Changing-the-loss-function">8.7.1.2. Changing the loss function</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#Changing-the-network-structure">8.7.2. Changing the network structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Validating-the-effect-of-noise">8.7.3. Validating the effect of noise</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Conclusion">8.8. Conclusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="#References">8.9. References</a></li>
</ul>
</li>
</ul>

            
          
          <div style="padding-right:20px; bottom:10px;">
            <a href="https://short-term.kikagaku.co.jp/dnn-seminar/">
              <img src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/img_handson.png" />
              <p style="padding:5px; font-size:small; line-height: 150%">ディープラーニングの詳しい解説や画像・自然言語の取り扱い、クラウド上のGPUを使った実践的な演習をご希望の方はこちらがおすすめです</p>
            </a>
          </div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">メディカルAI専門コース オンライン講義資料</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>8. Practical part: Time series analysis of monitoring data using the deep learning</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/notebooks/Sequential_Data_Analysis_with_Deep_Learning.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 5ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    background: #f5f5f5;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<p><a class="reference external" href="https://colab.research.google.com/github/preferred-medicine/medical-ai-course-materials/blob/master/notebooks/Sequential_Data_Analysis_with_Deep_Learning.ipynb"><img alt="colab-logo" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<div class="section" id="Practical-part:-Time-series-analysis-of-monitoring-data-using-the-deep-learning">
<h1>8. Practical part: Time series analysis of monitoring data using the deep learning<a class="headerlink" href="#Practical-part:-Time-series-analysis-of-monitoring-data-using-the-deep-learning" title="Permalink to this headline">¶</a></h1>
<p>As health consciousness rises and the number of exercising individuals increases, wearable devices such as activity meters are on the rise. Recently, cases of utilization of these devices in the healthcare field is increasing. It is now possible to monitor health status eg. Heart Rate in real time by acquiring data from sensor devices. In February 2018, a joint research study by Cardiogram Inc. and the University of California, San Francisco reported that it was possible to predict diabetes with
fairly high accuracy by applying Deep Learning to heart rate data. In addition, the development of sensor devices with features such as Electrocardiogram are now available in products such as the Apple Watch Series 4, where more detailed information can be acquired. Against this backdrop, it is clear that efforts to collect and analyze health monitoring data leading towards better health management, will become more and more popular in the future.</p>
<p>In this section, we will address the problem of detecting cardiac arrhythmias from signal waveform data of the electrocardiogram.</p>
<div class="section" id="Environment">
<h2>8.1. Environment<a class="headerlink" href="#Environment" title="Permalink to this headline">¶</a></h2>
<p>In this chapter, we use the following library.</p>
<ul class="simple">
<li><p>Cupy</p></li>
<li><p>Chainer</p></li>
<li><p>Scipy</p></li>
<li><p>Matplotlib</p></li>
<li><p>Seaborn</p></li>
<li><p>Pandas</p></li>
<li><p>WFDB</p></li>
<li><p>Scikit-learn</p></li>
<li><p>Imbalanced-learn</p></li>
</ul>
<p>Please execute (Shift + Enter) the following cells and install what you need.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>!apt -y -q install tree
!pip install wfdb==2.2.1 imbalanced-learn==0.4.3
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Unable to locate an executable at &#34;/Library/Java/JavaVirtualMachines/jdk1.8.0_191.jdk/Contents/Home/bin/apt&#34; (-1)
<span class="ansi-yellow-fg">DEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won&#39;t be maintained after that date. A future version of pip will drop support for Python 2.7.</span>
Requirement already satisfied: wfdb==2.2.1 in /Users/sandeepayyar/Documents/.env/lib/python2.7/site-packages (2.2.1)
Requirement already satisfied: imbalanced-learn==0.4.3 in /Users/sandeepayyar/Documents/.env/lib/python2.7/site-packages (0.4.3)
Requirement already satisfied: pandas&gt;=0.19.1 in /Users/sandeepayyar/Documents/.env/lib/python2.7/site-packages (from wfdb==2.2.1) (0.23.4)
Requirement already satisfied: numpy&gt;=1.11.0 in /Users/sandeepayyar/Documents/.env/lib/python2.7/site-packages (from wfdb==2.2.1) (1.15.4)
Requirement already satisfied: nose&gt;=1.3.7 in /Users/sandeepayyar/Documents/.env/lib/python2.7/site-packages (from wfdb==2.2.1) (1.3.7)
Requirement already satisfied: matplotlib&gt;=1.5.1 in /Users/sandeepayyar/Documents/.env/lib/python2.7/site-packages (from wfdb==2.2.1) (2.2.3)
Requirement already satisfied: scipy&gt;=0.19.0 in /Users/sandeepayyar/Documents/.env/lib/python2.7/site-packages (from wfdb==2.2.1) (1.1.0)
Requirement already satisfied: requests&gt;=2.10.0 in /Users/sandeepayyar/Documents/.env/lib/python2.7/site-packages (from wfdb==2.2.1) (2.20.0)
Requirement already satisfied: sklearn&gt;=0.0 in /Users/sandeepayyar/Documents/.env/lib/python2.7/site-packages (from wfdb==2.2.1) (0.0)
Requirement already satisfied: scikit-learn&gt;=0.20 in /Users/sandeepayyar/Documents/.env/lib/python2.7/site-packages (from imbalanced-learn==0.4.3) (0.20.3)
Requirement already satisfied: python-dateutil&gt;=2.5.0 in /Users/sandeepayyar/Documents/.env/lib/python2.7/site-packages (from pandas&gt;=0.19.1-&gt;wfdb==2.2.1) (2.7.5)
Requirement already satisfied: pytz&gt;=2011k in /Users/sandeepayyar/Documents/.env/lib/python2.7/site-packages (from pandas&gt;=0.19.1-&gt;wfdb==2.2.1) (2018.7)
Requirement already satisfied: kiwisolver&gt;=1.0.1 in /Users/sandeepayyar/Documents/.env/lib/python2.7/site-packages (from matplotlib&gt;=1.5.1-&gt;wfdb==2.2.1) (1.0.1)
Requirement already satisfied: cycler&gt;=0.10 in /Users/sandeepayyar/Documents/.env/lib/python2.7/site-packages (from matplotlib&gt;=1.5.1-&gt;wfdb==2.2.1) (0.10.0)
Requirement already satisfied: subprocess32 in /Users/sandeepayyar/Documents/.env/lib/python2.7/site-packages (from matplotlib&gt;=1.5.1-&gt;wfdb==2.2.1) (3.5.3)
Requirement already satisfied: six&gt;=1.10 in /Users/sandeepayyar/Documents/.env/lib/python2.7/site-packages (from matplotlib&gt;=1.5.1-&gt;wfdb==2.2.1) (1.11.0)
Requirement already satisfied: backports.functools-lru-cache in /Users/sandeepayyar/Documents/.env/lib/python2.7/site-packages (from matplotlib&gt;=1.5.1-&gt;wfdb==2.2.1) (1.5)
Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /Users/sandeepayyar/Documents/.env/lib/python2.7/site-packages (from matplotlib&gt;=1.5.1-&gt;wfdb==2.2.1) (2.3.0)
Requirement already satisfied: urllib3&lt;1.25,&gt;=1.21.1 in /Users/sandeepayyar/Documents/.env/lib/python2.7/site-packages (from requests&gt;=2.10.0-&gt;wfdb==2.2.1) (1.24.1)
Requirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in /Users/sandeepayyar/Documents/.env/lib/python2.7/site-packages (from requests&gt;=2.10.0-&gt;wfdb==2.2.1) (3.0.4)
Requirement already satisfied: idna&lt;2.8,&gt;=2.5 in /Users/sandeepayyar/Documents/.env/lib/python2.7/site-packages (from requests&gt;=2.10.0-&gt;wfdb==2.2.1) (2.7)
Requirement already satisfied: certifi&gt;=2017.4.17 in /Users/sandeepayyar/Documents/.env/lib/python2.7/site-packages (from requests&gt;=2.10.0-&gt;wfdb==2.2.1) (2018.10.15)
Requirement already satisfied: setuptools in /Users/sandeepayyar/Documents/.env/lib/python2.7/site-packages (from kiwisolver&gt;=1.0.1-&gt;matplotlib&gt;=1.5.1-&gt;wfdb==2.2.1) (40.6.3)
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>!curl https://colab.chainer.org/install | sh -
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  1580  100  1580    0     0   6930      0 --:--:-- --:--:-- --:--:--  6929
sh: line 9: nvidia-smi: command not found
********************************************************************************
GPU is not enabled!
Open &#34;Runtime&#34; &gt; &#34;Change runtime type&#34; and set &#34;Hardware accelerator&#34; to &#34;GPU&#34;.
********************************************************************************
</pre></div></div>
</div>
<p>When installation is completed, execute the following cell, import each library, and check the version.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>import os
import random
import numpy as np
import chainer
import scipy
import pandas as pd
import matplotlib
import seaborn as sn
import wfdb
import sklearn
import imblearn

chainer.print_runtime_info()
print(&quot;Scipy: &quot;, scipy.__version__)
print(&quot;Pandas: &quot;, pd.__version__)
print(&quot;Matplotlib: &quot;, matplotlib.__version__)
print(&quot;Seaborn: &quot;, sn.__version__)
print(&quot;WFDB: &quot;, wfdb.__version__)
print(&quot;Scikit-learn: &quot;, sklearn.__version__)
print(&quot;Imbalanced-learn: &quot;, imblearn.__version__)
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/Users/sandeepayyar/Documents/.env/lib/python2.7/site-packages/chainer/_environment_check.py:37: UserWarning: Accelerate has been detected as a NumPy backend library.
vecLib, which is a part of Accelerate, is known not to work correctly with Chainer.
We recommend using other BLAS libraries such as OpenBLAS.
For details of the issue, please see
https://docs.chainer.org/en/stable/tips.html#mnist-example-does-not-converge-in-cpu-mode-on-mac-os-x.

Please be aware that Mac OS X is not an officially supported OS.

  &#39;&#39;&#39;)  # NOQA
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Platform: Darwin-17.7.0-x86_64-i386-64bit
Chainer: 5.1.0
NumPy: 1.15.4
CuPy: Not Available
iDeep: Not Available
(&#39;Scipy: &#39;, &#39;1.1.0&#39;)
(&#39;Pandas: &#39;, u&#39;0.23.4&#39;)
(&#39;Matplotlib: &#39;, &#39;2.2.3&#39;)
(&#39;Seaborn: &#39;, &#39;0.9.0&#39;)
(&#39;WFDB: &#39;, &#39;2.2.1&#39;)
(&#39;Scikit-learn: &#39;, &#39;0.20.3&#39;)
(&#39;Imbalanced-learn: &#39;, &#39;0.4.3&#39;)
</pre></div></div>
</div>
<p>Also, in order to reproduce the execution result of this chapter, fix the random number seed introduced in Chapter 4 (4.2.4.4).</p>
<p>(This setting is not necessarily required for subsequent calculations.)</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>def reset_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    if chainer.cuda.available:
        chainer.cuda.cupy.random.seed(seed)
reset_seed(42)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Electrocardiogram-(ECG)-and-arrhythmia-diagnosis">
<h2>8.2. Electrocardiogram (ECG) and arrhythmia diagnosis<a class="headerlink" href="#Electrocardiogram-(ECG)-and-arrhythmia-diagnosis" title="Permalink to this headline">¶</a></h2>
<div class="line-block">
<div class="line">Electrocardiography is a method used to analyze the heart’s conduction system, which provides us with information about the heart’s electrical activity. The recording of the conduction system is physically represented as an ECG. The electrocardiogram is widely used for inspection/diagnosis of arrhythmia and ischemic heart disease.</div>
<div class="line">[<a class="reference external" href="https://en.wikipedia.org/wiki/Electrocardiography">Reference 1</a>, <a class="reference external" href="https://www.ningen-dock.jp/wp/wp-content/uploads/2013/09/d4bb55fcf01494e251d315b76738ab40.pdf">Reference 2</a>]．</div>
</div>
<p>A standard electrocardiogram is recorded from 3 bipolar limb leads (Ⅰ, Ⅱ, Ⅲ), 3 unipolar limb leads (𝑎𝑉𝑅, 𝑎𝑉𝐿, 𝑎𝑉𝐹) and 6 unipolar chest leads taken from the chest (chest lead) 𝑉1, 𝑉2, 𝑉3, 𝑉4, 𝑉5, 𝑉6 leads also called precordial or V leads, consisting of 12 guides in total.</p>
<p>Among them, particularly when screening arrhythmia, Ⅱ with induction 𝑉1 It is generally considered that diagnosis is carried out focusing on induction.</p>
<p>In the normal state of the heart, regular waveforms are observed in ECG, and this is called normal sinus rhythm (NSR). NSR describes the characteristic rhythm of the healthy human heart.</p>
<p>Specifically, it consists of the following three main waveforms</p>
<ol class="arabic simple">
<li><p><strong>P wave</strong> : depolarization of atrium (excitement of atrium)</p></li>
<li><p><strong>QRS wave</strong> : ventricular depolarization (ventricular excitation)</p></li>
<li><p><strong>T wave</strong> : ventricular repolarization (ventricular excitation constriction)</p></li>
</ol>
<p>In this order, the waveforms shown below are observed.</p>
<p><img alt="Schematic diagram of normal electrocardiogram" src="https://raw.githubusercontent.com/preferred-medicine/medical-ai-course-materials/master/notebooks/images/monitoring/sinus_rhythm.png" /></p>
<p>(Quoted from <a class="reference external" href="https://en.wikipedia.org/wiki/Electrocardiography">Reference 1</a>)</p>
<p>If these regular waveforms are disturbed then it is likely that there is an abnormality in the tuning. Therefore further diagnosis will be warranted to detect any potential condition such as arrhythmia.</p>
</div>
<div class="section" id="Data-sets">
<h2>8.3. Data sets<a class="headerlink" href="#Data-sets" title="Permalink to this headline">¶</a></h2>
<p>Here, we will use the ECG data set from <a class="reference external" href="https://www.physionet.org/physiobank/database/mitdb/">MIT-BIH Arrhythmia Database (mitdb)</a> published by Beth Israel Hospital and MIT.</p>
<p>The data set includes 48 records collected from 47 patients, and each record file contains two leads (𝐼𝐼,𝑉1) and is about 30 minutes long Signal data is stored. Annotation is given to the peak position of each R wave. (Please refer <a class="reference external" href="https://www.physionet.org/physiobank/database/html/mitdbdir/intro.htm">here</a> for details on data and annotations).</p>
<p>The database is managed by <a class="reference external" href="https://www.physionet.org/">PhysioNet</a> and we will use their provided Python package for downloading and reading the data.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>dataset_root = &#39;./dataset&#39;
download_dir = os.path.join(dataset_root, &#39;download&#39;)
</pre></div>
</div>
</div>
<p>First let’s download the mitdb database.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[24]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>wfdb.dl_database(&#39;mitdb&#39;, dl_dir=download_dir)
</pre></div>
</div>
</div>
<p>When the download is completed successfully, a message will be displayed. <code class="docutils literal notranslate"><span class="pre">Finished</span> <span class="pre">downloading</span> <span class="pre">files</span></code></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>print(sorted(os.listdir(download_dir)))
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">OSError</span>                                   Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-9-de07a81672ba&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> </span><span class="ansi-green-fg">print</span><span class="ansi-blue-fg">(</span>sorted<span class="ansi-blue-fg">(</span>os<span class="ansi-blue-fg">.</span>listdir<span class="ansi-blue-fg">(</span>download_dir<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>

<span class="ansi-red-fg">OSError</span>: [Errno 2] No such file or directory: &#39;./dataset/download&#39;
</pre></div></div>
</div>
<p>The number in the file name represents the record ID. Each record has three kinds of files,</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">.dat</span></code> : Signal (binary format)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">.atr</span></code> : Annotation (binary format)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">.hea</span></code> : Header (required for loading binary file)</p></li>
</ul>
</div>
<div class="section" id="Data-pre-processing">
<h2>8.4. Data pre-processing<a class="headerlink" href="#Data-pre-processing" title="Permalink to this headline">¶</a></h2>
<p>This section explains data preprocessing step which basically involves reading the downloaded files and converting them into the appropriate input form for the machine learning model.</p>
<p>In this section, preprocessing is performed by the following procedure.</p>
<ol class="arabic simple">
<li><p>Split record ID in advance for study / evaluation</p>
<ul class="simple">
<li><p>Out of 48 records,</p>
<ul>
<li><p>The signal of ID = (102, 104, 107, 217) is excluded because it contains the pace of the pacemaker (paced beats).</p></li>
<li><p>The signal with ID = 114 is excluded this time because a part of the waveform is inverted.</p></li>
<li><p>The ID (201, 202) are data from the same patient. Hence 202 is excluded.</p></li>
</ul>
</li>
<li><p>We divide 42 records in total excluding the above for learning (training) and testing (see <a class="reference external" href="https://ieeexplore.ieee.org/document/1306572">Reference 3</a> for the division method).]</p></li>
</ul>
</li>
<li><p>Read signal file (.dat)</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Ⅱ\)</span> Induced signal and <span class="math notranslate nohighlight">\(V_1\)</span> Induced signals are stored, but this time <span class="math notranslate nohighlight">\(Ⅱ\)</span> We only use guidance.．</p></li>
<li><p>Since the sampling frequency is 360 Hz, it means that the number is recorded at 360 paces per second.</p></li>
</ul>
</li>
<li><p>Reading annotation file (.atr)</p>
<ul class="simple">
<li><p>Obtain the position (positions) of each R wave peak and its label (symbols).</p></li>
</ul>
</li>
<li><p>Normalization of signals</p>
<ul class="simple">
<li><p>We convert the signals so that it becomes average 0 and dispersion 1.</p></li>
</ul>
</li>
<li><p>Signal segmentation</p>
<ul class="simple">
<li><p>We cut out fragments of 2 seconds (around 1 second before and after) around each R wave peak.</p></li>
</ul>
</li>
<li><p>Labeling of split signals</p>
<ul class="simple">
<li><p>The labels attached to each R wave peak are aggregated according to the following table (*), labels corresponding to normal beat (Normal) and ventricular ectopic beat (VEB) are given in this analysis Only the divided signal is used for learning and evaluation.</p></li>
</ul>
</li>
</ol>
<p>※ The standards recommended by the Association for the Advancement of Medical Instrumentation (AAMI) (<a class="reference external" href="https://ieeexplore.ieee.org/document/1306572">Reference 3</a>]) are roughly classified into five categories.</p>
<p><img alt="AAMIの分類基準" src="https://raw.githubusercontent.com/preferred-medicine/medical-ai-course-materials/master/notebooks/images/monitoring/aami_standard.png" /></p>
<p>(Quoted from <a class="reference external" href="https://arxiv.org/abs/1810.04121">Reference 4</a>])</p>
<p>Let’s first define the data preprocessing class by executing the following cells.</p>
<p>In the preprocessing class, the following member functions are defined.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">__init__()</span></code> (Constructor): Initialization of variables, division rules for learning and testing, aggregation rules of labels to be used</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">_load_data()</span></code> :Reading of signals and annotations</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">_normalize_signal()</span></code> : <code class="docutils literal notranslate"><span class="pre">method</span></code>Scale signal according to option</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">_segment_data()</span></code> : (<code class="docutils literal notranslate"><span class="pre">window_size</span></code>) Cut the read signal and annotation with constant width ()</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">preprocess_dataset()</span></code> : Create learning data and test data</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">_preprocess_dataset_core()</span></code> : <code class="docutils literal notranslate"><span class="pre">preprocess_datataset()</span></code>Main process called within.</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>class BaseECGDatasetPreprocessor(object):
    def __init__(
            self,
            dataset_root,
            window_size=720,  # 2 seconds
    ):
        self.dataset_root = dataset_root
        self.download_dir = os.path.join(self.dataset_root, &#39;download&#39;)
        self.window_size = window_size
        self.sample_rate = 360.
        # split list
        self.train_record_list = [
            &#39;101&#39;, &#39;106&#39;, &#39;108&#39;, &#39;109&#39;, &#39;112&#39;, &#39;115&#39;, &#39;116&#39;, &#39;118&#39;, &#39;119&#39;, &#39;122&#39;,
            &#39;124&#39;, &#39;201&#39;, &#39;203&#39;, &#39;205&#39;, &#39;207&#39;, &#39;208&#39;, &#39;209&#39;, &#39;215&#39;, &#39;220&#39;, &#39;223&#39;, &#39;230&#39;
        ]
        self.test_record_list = [
            &#39;100&#39;, &#39;103&#39;, &#39;105&#39;, &#39;111&#39;, &#39;113&#39;, &#39;117&#39;, &#39;121&#39;, &#39;123&#39;, &#39;200&#39;, &#39;210&#39;,
            &#39;212&#39;, &#39;213&#39;, &#39;214&#39;, &#39;219&#39;, &#39;221&#39;, &#39;222&#39;, &#39;228&#39;, &#39;231&#39;, &#39;232&#39;, &#39;233&#39;, &#39;234&#39;
        ]
        # annotation
        self.labels = [&#39;N&#39;, &#39;V&#39;]
        self.valid_symbols = [&#39;N&#39;, &#39;L&#39;, &#39;R&#39;, &#39;e&#39;, &#39;j&#39;, &#39;V&#39;, &#39;E&#39;]
        self.label_map = {
            &#39;N&#39;: &#39;N&#39;, &#39;L&#39;: &#39;N&#39;, &#39;R&#39;: &#39;N&#39;, &#39;e&#39;: &#39;N&#39;, &#39;j&#39;: &#39;N&#39;,
            &#39;V&#39;: &#39;V&#39;, &#39;E&#39;: &#39;V&#39;
        }
    def _load_data(
            self,
            base_record,
            channel=0  # [0, 1]
    ):
        record_name = os.path.join(self.download_dir, str(base_record))
        # read dat file
        signals, fields = wfdb.rdsamp(record_name)
        assert fields[&#39;fs&#39;] == self.sample_rate
        # read annotation file
        annotation = wfdb.rdann(record_name, &#39;atr&#39;)
        symbols = annotation.symbol
        positions = annotation.sample
        return signals[:, channel], symbols, positions

    def _normalize_signal(
            self,
            signal,
            method=&#39;std&#39;
    ):
        if method == &#39;minmax&#39;:
            # Min-Max scaling
            min_val = np.min(signal)
            max_val = np.max(signal)
            return (signal - min_val) / (max_val - min_val)
        elif method == &#39;std&#39;:
            # Zero mean and unit variance
            signal = (signal - np.mean(signal)) / np.std(signal)
            return signal
        else:
            raise ValueError(&quot;Invalid method: {}&quot;.format(method))

    def _segment_data(
            self,
            signal,
            symbols,
            positions
    ):
        X = []
        y = []
        sig_len = len(signal)
        for i in range(len(symbols)):
            start = positions[i] - self.window_size // 2
            end = positions[i] + self.window_size // 2
            if symbols[i] in self.valid_symbols and start &gt;= 0 and end &lt;= sig_len:
                segment = signal[start:end]
                assert len(segment) == self.window_size, &quot;Invalid length&quot;
                X.append(segment)
                y.append(self.labels.index(self.label_map[symbols[i]]))
        return np.array(X), np.array(y)

    def preprocess_dataset(
            self,
            normalize=True
    ):
        # preprocess training dataset
        self._preprocess_dataset_core(self.train_record_list, &quot;train&quot;, normalize)
        # preprocess test dataset
        self._preprocess_dataset_core(self.test_record_list, &quot;test&quot;, normalize)

    def _preprocess_dataset_core(
            self,
            record_list,
            mode=&quot;train&quot;,
            normalize=True
    ):
        Xs, ys = [], []
        save_dir = os.path.join(self.dataset_root, &#39;preprocessed&#39;, mode)
        for i in range(len(record_list)):
            signal, symbols, positions = self._load_data(record_list[i])
            if normalize:
                signal = self._normalize_signal(signal)
            X, y = self._segment_data(signal, symbols, positions)
            Xs.append(X)
            ys.append(y)
        os.makedirs(save_dir, exist_ok=True)
        np.save(os.path.join(save_dir, &quot;X.npy&quot;), np.vstack(Xs))
        np.save(os.path.join(save_dir, &quot;y.npy&quot;), np.concatenate(ys))
</pre></div>
</div>
</div>
<p>By specifying the root directory (dataset_root) of the data save destination <code class="docutils literal notranslate"><span class="pre">preprocess_dataset()</span></code> and executing, the preprocessed data is saved in a predetermined place in Numpy Array format.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>BaseECGDatasetPreprocessor(dataset_root).preprocess_dataset()
</pre></div>
</div>
</div>
<p>After execution, make sure that the following files are saved.</p>
<ul class="simple">
<li><p>train/X.npy : Signal for learning</p></li>
<li><p>train/y.npy : Label for learning</p></li>
<li><p>test/X.npy : signal for evaluation</p></li>
<li><p>test/y.npy : Evaluation label</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>!tree ./dataset/preprocessed
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
./dataset/preprocessed
├── test
│   ├── X.npy
│   └── y.npy
└── train
    ├── X.npy
    └── y.npy

2 directories, 4 files
</pre></div></div>
</div>
<p>Next, read the saved file and check the contents.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>X_train = np.load(os.path.join(dataset_root, &#39;preprocessed&#39;, &#39;train&#39;, &#39;X.npy&#39;))
y_train = np.load(os.path.join(dataset_root, &#39;preprocessed&#39;, &#39;train&#39;, &#39;y.npy&#39;))
X_test = np.load(os.path.join(dataset_root, &#39;preprocessed&#39;, &#39;test&#39;, &#39;X.npy&#39;))
y_test = np.load(os.path.join(dataset_root, &#39;preprocessed&#39;, &#39;test&#39;, &#39;y.npy&#39;))
</pre></div>
</div>
</div>
<p>The number of samples in the data set are as follows.</p>
<ul class="simple">
<li><p>For learning: 47738 samples</p></li>
<li><p>For evaluation: 45349 samples</p></li>
</ul>
<p>Each signal data is expressed as 2 (sec) * 360 (Hz) = 720 dimensional vector.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>print(&quot;X_train.shape = &quot;, X_train.shape, &quot; \t y_train.shape = &quot;, y_train.shape)
print(&quot;X_test.shape = &quot;, X_test.shape, &quot; \t y_test.shape = &quot;, y_test.shape)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
X_train.shape =  (47738, 720)    y_train.shape =  (47738,)
X_test.shape =  (45349, 720)     y_test.shape =  (45349,)
</pre></div></div>
</div>
<p>Each label is represented by an index,</p>
<ul class="simple">
<li><p>0: Normal beat (Normal)</p></li>
<li><p>1: Ventricular Ectopic Beat (VEB)</p></li>
</ul>
<p>Let’s count the number of samples for each label included in the training data set.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>uniq_train, counts_train = np.unique(y_train, return_counts=True)
print(&quot;y_train count each labels: &quot;, dict(zip(uniq_train, counts_train)))
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
y_train count each labels:  {0: 43995, 1: 3743}
</pre></div></div>
</div>
<p>For the evaluation data as well, count the number of samples for each label in the same way.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>uniq_test, counts_test = np.unique(y_test, return_counts=True)
print(&quot;y_test count each labels: &quot;, dict(zip(uniq_test, counts_test)))
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
y_test count each labels:  {0: 42149, 1: 3200}
</pre></div></div>
</div>
<p>We see that for both the learning data and evaluation data, VEB samples are less than 10% of the total number of samples. Most of them are normal beat samples.</p>
<p>Next, let us visualize normal beat and VEB signal data.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>%matplotlib inline
import matplotlib.pyplot as plt
</pre></div>
</div>
</div>
<p>An example of normal beat is shown below.</p>
<p>It can be confirmed that P wave - QRS wave - T wave appears regularly.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>idx_n = np.where(y_train == 0)[0]
plt.plot(X_train[idx_n[0]])
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>[&lt;matplotlib.lines.Line2D at 0x7f04055b3320&gt;]
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Sequential_Data_Analysis_with_Deep_Learning_42_1.png" src="../_images/notebooks_Sequential_Data_Analysis_with_Deep_Learning_42_1.png" />
</div>
</div>
<p>On the other hand, the waveform of VEB is disordered, and the shape of the R wave peak and the distance between peaks are observably different from the normal example.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>idx_s = np.where(y_train == 1)[0]
plt.plot(X_train[idx_s[0]])
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>[&lt;matplotlib.lines.Line2D at 0x7f040354c4a8&gt;]
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Sequential_Data_Analysis_with_Deep_Learning_44_1.png" src="../_images/notebooks_Sequential_Data_Analysis_with_Deep_Learning_44_1.png" />
</div>
</div>
<p>The purpose of this case study is to learn the ECG signal features well and construct a model that classifies normality / abnormality and predicts with high accuracy for new waveform samples.</p>
<p>In the next section, we will explain the model construction using deep learning.</p>
</div>
<div class="section" id="Series-data-analysis-when-using-the-deep-learning">
<h2>8.5. Series data analysis when using the deep learning<a class="headerlink" href="#Series-data-analysis-when-using-the-deep-learning" title="Permalink to this headline">¶</a></h2>
<div class="section" id="Learning">
<h3>8.5.1. Learning<a class="headerlink" href="#Learning" title="Permalink to this headline">¶</a></h3>
<p>First, we define a dataset class to read pre-processed data prepared in the previous section with Chainer.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>class ECGDataset(chainer.dataset.DatasetMixin):

    def __init__(
            self,
            path
    ):
        if os.path.isfile(os.path.join(path, &#39;X.npy&#39;)):
            self.X = np.load(os.path.join(path, &#39;X.npy&#39;))
        else:
            raise FileNotFoundError(&quot;{}/X.npy not found.&quot;.format(path))
        if os.path.isfile(os.path.join(path, &#39;y.npy&#39;)):
            self.y = np.load(os.path.join(path, &#39;y.npy&#39;))
        else:
            raise FileNotFoundError(&quot;{}/y.npy not found.&quot;.format(path))

    def __len__(self):
        return len(self.X)

    def get_example(self, i):
        return self.X[None, i].astype(np.float32), self.y[i]

</pre></div>
</div>
</div>
<p>Next, we define the network structure used for learning (and prediction).</p>
<p>This time we will use the same network structure as CNN based <strong>ResNet34</strong> , which is popular for image recognition task [<a class="reference external" href="https://arxiv.org/abs/1512.03385">Reference 5</a>]</p>
<p>However, since input signals are one-dimensional arrays, we will use 1D Convolution, instead of 2D Convolution, which is routinely used for image analysis etc., as with the gene analysis in the previous chapter.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>import chainer.functions as F
import chainer.links as L
from chainer import reporter
from chainer import Variable


class BaseBlock(chainer.Chain):

    def __init__(
            self,
            channels,
            stride=1,
            dilate=1
    ):
        self.stride = stride
        super(BaseBlock, self).__init__()
        with self.init_scope():
            self.c1 = L.ConvolutionND(1, None, channels, 3, stride, dilate, dilate=dilate)
            self.c2 = L.ConvolutionND(1, None, channels, 3, 1, dilate, dilate=dilate)
            if stride &gt; 1:
                self.cd = L.ConvolutionND(1, None, channels, 1, stride, 0)
            self.b1 = L.BatchNormalization(channels)
            self.b2 = L.BatchNormalization(channels)

    def __call__(self, x):
        h = F.relu(self.b1(self.c1(x)))
        if self.stride &gt; 1:
            res = self.cd(x)
        else:
            res = x
        h = res + self.b2(self.c2(h))
        return F.relu(h)


class ResBlock(chainer.Chain):

    def __init__(
            self,
            channels,
            n_block,
            dilate=1
    ):
        self.n_block = n_block
        super(ResBlock, self).__init__()
        with self.init_scope():
            self.b0 = BaseBlock(channels, 2, dilate)
            for i in range(1, n_block):
                bx = BaseBlock(channels, 1, dilate)
                setattr(self, &#39;b{}&#39;.format(str(i)), bx)

    def __call__(self, x):
        h = self.b0(x)
        for i in range(1, self.n_block):
            h = getattr(self, &#39;b{}&#39;.format(str(i)))(h)
        return h


class ResNet34(chainer.Chain):

    def __init__(self):
        super(ResNet34, self).__init__()
        with self.init_scope():
            self.conv1 = L.ConvolutionND(1, None, 64, 7, 2, 3)
            self.bn1 = L.BatchNormalization(64)
            self.resblock0 = ResBlock(64, 3)
            self.resblock1 = ResBlock(128, 4)
            self.resblock2 = ResBlock(256, 6)
            self.resblock3 = ResBlock(512, 3)
            self.fc = L.Linear(None, 2)

    def __call__(self, x):
        h = F.relu(self.bn1(self.conv1(x)))
        h = F.max_pooling_nd(h, 3, 2)
        for i in range(4):
            h = getattr(self, &#39;resblock{}&#39;.format(str(i)))(h)
        h = F.average(h, axis=2)
        h = self.fc(h)
        return h


class Classifier(chainer.Chain):

    def __init__(
            self,
            predictor,
            lossfun=F.softmax_cross_entropy
    ):
        super(Classifier, self).__init__()
        with self.init_scope():
            self.predictor = predictor
            self.lossfun = lossfun

    def __call__(self, *args):
        assert len(args) &gt;= 2
        x = args[:-1]
        t = args[-1]
        y = self.predictor(*x)

        # loss
        loss = self.lossfun(y, t)
        with chainer.no_backprop_mode():
            # other metrics
            accuracy = F.accuracy(y, t)
        # reporter
        reporter.report({&#39;loss&#39;: loss}, self)
        reporter.report({&#39;accuracy&#39;: accuracy}, self)

        return loss

    def predict(self, x):
        with chainer.function.no_backprop_mode(), chainer.using_config(&#39;train&#39;, False):
            x = Variable(self.xp.asarray(x, dtype=self.xp.float32))
            y = self.predictor(x)
            return y
</pre></div>
</div>
</div>
<p>Prepare the following functions as preparations for executing learning.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">create_train_dataset()</span></code>：<code class="docutils literal notranslate"><span class="pre">ECGDataset</span></code> Pass/Feed the learning data set to the class</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">create_trainer()</span></code>：Make necessary settings for learning, create a Trainer object</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>from chainer import optimizers
from chainer.optimizer import WeightDecay
from chainer.iterators import MultiprocessIterator
from chainer import training
from chainer.training import extensions
from chainer.training import triggers
from chainer.backends.cuda import get_device_from_id


def create_train_dataset(root_path):
    train_path = os.path.join(root_path, &#39;preprocessed&#39;, &#39;train&#39;)
    train_dataset = ECGDataset(train_path)

    return train_dataset


def create_trainer(
    batchsize, train_dataset, nb_epoch=1,
    device=0, lossfun=F.softmax_cross_entropy
):
    # setup model
    model = ResNet34()
    train_model = Classifier(model, lossfun=lossfun)

    # use Adam optimizer
    optimizer = optimizers.Adam(alpha=0.001)
    optimizer.setup(train_model)
    optimizer.add_hook(WeightDecay(0.0001))

    # setup iterator
    train_iter = MultiprocessIterator(train_dataset, batchsize)

    # define updater
    updater = training.StandardUpdater(train_iter, optimizer, device=device)

    # setup trainer
    stop_trigger = (nb_epoch, &#39;epoch&#39;)
    trainer = training.trainer.Trainer(updater, stop_trigger)
    logging_attributes = [
        &#39;epoch&#39;, &#39;iteration&#39;,
        &#39;main/loss&#39;, &#39;main/accuracy&#39;
    ]
    trainer.extend(
        extensions.LogReport(logging_attributes, trigger=(2000 // batchsize, &#39;iteration&#39;))
    )
    trainer.extend(
        extensions.PrintReport(logging_attributes)
    )
    trainer.extend(
        extensions.ExponentialShift(&#39;alpha&#39;, 0.75, optimizer=optimizer),
        trigger=(4000 // batchsize, &#39;iteration&#39;)
    )

    return trainer
</pre></div>
</div>
</div>
<p>Now that we are ready for learning, call the function to create the trainer.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>train_dataset = create_train_dataset(dataset_root)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>trainer = create_trainer(256, train_dataset, nb_epoch=1, device=0)
</pre></div>
</div>
</div>
<p>Let’s start learning (learning is completed in about 1 minute 30 seconds.)</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>%time trainer.run()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
epoch       iteration   main/loss   main/accuracy
0           7           1.1307      0.868304
0           14          0.273237    0.923549
0           21          0.0950281   0.96596
0           28          0.058416    0.982701
0           35          0.0658751   0.982143
0           42          0.0506687   0.985491
0           49          0.057125    0.986607
0           56          0.063658    0.986607
0           63          0.0600915   0.981027
0           70          0.0391555   0.988281
0           77          0.0325103   0.991629
0           84          0.0344455   0.987723
0           91          0.0281526   0.989955
0           98          0.0266191   0.991629
0           105         0.0318078   0.990513
0           112         0.0304052   0.991071
0           119         0.0293185   0.993304
0           126         0.0290823   0.989397
0           133         0.019204    0.996094
0           140         0.0177221   0.994978
0           147         0.0218593   0.990513
0           154         0.019589    0.994978
0           161         0.0257332   0.991629
0           168         0.0155559   0.99442
0           175         0.0161097   0.99442
0           182         0.0212924   0.992746
CPU times: user 1min 12s, sys: 14.7 s, total: 1min 27s
Wall time: 1min 27s
</pre></div></div>
</div>
<p>We observe that the main / accuracy has reached near 0.99 (99%).</p>
</div>
</div>
<div class="section" id="Evaluation">
<h2>8.6. Evaluation<a class="headerlink" href="#Evaluation" title="Permalink to this headline">¶</a></h2>
<p>In order to confirm the classification performance by applying the learned model to the evaluation data, we prepare the following functions.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">create_test_dataset()</span></code> : Read evaluation data</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">predict()</span></code> : Perform inference and output result array (correct answer label and predictive label)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">print_confusion_matrix()</span></code> : Output a table called a confusion matrix from the prediction result</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">print_scores()</span></code> : Output evaluation index of prediction accuracy from prediction result</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>from chainer import cuda
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix


def create_test_dataset(root_path):
    test_path = os.path.join(root_path, &#39;preprocessed&#39;, &#39;test&#39;)
    test_dataset = ECGDataset(test_path)
    return test_dataset


def predict(trainer, test_dataset, batchsize, device=-1):
    model = trainer.updater.get_optimizer(&#39;main&#39;).target
    ys = []
    ts = []
    for i in range(len(test_dataset) // batchsize + 1):
        if i == len(test_dataset) // batchsize:
            X, t = zip(*test_dataset[i*batchsize: len(test_dataset)])
        else:
            X, t = zip(*test_dataset[i*batchsize:(i+1)*batchsize])
        X = cuda.to_gpu(np.array(X), device)
        y = model.predict(X)
        y = cuda.to_cpu(y.data.argmax(axis=1))
        ys.append(y)
        ts.append(np.array(t))
    return np.concatenate(ts), np.concatenate(ys)


def print_confusion_matrix(y_true, y_pred):
    labels = sorted(list(set(y_true)))
    target_names = [&#39;Normal&#39;, &#39;VEB&#39;]
    cmx = confusion_matrix(y_true, y_pred, labels=labels)
    df_cmx = pd.DataFrame(cmx, index=target_names, columns=target_names)
    plt.figure(figsize = (5,3))
    sn.heatmap(df_cmx, annot=True, annot_kws={&quot;size&quot;: 18}, fmt=&quot;d&quot;, cmap=&#39;Blues&#39;)
    plt.show()


def print_scores(y_true, y_pred):
    target_names = [&#39;Normal&#39;, &#39;VEB&#39;]
    print(classification_report(y_true, y_pred, target_names=target_names))
    print(&quot;accuracy: &quot;, accuracy_score(y_true, y_pred))

</pre></div>
</div>
</div>
<p>Prepare the evaluation data set:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>test_dataset = create_test_dataset(dataset_root)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-2-a0db38863ff3&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> </span>test_dataset <span class="ansi-blue-fg">=</span> create_test_dataset<span class="ansi-blue-fg">(</span>dataset_root<span class="ansi-blue-fg">)</span>

<span class="ansi-red-fg">NameError</span>: name &#39;create_test_dataset&#39; is not defined
</pre></div></div>
</div>
<p>We will make predictions for the evaluation data. (Prediction is completed in about 17 seconds)</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>%time y_true_test, y_pred_test = predict(trainer, test_dataset, 256, 0)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
CPU times: user 15.6 s, sys: 2.73 s, total: 18.3 s
Wall time: 18.3 s
</pre></div></div>
</div>
<p>Let’s confirm the prediction result.</p>
<p>First, we create a table that summarizes the classification results of predictions, called <strong>confusion matrix</strong> . The rows show the correct solution label and the columns show the prediction label. The following summary values are obtained for each item.</p>
<ul class="simple">
<li><p>Upper left: Number of samples that are actually normal and predicted as normal</p></li>
<li><p>Upper right: Number of samples that are actually normal but predicted as VEB</p></li>
<li><p>Bottom left: Samples that are actually VEBs but predicted to be normal</p></li>
<li><p>Bottom right: Samples that are actually VEB but predicted as VEB</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>print_confusion_matrix(y_true_test, y_pred_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Sequential_Data_Analysis_with_Deep_Learning_68_0.png" src="../_images/notebooks_Sequential_Data_Analysis_with_Deep_Learning_68_0.png" />
</div>
</div>
<p>Next, let’s display the evaluation index score (Classfication Report) of prediction accuracy calculated from the prediction result.</p>
<p>In particular, please pay attention to the following scores.</p>
<ul class="simple">
<li><p>Rate of conformity (Precision): Percentage (Normal or VEB) of each expected diagnosis (correct answer was the same diagnosis result)</p></li>
<li><p>Recall (Recall): Percentage of each correct diagnosis result (Normal or VEB) that was correctly predicted (prediction was the same diagnostic result)</p></li>
<li><p>F1 value (F1-score): Harmonic mean of precision and recall</p></li>
<li><p>Accuracy: The percentage of all diagnostic results (Normal and VEB) that was correctly predicted (prediction was the same diagnostic result)</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>print_scores(y_true_test, y_pred_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
              precision    recall  f1-score   support

      Normal       0.99      0.94      0.96     42149
         VEB       0.52      0.90      0.66      3200

   micro avg       0.94      0.94      0.94     45349
   macro avg       0.76      0.92      0.81     45349
weighted avg       0.96      0.94      0.94     45349

accuracy:  0.9351694634942336
</pre></div></div>
</div>
<p>Predictive scores for normal beats with a large number of samples show high values, while scores tend to be low for VEBs with a smaller number of samples. It is known that this tendency is often observed in unbalanced data. In this case we see that the proportion of classes occupied by samples is extremely biased.</p>
<p>In the next section, we will introduce some trial and error for improving prediction model, including how to deal with such class imbalance problem.</p>
</div>
<div class="section" id="Towards-the-accuracy">
<h2>8.7. Towards the accuracy<a class="headerlink" href="#Towards-the-accuracy" title="Permalink to this headline">¶</a></h2>
<p>In this section, we devise a method to contribute to accuracy improvement by devising the learning device constructed in the previous section from various viewpoints such as “data set”, “objective function”, “learning model”, “preprocessing”.</p>
<p>When analyzing data sets using machine learning, it is often unknown beforehand which methods/models are effective for accuracy improvement, so trial and error is necessary. However, rather than trying random methods, it is important to consider methods that can be effective based on the nature of the target dataset.</p>
<p>First of all, let’s consider how to cope with the problem of class imbalance, which was mentioned as a challenge in the previous section.</p>
<div class="section" id="Corresponds-to-the-class-imbalance-data">
<h3>8.7.1. Corresponds to the class imbalance data<a class="headerlink" href="#Corresponds-to-the-class-imbalance-data" title="Permalink to this headline">¶</a></h3>
<p>As mentioned in the previous section, when constructing a learning device using class imbalance data , prediction results biased towards the majority of classes may be predicted, and the precision may be lowered for a small number of classes in some cases It is generally known. On the other hand, in real-world tasks (including this dataset), it is often the case that it is important to accurately detect a few abnormal samples contained in the majority of normal samples. Under these
circumstances, there are several strategies to learn the model focusing on the detection of a few classes.</p>
<p>In particular,</p>
<ol class="arabic simple">
<li><p><strong>sampling</strong></p>
<ul class="simple">
<li><p>Sampling is performed from the imbalance data set, and a data set with a balanced class ratio is created.</p>
<ul>
<li><p><strong>Undersampling</strong> : Reduce the majority of normal samples.</p></li>
<li><p><strong>Oversampling</strong> : Inflate a few abnormal samples.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Weight adjustment of loss function</strong></p>
<ul class="simple">
<li><p>The penalty for misclassifying normal samples as abnormal is small, and the penalty for misclassifying abnormal samples as normal is increased.</p></li>
<li><p>For example, use reciprocal of the existence ratio of sample number as a weight.</p></li>
</ul>
</li>
<li><p><strong>Change of objective function (loss function)</strong></p>
<ul class="simple">
<li><p>Introduction of an objective function that improves prediction score for abnormal samples．</p></li>
</ul>
</li>
<li><p><strong>Abnormality detection</strong></p>
<ul class="simple">
<li><p>Data distribution of normal samples is assumed, and samples deviating sufficiently therefrom are regarded as abnormal.</p></li>
</ul>
</li>
</ol>
<p>There are methods such as. In this section, we will introduce examples of “1. sampling” and “3. Change of objective function”.</p>
<div class="section" id="Sampling">
<h4>8.7.1.1. Sampling<a class="headerlink" href="#Sampling" title="Permalink to this headline">¶</a></h4>
<p>Consider combining <strong>Undersampling</strong> and <strong>Oversampling</strong> to eliminate dataset imbalance.</p>
<p>In this time we will perform the sampling with the following steps.</p>
<ol class="arabic simple">
<li><p>By Undersampling, only normal beat samples are reduced to 1/4 (all VEB samples are left)</p>
<ul class="simple">
<li><p>Here we adopt simple random sampling. Due to the random nature, it is possible to delete samples that are important for classification (near the identification boundary with the VEB sample).</p></li>
<li><p>There are some methods to alleviate the problem of random sampling, but this time it is not used.</p></li>
</ul>
</li>
<li><p>By Oversampling, VEB samples are inflated to the same number as normal beat samples after Undersampling</p>
<ul class="simple">
<li><p>SMOTE (Synthetic Minority Over-sampling TEchnique) is adopted.</p></li>
<li><p>If it is the simplest way to inflate data randomly, it tends to cause over learning. In SMOTE, the data points between the VEB sample and its neighboring VEB sample are randomly generated and added to the data to ease the influence of over learning.</p></li>
</ul>
</li>
</ol>
<p>We define the class to do the sampling .<code class="docutils literal notranslate"><span class="pre">SampledECGDataset</span></code></p>
<p>Also, prepare a <code class="docutils literal notranslate"><span class="pre">create_sampled_train_datset()</span></code> function to read the class and create a data set object for study .</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>from imblearn.datasets import make_imbalance
from imblearn.over_sampling import SMOTE


class SampledECGDataset(ECGDataset):

    def __init__(
            self,
            path
    ):
        super(SampledECGDataset, self).__init__(path)
        _, counts = np.unique(self.y, return_counts=True)
        self.X, self.y = make_imbalance(
            self.X, self.y,
            sampling_strategy={0: counts[0]//4, 1: counts[1]}
        )
        smote = SMOTE(random_state=42)
        self.X, self.y = smote.fit_sample(self.X, self.y)


def create_sampled_train_dataset(root_path):
    train_path = os.path.join(root_path, &#39;preprocessed&#39;, &#39;train&#39;)
    train_dataset = SampledECGDataset(train_path)

    return train_dataset
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>train_dataset = create_sampled_train_dataset(dataset_root)
</pre></div>
</div>
</div>
<p>Let’s create a trainer and execute learning as before. (The learning is completed in about 1 minute.)</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>trainer = create_trainer(256, train_dataset, nb_epoch=2, device=0)
%time trainer.run()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
epoch       iteration   main/loss   main/accuracy
0           7           1.55422     0.71317
0           14          0.282043    0.902344
0           21          0.161596    0.939174
0           28          0.109996    0.960379
0           35          0.0750687   0.976004
0           42          0.0759563   0.969866
0           49          0.0531007   0.979911
0           56          0.0477525   0.984375
0           63          0.0657228   0.981585
0           70          0.0559838   0.979911
0           77          0.0410638   0.985491
0           84          0.0311444   0.989397
1           91          0.0260796   0.993304
1           98          0.0306111   0.990513
1           105         0.0267346   0.987165
1           112         0.0161048   0.995536
1           119         0.0202833   0.991629
1           126         0.0112008   0.998326
1           133         0.0164346   0.99442
1           140         0.0189331   0.992746
1           147         0.0153746   0.99442
1           154         0.0173289   0.994978
1           161         0.01027     0.996094
1           168         0.01294     0.995536
CPU times: user 55.1 s, sys: 13.4 s, total: 1min 8s
Wall time: 1min 8s
</pre></div></div>
</div>
<p>When learning is completed, try predicting with the evaluation data and check the accuracy.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>%time y_true_test, y_pred_test = predict(trainer, test_dataset, 256, 0)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
CPU times: user 15.1 s, sys: 2.91 s, total: 18 s
Wall time: 18 s
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>print_confusion_matrix(y_true_test, y_pred_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Sequential_Data_Analysis_with_Deep_Learning_84_0.png" src="../_images/notebooks_Sequential_Data_Analysis_with_Deep_Learning_84_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>print_scores(y_true_test, y_pred_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
              precision    recall  f1-score   support

      Normal       1.00      0.91      0.95     42149
         VEB       0.45      0.96      0.61      3200

   micro avg       0.91      0.91      0.91     45349
   macro avg       0.72      0.93      0.78     45349
weighted avg       0.96      0.91      0.93     45349

accuracy:  0.9144854351804891
</pre></div></div>
</div>
<p>Compare with the previous prediction result, please check whether the detection accuracy (especially recall) for the VEB sample is improved by the sampling effect.</p>
<p>(Please note that precision will not necessarily be improved because of influences such as randomness of sampling and initial value dependency of learning.)</p>
</div>
<div class="section" id="Changing-the-loss-function">
<h4>8.7.1.2. Changing the loss function<a class="headerlink" href="#Changing-the-loss-function" title="Permalink to this headline">¶</a></h4>
<p>Next, by changing the loss function, we will consider a method to improve accuracy on a few abnormal samples. Several loss functions focusing on improving prediction accuracy of minority classes have been proposed so far, but this time we will use the loss function called <strong>Focal loss</strong> .</p>
<p>Focal loss is the loss function proposed in the research paper [<a class="reference external" href="https://arxiv.org/abs/1708.02002">Reference 6</a>] of the object detection method of images . In the One-stage object detection method, there are many cases where the object actually exists in a large number of candidate regions, and in many cases there are at most several, it is a class imbalance task, and the problem that learning does not progress well There is. To deal with these problems, focal loss was proposed and is
described by the following equation.</p>
<div class="math notranslate nohighlight">
\[FL(p_t) = - (1 - p_t)^{\gamma}\log(p_t)\]</div>
<p>here<span class="math notranslate nohighlight">\(p_t\)</span> is the output (probability value) of Softmax function.<span class="math notranslate nohighlight">\(\gamma = 0\)</span>, It is equal to normal Softmax cross-entropy loss,<span class="math notranslate nohighlight">\(\gamma &gt; 0\)</span>, It has the effect of reducing the relative loss for clearly classifiable (easy to distinguish) samples. As a result, it is expected that studies will progress with attention to samples that are difficult to classify.</p>
<p>The figure below plots the relationship between the predicted probability value of the correct class and the loss at that time,<span class="math notranslate nohighlight">\(\gamma\)</span>It shows how the relative loss will decline when changing the value of.</p>
<p><img alt="正解予測確率と損失の関係" src="https://raw.githubusercontent.com/preferred-medicine/medical-ai-course-materials/master/notebooks/images/monitoring/focal_plot.png" /></p>
<p>(Cited from [<a class="reference external" href="https://arxiv.org/abs/1708.02002">Reference 6</a>])</p>
<p>Let’s actually define the Focal loss function.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>from chainer.backends.cuda import get_array_module

def focal_loss(x, t, class_num=2, gamma=0.5, eps=1e-6):
    xp = get_array_module(t)

    p = F.softmax(x)
    p = F.clip(p, x_min=eps, x_max=1-eps)
    log_p = F.log_softmax(x)
    t_onehot = xp.eye(class_num)[t.ravel()]

    loss_sce = -1 * t_onehot * log_p
    loss_focal = F.sum(loss_sce * (1. - p) ** gamma, axis=1)

    return F.mean(loss_focal)
</pre></div>
</div>
</div>
<p>Do not perform the data sampling performed in the previous item, change the loss function to focal loss after setting the same setting as in the initial (§ 8.5) learning.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>train_dataset = create_train_dataset(dataset_root)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>trainer = create_trainer(256, train_dataset, nb_epoch=1, device=0, lossfun=focal_loss)
</pre></div>
</div>
</div>
<p>Let’s start learning. (Learning is completed in about 1 minute 30 seconds.)</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>%time trainer.run()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
epoch       iteration   main/loss   main/accuracy
0           7           1.21684     0.876674
0           14          0.149564    0.957589
0           21          0.0536211   0.976004
0           28          0.0491485   0.979353
0           35          0.0278858   0.987165
0           42          0.0252027   0.989397
0           49          0.0311808   0.986607
0           56          0.0199696   0.989955
0           63          0.0132789   0.99442
0           70          0.0140394   0.996094
0           77          0.0144007   0.993304
0           84          0.0169302   0.992746
0           91          0.0165225   0.992746
0           98          0.0110192   0.996094
0           105         0.0177556   0.992746
0           112         0.0139628   0.994978
0           119         0.0113324   0.994978
0           126         0.010316    0.996094
0           133         0.0103831   0.995536
0           140         0.0128646   0.995536
0           147         0.0246793   0.992188
0           154         0.00787007  0.996652
0           161         0.0168202   0.994978
0           168         0.00875182  0.996652
0           175         0.015041    0.993862
0           182         0.00939075  0.99442
CPU times: user 1min 4s, sys: 13.7 s, total: 1min 17s
Wall time: 1min 17s
</pre></div></div>
</div>
<p>When learning is completed, let’s check the prediction result with the evaluation data.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>%time y_true_test, y_pred_test = predict(trainer, test_dataset, 256, 0)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
CPU times: user 15.1 s, sys: 2.8 s, total: 17.9 s
Wall time: 17.9 s
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>print_confusion_matrix(y_true_test, y_pred_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Sequential_Data_Analysis_with_Deep_Learning_97_0.png" src="../_images/notebooks_Sequential_Data_Analysis_with_Deep_Learning_97_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>print_scores(y_true_test, y_pred_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
              precision    recall  f1-score   support

      Normal       0.99      0.95      0.97     42149
         VEB       0.57      0.93      0.71      3200

   micro avg       0.95      0.95      0.95     45349
   macro avg       0.78      0.94      0.84     45349
weighted avg       0.96      0.95      0.95     45349

accuracy:  0.9456437848684646
</pre></div></div>
</div>
<p>Please compare the prediction result of the initial model with the prediction result of this time.</p>
<p>(If there is a surplus, <span class="math notranslate nohighlight">\(\gamma\)</span> Please check the influence on the prediction result when changing the value of. )</p>
</div>
</div>
<div class="section" id="Changing-the-network-structure">
<h3>8.7.2. Changing the network structure<a class="headerlink" href="#Changing-the-network-structure" title="Permalink to this headline">¶</a></h3>
<p>Next, consider <strong>changing</strong> the <strong>network structure</strong> used for learning .</p>
<p>Here we extend the following for the ResNet 34 structure we used first</p>
<ol class="arabic simple">
<li><p>Change 1D Convolution to，<strong>1D Dilated Convolution</strong></p>
<ul class="simple">
<li><p>By using Dilated Convolution, it is expected that a wider range of features can be extracted while suppressing the increase of parameter number (same motivation as in genetic analysis).．</p></li>
<li><p>In tasks where a wide range of features are not important, there is a possibility that accuracy does not improve (or in some cases, accuracy deteriorates).</p></li>
</ul>
</li>
<li><p>Add the entire bonding layer before the final layer and apply <strong>Dropout</strong></p>
<ul class="simple">
<li><p>We expect that generalization performance of learning equipment will be improved by doing dropout. However, in several previous studies [<a class="reference external" href="https://arxiv.org/abs/1506.02158v6">Ref. 7</a>] etc.), it is reported that simply improving generalization performance can not be expected by merely applying Dropout immediately after the convolution layer, I will apply it.</p></li>
</ul>
</li>
</ol>
<p>Let’s define a network with the above extension. (ResBlock class has already been defined at initial model construction)</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>class DilatedResNet34(chainer.Chain):

    def __init__(self):
        super(DilatedResNet34, self).__init__()
        with self.init_scope():
            self.conv1 = L.ConvolutionND(1, None, 64, 7, 2, 3)
            self.bn1 = L.BatchNormalization(64)
            self.resblock0 = ResBlock(64, 3, 1)
            self.resblock1 = ResBlock(128, 4, 1)
            self.resblock2 = ResBlock(256, 6, 2)
            self.resblock3 = ResBlock(512, 3, 4)
            self.fc1 = L.Linear(None, 512)
            self.fc2 = L.Linear(None, 2)

    def __call__(self, x):
        h = F.relu(self.bn1(self.conv1(x)))
        h = F.max_pooling_nd(h, 3, 2)
        for i in range(4):
            h = getattr(self, &#39;resblock{}&#39;.format(str(i)))(h)
        h = F.average(h, axis=2)
        h = F.dropout(self.fc1(h), 0.5)
        h = self.fc2(h)
        return h
</pre></div>
</div>
</div>
<p>After making the same settings as in the initial (§ 8.5) learning <code class="docutils literal notranslate"><span class="pre">DilatedResNet34</span></code>, change the network structure and learn.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>def create_trainer(
    batchsize, train_dataset, nb_epoch=1,
    device=0, lossfun=F.softmax_cross_entropy
):
    # setup model
    model = DilatedResNet34()
    train_model = Classifier(model, lossfun=lossfun)

    # use Adam optimizer
    optimizer = optimizers.Adam(alpha=0.001)
    optimizer.setup(train_model)
    optimizer.add_hook(WeightDecay(0.0001))

    # setup iterator
    train_iter = MultiprocessIterator(train_dataset, batchsize)

    # define updater
    updater = training.StandardUpdater(train_iter, optimizer, device=device)

    # setup trainer
    stop_trigger = (nb_epoch, &#39;epoch&#39;)
    trainer = training.trainer.Trainer(updater, stop_trigger)
    logging_attributes = [
        &#39;epoch&#39;, &#39;iteration&#39;,
        &#39;main/loss&#39;, &#39;main/accuracy&#39;
    ]
    trainer.extend(
        extensions.LogReport(logging_attributes, trigger=(2000 // batchsize, &#39;iteration&#39;))
    )
    trainer.extend(
        extensions.PrintReport(logging_attributes)
    )
    trainer.extend(
        extensions.ExponentialShift(&#39;alpha&#39;, 0.75, optimizer=optimizer),
        trigger=(4000 // batchsize, &#39;iteration&#39;)
    )

    return trainer
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>train_dataset = create_train_dataset(dataset_root)
test_dataset = create_test_dataset(dataset_root)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>trainer = create_trainer(256, train_dataset, nb_epoch=1, device=0)
</pre></div>
</div>
</div>
<p>Let’s start learning as before. (Learning is completed in about 1 minute 30 seconds.)</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>%time trainer.run()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
epoch       iteration   main/loss   main/accuracy
0           7           1.06941     0.860491
0           14          0.204161    0.924665
0           21          0.164622    0.932478
0           28          0.103986    0.958147
0           35          0.0782762   0.973772
0           42          0.0455737   0.988281
0           49          0.030971    0.989955
0           56          0.0431471   0.988839
0           63          0.0427462   0.985491
0           70          0.0333034   0.991629
0           77          0.0239444   0.993862
0           84          0.0325211   0.989397
0           91          0.0281632   0.991071
0           98          0.0222488   0.989955
0           105         0.0232916   0.992746
0           112         0.0210675   0.992746
0           119         0.00808897  0.997768
0           126         0.0198379   0.991629
0           133         0.0183009   0.99442
0           140         0.0103409   0.996652
0           147         0.0131249   0.995536
0           154         0.0128261   0.993862
0           161         0.0152106   0.996652
0           168         0.00777262  0.996652
0           175         0.0233488   0.993304
0           182         0.0204095   0.995536
CPU times: user 1min, sys: 14.3 s, total: 1min 14s
Wall time: 1min 14s
</pre></div></div>
</div>
<p>When learning is completed, try predicting with the evaluation data and check the accuracy.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>%time y_true_test, y_pred_test = predict(trainer, test_dataset, 256, 0)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
CPU times: user 15.1 s, sys: 2.8 s, total: 17.9 s
Wall time: 17.9 s
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>print_confusion_matrix(y_true_test, y_pred_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Sequential_Data_Analysis_with_Deep_Learning_112_0.png" src="../_images/notebooks_Sequential_Data_Analysis_with_Deep_Learning_112_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>print_scores(y_true_test, y_pred_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
              precision    recall  f1-score   support

      Normal       0.99      0.96      0.98     42149
         VEB       0.63      0.91      0.74      3200

   micro avg       0.96      0.96      0.96     45349
   macro avg       0.81      0.94      0.86     45349
weighted avg       0.97      0.96      0.96     45349

accuracy:  0.9553683653443296
</pre></div></div>
</div>
<p>Please compare the prediction result of the initial model with the prediction result of this time.</p>
</div>
<div class="section" id="Validating-the-effect-of-noise">
<h3>8.7.3. Validating the effect of noise<a class="headerlink" href="#Validating-the-effect-of-noise" title="Permalink to this headline">¶</a></h3>
<p>Finally, we will examine the <strong>removal of noise</strong> contained in electrocardiogram .</p>
<p>External noise such as the following may be included in the electrocardiogram waveform.[<a class="reference external" href="http://www.iosrjournals.org/iosr-jece/papers/ICETEM/Vol.%201%20Issue%201/ECE%2006-40-44.pdf">Reference 8</a>]</p>
<ul class="simple">
<li><p>high frequency</p>
<ul>
<li><p><strong>EMG noise</strong> (Electromyogram noise)</p>
<ul>
<li><p>Due to body movements, electrical activity of the muscles may get mixed in the electrocardiogram.</p></li>
</ul>
</li>
<li><p><strong>Power line</strong> induction failure (Power line interference)</p>
<ul>
<li><p>An alternating current may flow due to electrostatic induction and it may get mixed in the electrocardiogram.</p></li>
<li><p>Magnetic lines of force are generated by current flowing in the power supply wiring, and alternating current may flow due to electromagnetic induction action.</p></li>
</ul>
</li>
<li><p><strong>Additive white Gaussian noise</strong> (Additive white Gaussian noise)</p>
<ul>
<li><p>White noise comes in due to various factors derived from the external environment.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Low-frequency</p>
<ul>
<li><p><strong>Baseline drift</strong> (Baseline wandering)</p>
<ul>
<li><p>Baseline may fluctuate slowly due to insufficient attachment of electrode, perspiration, body movement and so on.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>When analyzing an electrocardiogram, pre-processing to eliminate noise as described above is generally performed in order to accurately distinguish abnormal waveforms such as tachycardia and bradycardia.</p>
<p>There are several ways to eliminate noise, but the simplest is to apply a linear filter. Let’s try noise elimination using a Butterworth filter which is one of linear filters this time.</p>
<p><code class="docutils literal notranslate"><span class="pre">BaseECGDatasetPreprocessor</span></code> We added a signal noise elimination function <code class="docutils literal notranslate"><span class="pre">DenoiseECGDatasetPreprocessor</span></code> to define the class.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>from scipy.signal import butter, lfilter


class DenoiseECGDatasetPreprocessor(BaseECGDatasetPreprocessor):

    def __init__(
            self,
            dataset_root=&#39;./&#39;,
            window_size=720
    ):
        super(DenoiseECGDatasetPreprocessor, self).__init__(
        dataset_root, window_size)

    def _denoise_signal(
            self,
            signal,
            btype=&#39;low&#39;,
            cutoff_low=0.2,
            cutoff_high=25.,
            order=5
    ):
        nyquist = self.sample_rate / 2.
        if btype == &#39;band&#39;:
            cut_off = (cutoff_low / nyquist, cutoff_high / nyquist)
        elif btype == &#39;high&#39;:
            cut_off = cutoff_low / nyquist
        elif btype == &#39;low&#39;:
            cut_off = cutoff_high / nyquist
        else:
            return signal
        b, a = butter(order, cut_off, analog=False, btype=btype)
        return lfilter(b, a, signal)

    def _segment_data(
            self,
            signal,
            symbols,
            positions
    ):
        X = []
        y = []
        sig_len = len(signal)
        for i in range(len(symbols)):
            start = positions[i] - self.window_size // 2
            end = positions[i] + self.window_size // 2
            if symbols[i] in self.valid_symbols and start &gt;= 0 and end &lt;= sig_len:
                segment = signal[start:end]
                assert len(segment) == self.window_size, &quot;Invalid length&quot;
                X.append(segment)
                y.append(self.labels.index(self.label_map[symbols[i]]))
        return np.array(X), np.array(y)

    def prepare_dataset(
            self,
            denoise=False,
            normalize=True
    ):
        if not os.path.isdir(self.download_dir):
            self.download_data()

        # prepare training dataset
        self._prepare_dataset_core(self.train_record_list, &quot;train&quot;, denoise, normalize)
        # prepare test dataset
        self._prepare_dataset_core(self.test_record_list, &quot;test&quot;, denoise, normalize)

    def _prepare_dataset_core(
            self,
            record_list,
            mode=&quot;train&quot;,
            denoise=False,
            normalize=True
    ):
        Xs, ys = [], []
        save_dir = os.path.join(self.dataset_root, &#39;preprocessed&#39;, mode)
        for i in range(len(record_list)):
            signal, symbols, positions = self._load_data(record_list[i])
            if denoise:
                signal = self._denoise_signal(signal)
            if normalize:
                signal = self._normalize_signal(signal)
            X, y = self._segment_data(signal, symbols, positions)
            Xs.append(X)
            ys.append(y)
        os.makedirs(save_dir, exist_ok=True)
        np.save(os.path.join(save_dir, &quot;X.npy&quot;), np.vstack(Xs))
        np.save(os.path.join(save_dir, &quot;y.npy&quot;), np.concatenate(ys))

</pre></div>
</div>
</div>
<p>Applying a linear filter may make it easier for the learning model to capture the abnormal beat pattern as a feature. On the other hand, please be aware that important information may be removed when detecting abnormal beats.</p>
<p>There are some rough classifications in linear filters, depending on their frequency characteristics (frequency band components in which bands are blocked). For example, there are the following.</p>
<ul class="simple">
<li><p><strong>Low-pass filter</strong> : Only low-frequency components pass (high-frequency components are blocked)</p></li>
<li><p><strong>High-pass filter</strong> : Only high-frequency components pass (low-frequency components are blocked)</p></li>
<li><p><strong>Band-pass filter</strong> : Only certain band components pass (low frequency, high frequency components are blocked)</p></li>
</ul>
<p><img alt="Classification by frequency characteristic of linear filter" src="https://raw.githubusercontent.com/preferred-medicine/medical-ai-course-materials/master/notebooks/images/monitoring/band_form.png" /></p>
<p>(Cited from [<a class="reference external" href="https://en.wikipedia.org/wiki/Filter_%28signal_processing%29">Reference 9</a>])</p>
<p>In mitdb, the low frequency of 0.1 Hz or less and the high frequency of 100 Hz or more have been removed by the band pass filter beforehand, and here, furthermore, the high frequency noise is removed by the low pass Butterworth filter of 25 Hz.</p>
<p>Let’s activate the noise elimination option and try preprocessing.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>DenoiseECGDatasetPreprocessor(dataset_root).prepare_dataset(denoise=True)
</pre></div>
</div>
</div>
<p>In fact, let’s visualize the waveform after removal of high-frequency noise.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>X_train_d = np.load(os.path.join(dataset_root, &#39;preprocessed&#39;, &#39;train&#39;, &#39;X.npy&#39;))
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>plt.subplots(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(X_train[idx_n[0]])
plt.subplot(1, 2, 2)
plt.plot(X_train_d[idx_n[0]])
plt.show()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Sequential_Data_Analysis_with_Deep_Learning_124_0.png" src="../_images/notebooks_Sequential_Data_Analysis_with_Deep_Learning_124_0.png" />
</div>
</div>
<p>The left figure shows the waveform before filtering and the right figure shows the waveform after filtering. I think that it can be confirmed that fine vibration has been removed.</p>
<p>Let’s try learning using the noise-removed data as before. (Learning is completed in about 1 minute 30 seconds.)</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>train_dataset = create_train_dataset(dataset_root)
test_dataset = create_test_dataset(dataset_root)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>trainer = create_trainer(256, train_dataset, nb_epoch=1, device=0)
%time trainer.run()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
epoch       iteration   main/loss   main/accuracy
0           7           1.21514     0.851562
0           14          0.283428    0.951451
0           21          0.157269    0.977679
0           28          0.141721    0.962612
0           35          0.0516044   0.986049
0           42          0.068243    0.987723
0           49          0.0541831   0.987723
0           56          0.0363404   0.993304
0           63          0.0441804   0.992188
0           70          0.0271119   0.991629
0           77          0.00849581  0.99721
0           84          0.0216285   0.996652
0           91          0.034815    0.990513
0           98          0.0259185   0.991629
0           105         0.0240092   0.993862
0           112         0.00988215  0.99721
0           119         0.0122308   0.996094
0           126         0.00706834  0.998326
0           133         0.0196008   0.994978
0           140         0.0134032   0.996652
0           147         0.0330729   0.993304
0           154         0.0230272   0.993304
0           161         0.00712667  0.99721
0           168         0.0140302   0.996094
0           175         0.00879468  0.995536
0           182         0.00927302  0.99721
CPU times: user 1min, sys: 13.8 s, total: 1min 14s
Wall time: 1min 13s
</pre></div></div>
</div>
<p>When learning is completed, try predicting with the evaluation data and check the accuracy.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>%time y_true_test, y_pred_test = predict(trainer, test_dataset, 256, 0)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
CPU times: user 15.2 s, sys: 2.81 s, total: 18 s
Wall time: 18 s
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>print_confusion_matrix(y_true_test, y_pred_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Sequential_Data_Analysis_with_Deep_Learning_130_0.png" src="../_images/notebooks_Sequential_Data_Analysis_with_Deep_Learning_130_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span>print_scores(y_true_test, y_pred_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
              precision    recall  f1-score   support

      Normal       0.99      0.97      0.98     42149
         VEB       0.67      0.93      0.78      3200

   micro avg       0.96      0.96      0.96     45349
   macro avg       0.83      0.95      0.88     45349
weighted avg       0.97      0.96      0.97     45349

accuracy:  0.9632185935742795
</pre></div></div>
</div>
<p>Let’s see how the prediction accuracy has changed by eliminating high frequency noise.</p>
</div>
</div>
<div class="section" id="Conclusion">
<h2>8.8. Conclusion<a class="headerlink" href="#Conclusion" title="Permalink to this headline">¶</a></h2>
<p>In this chapter, we addressed the problem of arrhythmia detection using ECG’s public data set.</p>
<p>The things I wanted to tell through this lecture are as follows.</p>
<ol class="arabic simple">
<li><p>Minimum knowledge necessary for analyzing electrocardiogram</p></li>
<li><p>Basic preprocessing procedure for analyzing monitoring data</p></li>
<li><p>Construction of learning device using CNN-based model</p></li>
<li><p>Learning method considering the nature of data set and ingenuity of preprocessing</p></li>
</ol>
<p>Also, I have tried various methods to improve accuracy, but in real world tasks it is almost impossible to find out what kind of idea works effectively. Therefore, while doing trial and error, we need to find ways to adapt to that problem setting.</p>
<p>As further efforts, there is room to consider the following contents, for example.</p>
<ul class="simple">
<li><p>Add information</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(Ⅱ\)</span> In addition to induction signals,<span class="math notranslate nohighlight">\(V_1\)</span>Induction signals are given as input at the same time. [<a class="reference external" href="https://www.kdd.org/kdd2018/files/deep-learning-day/DLDay18_paper_16.pdf">Reference10</a></p></li>
</ul>
</li>
<li><p>Devices for pretreatment</p>
<ul>
<li><p>Change of segment length</p>
<ul>
<li><p>Long-term waveform information is extracted by inputting longer segment. (<a class="reference external" href="https://arxiv.org/abs/1810.04121">Reference 4</a>] uses a segment of 10 seconds for analysis)</p></li>
<li><p>The increase of input information may make learning difficult.．</p></li>
</ul>
</li>
<li><p>Resampling</p>
<ul>
<li><p>By lowering the sampling frequency, long-term waveform information is extracted.(Downsampling to 180 Hz in [<a class="reference external" href="https://arxiv.org/abs/1810.04121">Reference 4</a>]</p>
<ul>
<li><p>There is a possibility that learning may be affected by rough waveform.．</p></li>
<li><p>Without proper pretreatment, distortion called loopback noise occurs.</p></li>
<li><p>(The process of reducing the information before inputting to the model is common in fields such as image analysis)</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Add label</p>
<ul>
<li><p>In addition to Normal, VEB, SVEB (supraventricular ectopic beat) etc. was also added.</p></li>
</ul>
</li>
<li><p>Changing the way labels are given</p>
<ul>
<li><p>If a segment label contains a peak label other than normal, give that label preferentially, etc.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Change model</p>
<ul>
<li><p>In order to extract long-term features, incorporate RNN-based structure (such as LSTM) in the latter part of CNN (<a class="reference external" href="https://arxiv.org/abs/1810.04121">Reference 4</a>]etc)．</p></li>
</ul>
</li>
</ul>
<p>If you have enough resources, please try to challenge.</p>
<p>Recently, several cases of publishing research results have been issued for large-scale monitoring data gathered independently.．</p>
<ul class="simple">
<li><p>In collaboration with Cardiogram Corporation and the University of California, Heart Rate data was collected from the activity meter and Deep Heart which predicts diabetes preliminary group using deep learning was announced [<a class="reference external" href="https://arxiv.org/abs/1802.02511">Reference 11</a>]．</p></li>
<li><p>In the laboratory of Andrew Ng. Of Stanford University, from the ECG record independently collected, We built a model to predict the type of waveform classification, and conducted a comparative experiment with a doctor [<a class="reference external" href="https://arxiv.org/abs/1707.01836">Reference 12</a>]．</p></li>
</ul>
<p>Because advances in devices have made it easier to gather precise information, we believe that such research will become increasingly popular in the future.</p>
</div>
<div class="section" id="References">
<h2>8.9. References<a class="headerlink" href="#References" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p><strong>Electrocardiography</strong> Wikipedia: The Free Encyclopedia. Wikimedia Foundation, Inc. 22 July 2004. Web. 10 Aug. 2004, [<a class="reference external" href="https://en.wikipedia.org/wiki/Electrocardiography">Link</a>]</p></li>
<li><p><strong>ECG Evaluation Manual</strong> , Japan Society of Human Dogs, April, Heisei 20, [<a class="reference external" href="https://www.ningen-dock.jp/wp/wpcontent/uploads/2013/09/d4bb55fcf01494e251d315b76738ab40.pdf">Link</a>]</p></li>
<li><p><strong>Automatic classification of heartbeats using ECG morphology and heartbeat interval features</strong> , Phillip de Chazal et al., June 2004, [<a class="reference external" href="https://ieeexplore.ieee.org/document/1306572">Link</a>]</p></li>
<li><p><strong>Inter-Patient ECG Classification with Convolutional and Recurrent Neural Networks</strong> , Li Guo et al., Sep 2018, [<a class="reference external" href="https://arxiv.org/abs/1810.04121">Link</a>]</p></li>
<li><p><strong>Deep Residual Learning for Image Recognition</strong> , Kaiming He et al., Dec 2015, [<a class="reference external" href="https://arxiv.org/abs/1512.03385">Link</a>]</p></li>
<li><p><strong>Focal Loss for Dense Object Detection</strong> , Tsung-Yi Lin et al., Aug 2017, [<a class="reference external" href="https://arxiv.org/abs/1708.02002">Link</a>]</p></li>
<li><p><strong>Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational Inference</strong> , Yarin Gal et al., Jun 2015, [<a class="reference external" href="https://arxiv.org/abs/1506.02158v6">Link</a>]</p></li>
<li><p><strong>Noise Analysis and Different Denoising Techniques of ECG Signal - A Survey</strong> , Aswathy Velayudhan et al., ICETEM 2016, [<a class="reference external" href="http://www.iosrjournals.org/iosr-jece/papers/ICETEM/Vol.%201%20Issue%201/ECE%2006-40-44.pdf">Link</a>]</p></li>
<li><p><strong>Filter (signal processing)</strong> , Wikipedia: The Free Encyclopedia. Wikimedia Foundation, Inc. 22 July 2004. Web. 10 Aug. 2004, [<a class="reference external" href="https://en.wikipedia.org/wiki/Filter_%28signal_processing%29">Link</a>]</p></li>
<li><p><strong>Arrhythmia Detection from 2-lead ECG using Convolutional Denoising Autoencoders</strong> , Keiichi Ochiai et al., KDD2018, [<a class="reference external" href="https://www.kdd.org/kdd2018/files/deep-learning-day/DLDay18_paper_16.pdf">Link</a>]</p></li>
<li><p><strong>DeepHeart: Semi-Supervised Sequence Learning for Cardiovascular Risk Prediction</strong> , Brandon Ballinger et al., Feb 2018, [<a class="reference external" href="https://arxiv.org/abs/1802.02511">Link</a>]</p></li>
<li><p><strong>Cardiologist-Level Arrhythmia Detection with Convolutional Neural Networks</strong> , Pranav Rajpurkar et al., Jul 2017, [<a class="reference external" href="https://arxiv.org/abs/1707.01836">Link</a>]</p></li>
</ol>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="DNA_Sequence_Data_Analysis.html" class="btn btn-neutral" title="7. Practical part: sequence analysis using deep learning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Preferred Networks &amp; キカガク

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>